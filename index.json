[{"categories":null,"content":"I was recently asked on Twitter, “What, who, have influenced how you think about programming and have helped inspire you and keep you humble?” This was in response to my saying that I am baffled at how bad I was at programming at the beginning of my career but still successful. I felt that the question warranted more space than Twitter provides. The following are the key people in my career who have helped me grow and inspire me. They are roughly in chronological order. ","date":"2023-05-23","objectID":"/blog/2023/05/who-has-inspired-you/:0:0","tags":null,"title":"Who has Inspired You?","uri":"/blog/2023/05/who-has-inspired-you/"},{"categories":null,"content":"Justin My first real job out of college was as an Industrial Engineer at Siltronic AG. I worked in the Supply Chain group and it was my responsibility to increase the efficiency of the facility. The facility was a 200mm silicon wafer manufacturer. At the time much of the industry had already moved to 300mm but there were many legacy facilities that still worked with 200mm. This meant that we had to maximize efficiency since there were not going to be significant capital investments because 200mm was not a growth part of the industry. It quickly became apparent to me that the scheduling algorithms being used to feed the line were leading to too much work in progress (WIP) which caused Cycle Times to balloon. Long Cycle Times meant that metrology feedback was slow so it was easy to have quality excursions, creating a rework. We had to get Cycle Times down, which meant that the WIP also had to go down. Leadership feared that we couldn’t maximize the utilization of critical tools unless we had significant WIP piled up throughout the facility. I reassured leadership that with the right algorithms in place, we could reduce WIP but still maintain high utilization of key bottlenecks in the fab. The problem was that we needed a programmer to write the algorithms. The IT department was swamped with support work, so none of their programmers were available. I figured that I could write the algorithm, but I had no idea how to integrate it into the software running the fab. Enter Justin. Justin was the programmer for the Supply Chain group. He was a self-taught programmer who had worked his way up from working in the fab to being a developer. He had learned Groovy, Grails, and SQL and had built many of the tools the Supply Chain group used to manage the fab. I began pestering him with questions and he patiently helped me build the tools we needed in Groovy. Justin was the first developer mentor I had. Without his patience and guidance, I would not be where I am today. I owe a great deal of my success to his kindness and patience. If only we could all have a mentor like Justin in our lives. We successfully halved the facility’s WIP and Cycle Time and got awards for our work. I was flown out to the headquarters in Germany for an Industrial Engineering Symposium where I was asked to present on what we had done. None of that success would have been possible without Justin. I am grateful that I am friends with Justin to this day. ","date":"2023-05-23","objectID":"/blog/2023/05/who-has-inspired-you/:1:0","tags":null,"title":"Who has Inspired You?","uri":"/blog/2023/05/who-has-inspired-you/"},{"categories":null,"content":"Don Syme While Groovy may have been the first language I did any productive work in, it wasn’t till F# that I turned into a real developer. I had moved on from Siltronic and now worked at an online retailer where we specialized in selling on the Amazon marketplace. We used R to calculate statistical models of the demand, supply, and pricing fluctuations of a large number of products. We would then use the results of these models to optimize our ordering to maximize profitability. The problem we kept running into though was that R was not designed as an industrial-strength programming language. R is a great statistics calculator. It’s a poor production language. I remember having dabbled with the F# language several years prior and thought that the Units of Measure feature would be helpful in all of our financial modeling. I decided to pick it back up and see how difficult it would be to translate our R code to F#. Turns out that F# really shines at data transformation work. We were able to build fast and robust services in a relatively short period. Our entire data pipeline was transformed into F# within a few weeks. It ran faster and more reliably than anything previous. It was at this point that I start taking software development as a discipline more seriously. I start reading everything I can on F# and on Don Syme, the creator of F#. Before long I learn that he was primarily responsible for Generics in .NET. What struck me, though, was his humility. In all the interviews I have watched or listened to, I have never sensed an ounce of pride coming from the man. He is not brash or abrasive. He does not lord his accomplishments over others. He appears to have infinite patience with people continually asking for Higher-Kinded Types in F#. For all of his technical accomplishments, it’s Don’s character that most impresses me. He is the model of a brilliant developer who is still kind and approachable. F# has benefited tremendously from his leadership, and I hope to emulate how he treats other developers. ","date":"2023-05-23","objectID":"/blog/2023/05/who-has-inspired-you/:2:0","tags":null,"title":"Who has Inspired You?","uri":"/blog/2023/05/who-has-inspired-you/"},{"categories":null,"content":"Rich Hickey While I have never written any Clojure, I benefited from the presentations that Rich Hickey has made. My favorite is one he gave at Rails Conf titled Simplicity Matters. I have found all of his talks insightful and have opened my mind to new ways of thinking about software development and what we are fundamentally doing at the end of the day. ","date":"2023-05-23","objectID":"/blog/2023/05/who-has-inspired-you/:3:0","tags":null,"title":"Who has Inspired You?","uri":"/blog/2023/05/who-has-inspired-you/"},{"categories":null,"content":"Mark Seemann When I was early on in my exploration of Functional Programming, I found the blog of Mark Seemann. Mark has a way of simplifying complex concepts down to concrete terms that are easy to understand. One of my favorite memories was at the first NDC Minnesota conference. The night before the conference began there was an option to go on a boat tour of St. Paul and have dinner with the speakers. Mark was on the boat and I ended up getting to sit and chat with him for a couple of hours. He was kind and gracious. I remember mentioning to him my struggle with understanding what a Monad is and he said, “It’s just SelectMany.” At that moment, an explosion went off in my mind, and I was like, “Oh, wow. Yeah, that’s all it is!” Mark has always inspired me with his ability to find the right bridge to show people to help them grasp complex concepts. ","date":"2023-05-23","objectID":"/blog/2023/05/who-has-inspired-you/:4:0","tags":null,"title":"Who has Inspired You?","uri":"/blog/2023/05/who-has-inspired-you/"},{"categories":null,"content":"Scott Wlaschin Every F# developer will eventually go on a pilgrimage to “F# for Fun and for Profit”. I haven’t met an F# dev who hasn’t spent some time on Scott’s website. Scott has an incredible ability to translate scary concepts into simple ideas. I place Scott and Mark in the same category of brilliant developers who know how to explain things to us, newbies. Scott is a treasure and one of the key people who has made F# approachable. ","date":"2023-05-23","objectID":"/blog/2023/05/who-has-inspired-you/:5:0","tags":null,"title":"Who has Inspired You?","uri":"/blog/2023/05/who-has-inspired-you/"},{"categories":null,"content":"John Carmack John Carmack needs no introduction. He is considered by many to be one of the best developers alive. If you haven’t read the book “Masters of DOOM,” do yourself a favor and buy it. The audio version is narrated by Will Wheaton, who does a brilliant job. There is one quote that has had the most impact, though. It comes from an interview that Lex Fridman did with John. The whole thing is fantastic (5 hours!?!?), but the quote comes from 5:07:00. “You can’t learn everything, but you have to convince yourself that you can learn anything…” John Carmack The implications of this are massive. Yes, it is insane to think that you can learn everything. Yet, you are fully capable of learning anything, should you choose to apply yourself. It may be difficult. It may take a long time, but it is possible. ","date":"2023-05-23","objectID":"/blog/2023/05/who-has-inspired-you/:6:0","tags":null,"title":"Who has Inspired You?","uri":"/blog/2023/05/who-has-inspired-you/"},{"categories":null,"content":"Jonathan Blow Ah, now it’s getting spicy. Jonathan Blow is a divisive figure. Some people love him; others hate him. He has strong opinions on the state of the software industry and has no problem calling things bullshit. In some ways, Jonathan Blow is the polar opposite of Don Syme. I have a hard time thinking about two people with personalities that are more different. Now, I’m not going to tell you how to think about Jonathan Blow; you are entitled to your opinion, but here’s how he has helped me. As I have been on my journey to learn to write high-performance code, I inevitably found myself looking to game engine development for inspiration. I also research various programming languages to draw inspiration for how to write F#. Eventually, I find Jonathan Blow, who is working on Jai. I don’t know if this is a character flaw or not, but I appreciate people who are willing to speak their minds frankly. I have spent so much time in corporate America and politicking that I find it exhausting. So here is this Jonathan Blow character saying all this Object Oriented (OO) stuff is bullshit and I was like, “Wait? Really? You can say that?” I cannot tell you how dumb OO has made me feel. I despaired early in my career because I failed any time I tried to learn OO or design with OO. I thought I was too dumb to be a “real” programmer. So when I found this Jonathan Blow guy who is clearly very intelligent, blasting OO, I felt liberated. “Maybe I’m not dumb? Maybe I can be a real programmer?” I don’t agree with Jonathan Blow on many points, but he gave me hope that maybe I wasn’t the problem and that maybe OO isn’t the best way to compose software. I wrote him several questions years ago. I didn’t expect to hear back from him. To my surprise, I got a long response answering my questions in detail. It was an incredibly kind and gracious letter—nothing like the bluster that I see on YouTube. ","date":"2023-05-23","objectID":"/blog/2023/05/who-has-inspired-you/:7:0","tags":null,"title":"Who has Inspired You?","uri":"/blog/2023/05/who-has-inspired-you/"},{"categories":null,"content":"Casey Muratori Alright, let’s finish off this list with some additional spice. Casey is famous (infamous?) for speaking out against the insanity that has gripped a great deal of the software industry. His video Clean Code, Horrible Performance made Twitter fun for a few days. In fact, the response video I did to his is still the most watched thing I have made. Where Casey shines, though, is his enormous generosity. He has poured countless hours into creating the Handmade Hero video series to teach people to write a game engine from scratch. He also went ahead and created a course to learn Performance-Aware Programming. Creating educational material is incredibly difficult and time-consuming. It is also typically not a high-paying endeavor. For all of Casey’s technical chops, what really impresses me is his willingness to offer solutions to problems. Instead of just griping about the state of the industry, he actively creates educational material to help people. He puts in the work to help solve the problem. A large part of why I created the “Fast F#” YouTube channel is because Casey inspired me. Instead of just being frustrated with slow F# code, offer tools and resources for people to learn to write fast code. Be the solution. ","date":"2023-05-23","objectID":"/blog/2023/05/who-has-inspired-you/:8:0","tags":null,"title":"Who has Inspired You?","uri":"/blog/2023/05/who-has-inspired-you/"},{"categories":null,"content":"Conclusion There are many more people who have helped and inspired me along the way. I am grateful for the F# community and their patience with my insane questions. There’s a joke in the F# community that people come for the language but stay for the people. I have found this to be true. F# is not a perfect language, not by a long shot. But it is a great language, and there is an incredible community around it. ","date":"2023-05-23","objectID":"/blog/2023/05/who-has-inspired-you/:9:0","tags":null,"title":"Who has Inspired You?","uri":"/blog/2023/05/who-has-inspired-you/"},{"categories":null,"content":"Lately, I’ve been wrestling with the question, “What is a Programming Language?” Now, I’m not talking about what computer science is. As I am not a Computer Scientist myself, I will not hazard to try and answer that question. Feel free to refer to your local CS major to answer that question. I’m concerned with the question of what a Programming Language is. My programming journey began with QBASIC on an old Windows 3.1 operating system. Later in life, I discovered VBA in Excel and SQL for querying databases. Along the way, I dabble in Groovy, struggle with MATLAB, suffer with C++, get lost in C#, and ultimately land on F#. The only reason I am attracted to F# is the Units of Measure feature. I create financial models of supply chains, and keeping track of units can consume significant time. I consistently run into bugs where units are not correctly converted. With F#’s Units of Measure feature, I am confident that my algebra is correct, and I never look back. At this point, I won’t seriously consider another language unless it has some facilities for verifying algebra at compile time. I had started life as a Chemical Engineer (ChemE) but quickly learned that I didn’t want to babysit tools in a clean room for the rest of my life. During my internships, I see that companies are good at keeping their tools running but struggle to schedule their facility to maximize efficiency. For this reason, I return to school to study Industrial Engineering (IE). I remember one of my ChemE professors chiding me, saying, “Matthew, you know we call IE Imaginary Engineering, right?” Despite my ChemE professor’s low opinion of IE’s, I decide to continue. Now armed with a degree in IE, I quickly find that the companies I work with lack the staffing to build the scheduling tools they need. I know the algorithm to solve their problem, but they don’t have a programmer to implement it. Undaunted, I bought some books and started hacking away. As I keep building new solutions and evolving my skills, I start thinking about the design of programming languages. I’ll be honest; when I learned F#, I had no idea what a lambda function was or why someone should care. It’s funny now how indifferent I was to all the features many people rave about when it comes to F#. Higher-Order Functions? Whatever. Immutability? Shrug. I had no idea why F# worked so well for my problems, I just knew that it did, and that was enough for me. ","date":"2023-05-08","objectID":"/blog/2023/05/what-is-a-programming-language/:0:0","tags":null,"title":"What is a Programming Language?","uri":"/blog/2023/05/what-is-a-programming-language/"},{"categories":null,"content":"Complexity Drives Growth Over time I begin tackling more complex problems. I am forced to grow my understanding of how F#, and programming languages in general, work. I start to appreciate the beauty of F#. I realize why libraries are formed the way they are and how these functions and types interact. I am learning about Higher-Order Functions, Functors, Monads, and the beautiful world of Category Theory. In my search, I come across Bartosz Milewski and his excellent book “Category Theory for Programmers.” I think I have come to the holy land. Category Theory is the answer! If we have an expressive enough type system, we can solve all our problems. I form the opinion that a programming language is a means of expressing a domain and the transformations of that domain. Programs are wrong because we lack the facilities to express these relationships. A good programming language allows us to fully express the problem domain in a way that makes incorrect states unrepresentable. I join the bandwagon of F# developers who call for adding Higher-Kinded Types and Types Classes to F#. To my dismay, these requests are turned down. I begin to form a grudge against F#. I don’t say anything because I make my living writing F#, and you don’t poison the well you drink from. I believe that F# is an inferior programming language, and I begin looking at Haskell wistfully. “My life would be so much better if I could just work in a ‘real’ programming language,” I think. I spend several years in this state. Begrudging the “limitations” of F# but still being wildly productive, despite my frustrations. I consistently run into roadblocks with the type system. If the F# Slack had enough history, you could go and see some of my frustrations spill over. Time and again, I am forced to engineer around the “shortcomings” of F#. ","date":"2023-05-08","objectID":"/blog/2023/05/what-is-a-programming-language/:1:0","tags":null,"title":"What is a Programming Language?","uri":"/blog/2023/05/what-is-a-programming-language/"},{"categories":null,"content":"Performance, the Harsh Tutor At this point, I have a high opinion of myself. I view myself as a competent F# developer, and some scars from having to build and maintain some large, complex pieces of code. I think that I’m decent at my job and can deliver features in a reasonable amount of time. I mentor several junior developers and see them become successful developers in their own right. I must be a real Senior developer at this point, right? I have always been a performance junkie, but most of my efforts have been focused on the performance of supply chains and manufacturing facilities. I had never turned my gaze to the performance of programming languages and computers. I know some languages are slow (Python, Ruby, VBA), and some are fast (C, C++, FORTRAN). But I have never dove into why this is the case. I have never run into a situation where the performance of my code is a problem. Almost all the performance issues I have encountered have been because of slow SQL queries or too many network calls. Then one day, I am working on a problem where a service suddenly sees the time to complete an analysis take twice as long. No one on the team seems to know why so I volunteer to diagnose the issue. I begin benchmarking the service, trying to find the root cause. After a day, I realize that something is happening when the number of items exceeds a certain threshold. I find that odd, so I start profiling. Eventually, I found that our code spends much of the time adding items to a dictionary. I continue pulling on the thread and find that .NET has some interesting behavior regarding GetHashCode and equality. It turns out all of the items being added to the Dictionary are hashing to the same bucket in the Dictionary, so our Dictionary has turned into a List. O(1) operations are now O(n) which is why our service performance is blowing up. How can something as simple as hash codes and equality cause such a performance difference? Little did I know that I was beginning to tumble down the rabbit hole after Alice. I have no idea how deep of a hole I have stepped into. F# has fantastic defaults which guide you down the pit of success. I could be a successful developer without understanding how things worked. Until now, I was so confident in my grasp of types, functions, and Category Theory that I thought I could tackle most problems. What I have failed to grasp up to this point is that a programming language is not just a tool for expressing a problem domain; it is a means of communicating with a computer. It seems so obvious in retrospect it is embarrassing. I laugh at myself now for being so profoundly naïve. Until now, the hardware was wholly abstracted away, so I did not even need to think about it. Now that I saw the implications, I could not unsee it. By seeking to understand how F# works, I am suddenly exposed to a whole world of design that I never thought about. I dive into how CPUs operate, and I see that they are data factories, not unlike the manufacturing facilities that I am spending all my time optimizing. Data moves into a register, operations are applied to the data, and then it is returned to memory. You can think of products being manufactured in a facility similarly. Raw material moves onto a machine, the machine applies a transformation, and the updated material is then moved to a new operation. A new paradigm for thinking about programming languages emerges, “This is just a tool for expressing operations over data.” I am mad at myself for being so thoroughly entranced by the ideas of Category Theory. All this abstraction is just obfuscating what I am trying to do. I have data; I want to transform that data and then present the result. Nothing more, nothing less. How much time have I wasted trying to create the perfect Monad? Who cares whether some types form a Bifunctor or not? I need to transform some data and get on with my life. ","date":"2023-05-08","objectID":"/blog/2023/05/what-is-a-programming-language/:2:0","tags":null,"title":"What is a Programming Language?","uri":"/blog/2023/05/what-is-a-programming-language/"},{"categories":null,"content":"Synthesis I now disdain my years of loving Category Theory and being entranced by the idea of eliminating bugs through expressive type systems. I am now obsessed with performance and how to extract the most performance from a computer. I take pride in the fact that my simulations are several orders of magnitude faster than other industry-leading tools. I read every resource I can on game engines and how C developers extract the most performance possible from a CPU. I am still working in F# but writing a very different F#. I am keenly aware of when I am passing a value or a reference. I see where memory is being copied, and allocations are happening. I stay off of the Heap as much as possible. I am thinking about registers and the L1 cache. I am measuring branch mispredictions and cache misses. I view any inefficiency as a failure, but at the end of the day, I’m exhausted. Becoming aware of how CPUs work is both a blessing and a curse. It is a blessing that you now know how to make your code orders of magnitude faster. It is a curse in that you see inefficiency everywhere. At the beginning of my degree in IE, my graduate advisor Dr. Kim warned me, “Matthew, once you learn how to design an efficient facility, you are going to start seeing inefficiency everywhere in the world…and it will drive you mad.” Well, Dr. Kim, you were right. I see it. I see it in supply chains and in my code, and yes, I have gone mad. I have come to a new place now. I no longer need every line of code I write to be the most efficient thing possible. I see Category Theory’s merits and how it helps us write correct code. At the same time, I have to synthesize that with my understanding of how a CPU works. When I think about the original question, “What is a programming language?” I have to take a step back. On the one hand, it is a tool of thought for expressing a problem domain. A well-formed set of types and functions can make it easy to solve a problem and maintain robust code. On the other hand, a programming language is a tool for us to harness the power of computers. It allows us to transform data to answer questions. A great programming language allows us to do both. We can both accurately describe our domain so that other humans can understand our code, while computers can efficiently compute the results. It is not either/or; it’s both/and. No language is perfect. All languages are a product of compromises. So I continue to learn to thread the needle between elegant domain representations and efficient code. It is staggering how far programming languages have come in the last few decades. I, for one, look forward to what beautiful things that may come. Thoughts? Email me at hi@fastfsharp.com to continue the conversation 😊. ","date":"2023-05-08","objectID":"/blog/2023/05/what-is-a-programming-language/:3:0","tags":null,"title":"What is a Programming Language?","uri":"/blog/2023/05/what-is-a-programming-language/"},{"categories":null,"content":"I have been puzzling over programming languages and performance and I want to put my thoughts out and invite feedback. We are at an interesting place in the software industry. Software development, as a profession, is not even a century old yet, but there are many strong opinions on the “right” way to write software. Object Oriented Programming (OOP) is the dominant paradigm but Functional Programming (FP), or at least Functional Style, has seen increased interest. I also have a narrow view of the industry so perhaps my perceptions are off. While our hardware has never been faster, the software is often derided as slow and buggy, and rightly so. CPUs have grown in performance by several orders of magnitude, but we don’t feel that when we use the software. Some say that we are just tackling more complex problems and this is the price that we pay. I’m not sure the difficulty of our problems has grown at the same pace as computer performance. For perspective, the Intel 386 is a consumer chip that existed when I was a child. It had a single core that ran at up to 40 MHz. Today, I am writing this on a PC with an Intel 13900K with 8 Performance cores that can sustain 5.4 GHz together. Taken together, that gives a theoretical performance improvement of over 1,000x. That says nothing of the advances in chip design when it comes to branch prediction, out-of-order execution, cache improvements, and SIMD. Are we really tackling problems that are over 1,000x more difficult than we were a few decades ago? Really? ","date":"2023-05-04","objectID":"/blog/2023/05/what-is-going-on-with-software/:0:0","tags":null,"title":"What is Going On With Software?","uri":"/blog/2023/05/what-is-going-on-with-software/"},{"categories":null,"content":"The Siren Call of Abstraction My perception of what happened is that programmers got expensive, so businesses wanted tools that made developers more productive. Languages were created that promised to make developers more productive. Early on this looked like the C programming language which freed the developer from thinking about the particular machine assembly they were programming for. Later we added Polymorphism and Interfaces so we could abstract over types. Along the way, we decided we didn’t want to worry about memory so we added technologies for automatic memory management with Garbage Collectors and Reference Counting. We then gained the ability to create and share libraries so we could just build on someone else’s abstractions. We just kept adding abstractions to continue to “simplify” things for developers. In the meantime, almost all awareness of the physical hardware our software is running on has been lost. It has gotten to the point where I regularly talk with developers who have no concept of the memory hierarchies inside an x86 chip. And you know what? I don’t blame them. The vast majority of the industry says it doesn’t matter. CPUs are fast, and compilers are good, so all that magic will make the problem disappear. What strikes me odd about that is that in no other engineering discipline do they abstract away physical reality when they think about the problem. Civil Engineers designing a bridge must consider the terrain where the bridge will be built. Industrial Engineers take into account a building’s physical layout when designing workstations. Chemical Engineers consider the ambient environment when designing processes so that the chemistry actually works. Only in Software Engineering do we ignore the physical limitations of our systems. The problem is that ignoring reality and embracing abstraction worked. It worked really well. Programmers could crank out features faster and managers were delighted. We were still riding the wild gains in performance that chip makers were cranking out so if something was slow today, it would be fast enough in 6 months. We were drinking deeply of the wine of Abstraction and times were good. ","date":"2023-05-04","objectID":"/blog/2023/05/what-is-going-on-with-software/:1:0","tags":null,"title":"What is Going On With Software?","uri":"/blog/2023/05/what-is-going-on-with-software/"},{"categories":null,"content":"Abstraction Hangover Fast forward a couple of decades and we are no longer seeing these leaps in hardware performance but the software industry is still hooked on the drug of abstraction. The old adage of, “Every problem in computer science is solved with another layer of abstraction,” is still going strong. And as much as I would like to deride this, it’s done much good. I never would have become a developer if it wasn’t for F# which is far removed from physical hardware. Tools like Excel and VBA have enabled countless people to write just a little bit of code to make their lives enormously better. We cannot deny that a significant amount of good has come from this. There is a real problem, though. Software is getting slower and buggier. It is becoming more and more of a pain to interact with software systems. We have schools that can’t turn off their lights due to bugs in the software. We had an upset in the JavaScript ecosystem when the left-pad library was removed from NPM which is an example of over-reliance on someone else’s abstraction. Perhaps we need to introspect about the quality of the foundations we are building on. These abstractions have been built up so high that a single person cannot understand them all. The software developer field has fractionated into a myriad of separate disciplines and areas of focus. ","date":"2023-05-04","objectID":"/blog/2023/05/what-is-going-on-with-software/:2:0","tags":null,"title":"What is Going On With Software?","uri":"/blog/2023/05/what-is-going-on-with-software/"},{"categories":null,"content":"Personal Reflection When I think about my own career and look at the software development industry I am concerned. I am concerned because I don’t want to add to this mess. I want to be helping. I want to write quality software. I am asking myself, “Am I addicted to Abstraction?” This is a difficult thing for me to unpack because my ability to feed my family is currently tied to my ability to write software. It doesn’t surprise me when people bristle when Casey Muratori calls out Clean Code as bad for performance. It’s no surprise that Jonathan Blow gets so much heat when he goes on one of his tirades about the state of the industry. They are rightly pointing out that the software industry is in a dismal place but people’s well-being is depending on their developer job so they lash back. We’ve overbuilt our abstractions but we are all still being paid so we don’t want to upset the boat. Aside: I’m a fan of Casey and Jon. I don’t know them personally, but I’ve learned a great deal from them. At the same time, I am a beneficiary of these abstractions. I wouldn’t have my job without them. I tried learning C years ago and it was brutal. C++ hurt me so badly that I swore I’d never write it again. I don’t think the solution is for everyone to return to C. I don’t think we just throw out all of the progress and innovation that has occurred. Perhaps we need to make it easier for people to work at a low level. This is why I am a fan of Rust because it seems to be a new way to think about how we let people work at a low level safely. Personally, I’m working on how to make F# a language to write high-performance code in an easy and concise way. I constantly strive to write code that is simple and fast. That works in harmony with the CPU, not just ignoring it. I like how .NET has been taking performance more seriously, and I hope that trend continues. I think we need to see a renaissance in software development where we start peeling back some of these layers of abstraction. We need to make it easier to manage memory. We need to explore new paradigms which allow us to work in harmony with the hardware and not just abstract it away. ","date":"2023-05-04","objectID":"/blog/2023/05/what-is-going-on-with-software/:3:0","tags":null,"title":"What is Going On With Software?","uri":"/blog/2023/05/what-is-going-on-with-software/"},{"categories":null,"content":" This article is for the 2022 F# Advent Calendar Full code for article found here While building the Aidos engine for Discrete Rate Simulation, I often have to build custom collections to meet the project’s performance requirements. Don’t get me wrong, many of the collections built into .NET are great, but they are general-purpose. General-purpose collections must meet the requirements of many use cases. I have a single use case in mind for my work, and performance is one of the critical features. If our engine isn’t orders of magnitude faster than the competition, we don’t have a compelling product. In Aidos, I often need to track items that have changed during a time step of the simulation. I also track entities with an int that has been annotated with a Unit of Measure. This means an entity ends up being an int\u003c'EntityType\u003e. I also cannot have duplicates for my use case, so I need to maintain a distinct set of entities that have changed. One way to do this would be to use a HashSet. HashSet is built into .NET and provides O(1) insertion, which you would think would be ideal for this use case. The downside to a HashSet is that its memory will be allocated on the heap. If you have to create a HashSet for every iteration of a hot loop, this can cause excess GC pressure. Instead, what I use is a custom BitSet. BitSet is a struct that wraps an array of uint64, which acts as a bit array that I manually manage. The .NET runtime has a BitArray class, but it does not provide the API I need for my use cases. I need to iterate through all the set bits and call a function with the index of the set bit as an argument. One of the advantages of the BitSet approach over HashSet is that the array used by the BitSet can be allocated from an ArrayPool, which means that BitSet will never increase GC pressure or take up room on the heap. The other is that it is faster than HashSet for iterating through the set bits using the BitSet.iter function. Here is the definition of the BitSet type: [\u003cStruct\u003e] type BitSet\u003c[\u003cMeasure\u003e] 'Measure\u003e(buckets: uint64[]) = new(capacity: int) = let bucketsRequired = (capacity + 63) \u003e\u003e\u003e 6 let buckets: uint64[] = Array.zeroCreate bucketsRequired BitSet\u003c_\u003e buckets /// WARNING: Public for inlining member _._buckets = buckets member _.Capacity = buckets.Length * 64 member b.Count = let mutable total = 0 for bucket in b._buckets do total \u003c- total + System.Numerics.BitOperations.PopCount bucket total member b.Item with get (itemKey: int\u003c'Measure\u003e) = let bucketId, mask = Helpers.computeBucketAndMask itemKey let buckets = b._buckets let bucket = buckets[bucketId] (bucket \u0026\u0026\u0026 mask) \u003c\u003e 0UL member b.Contains(itemKey: int\u003c'Measure\u003e) = let bucketId, mask = Helpers.computeBucketAndMask itemKey let buckets = b._buckets let bucket = buckets[bucketId] (bucket \u0026\u0026\u0026 mask) \u003c\u003e 0UL member b.Add(itemKey: int\u003c'Measure\u003e) = let bucketId, mask = Helpers.computeBucketAndMask itemKey let bucket = buckets[bucketId] buckets[bucketId] \u003c- bucket ||| mask member b.Remove(itemKey: int\u003c'Measure\u003e) = let bucketId, mask = Helpers.computeBucketAndMask itemKey let buckets = b._buckets let bucket = buckets[bucketId] buckets[bucketId] \u003c- bucket \u0026\u0026\u0026 ~~~mask We also have a BitSet module where we define the functions for operating on BitSet. Here I show just the iter function. iter loops through each set bit in the array and calls the lambda f with the index of the set bit as the argument. module BitSet = let inline iter ([\u003cInlineIfLambda\u003e] f: int\u003c'Measure\u003e -\u003e unit) (b: BitSet\u003c'Measure\u003e) = let mutable i = 0 // Source of algorithm: https://lemire.me/blog/2018/02/21/iterating-over-set-bits-quickly/ while i \u003c b._buckets.Length do let mutable bitSet = b._buckets[i] while bitSet \u003c\u003e 0UL do let r = System.Numerics.BitOperations.TrailingZeroCount bitSet let itemId = (i \u003c\u003c\u003c 6) + r |\u003e LanguagePrimitives.Int32WithMeasure\u003c'Measure\u003e (f itemId) bitSet \u003c- bitSet ^^^ (1UL \u003c\u003c\u003c r) i \u003c- i + 1 I set up a benchmark to compare the performance of HashSet and BitSet. I define a unit of","date":"2022-12-01","objectID":"/blog/2022/12/fast-enumeration-of-bitset/:0:0","tags":null,"title":"Fast Iteration of Set Bits in BitSet","uri":"/blog/2022/12/fast-enumeration-of-bitset/"},{"categories":null,"content":"The Problem Now, you may be thinking that BitSet is great, but there is a downside to this approach. The iter function takes a lambda as one of its arguments. Whenever the BitSet encounters a set bit, it then calls the lambda with the index of the set bit. Lambdas are intrinsic to programming in F#, but they have limitations. One of those limitations is that they cannot capture Span\u003c'T\u003e or ByRefLike types. Most of the time, this is not a big deal. F# developers are not often known as hardcore performance programmers, so most F# developers will not run into this problem. I, on the other hand, work with Span\u003c'T\u003e and ByRefLike types all the time. They can be incredibly powerful for increasing your program’s performance and decreasing memory allocations. Now, a word of caution. You probably don’t need this. You can lead a very happy life as an F# developer, and never worry about this. This limitation only becomes an issue when you are trying to eke out every bit of performance you can, and you are likely not in that scenario. I happen to be in an odd situation because I work for a company with an F# dev team, and I’m tasked with writing libraries for others to use that must be fast. For strategic reasons, we constrain ourselves to F#, so calling out to C/C++/Rust is not an option. You will appreciate what we cover next if you find yourself in a similar situation. I have asked about relaxing some of the compiler restrictions around lambdas and capturing Span\u003c'T\u003e, but the effort would be large. The more I dug into how the F# compiler and the CLR interact, my appreciation for the complexity of the problem grows. This is also not the most important feature for the growth of F#, so I’m not going to push for it. I hope to get good enough to contribute it to the F# compiler someday 😊. ","date":"2022-12-01","objectID":"/blog/2022/12/fast-enumeration-of-bitset/:1:0","tags":null,"title":"Fast Iteration of Set Bits in BitSet","uri":"/blog/2022/12/fast-enumeration-of-bitset/"},{"categories":null,"content":"The Solution So how do we get around this limitation? BitSet is intended for these hot loops where we likely want to be able to use some stack-allocated memory. This means we must be able to work with BitSet and Span\u003c'T\u003e simultaneously. The simple solution is to expose a new way of iterating through the set bits in the BitSet. We can implement IEnumerable\u003c'T\u003e for BitSet and use a for...in...do loop. The easiest way to implement IEnumerable\u003c'T\u003e for BitSet is to define a BitSetEnumerator, which takes the logic used in the iter function but exposes it in a way that the IEnumerable\u003c'T\u003e interface expects. Let’s see what that looks like: type BitSetEnumerator\u003c[\u003cMeasure\u003e] 'Measure\u003e(buckets: uint64[]) = let mutable bucketIdx = 0 let mutable curBucket = 0UL let mutable curItem = LanguagePrimitives.Int32WithMeasure\u003c'Measure\u003e -1 member _.Current = if curItem \u003c 0\u003c_\u003e then raise (InvalidOperationException \"Enumeration has not started. Call MoveNext.\") else curItem member b.MoveNext() = // Check if we have actually started iteration if curItem \u003c 0\u003c_\u003e then curBucket \u003c- buckets[bucketIdx] // There are still items in the Current bucket we should return if curBucket \u003c\u003e 0UL then let r = System.Numerics.BitOperations.TrailingZeroCount curBucket curItem \u003c- LanguagePrimitives.Int32WithMeasure\u003c'Measure\u003e((bucketIdx \u003c\u003c\u003c 6) + r) curBucket \u003c- curBucket ^^^ (1UL \u003c\u003c\u003c r) true // We need to move to the next bucket of items else bucketIdx \u003c- bucketIdx + 1 if bucketIdx \u003c buckets.Length then curBucket \u003c- buckets[bucketIdx] b.MoveNext() else false member _.Reset() = bucketIdx \u003c- 0 curBucket \u003c- 0UL curItem \u003c- LanguagePrimitives.Int32WithMeasure\u003c'Measure\u003e -1 interface IEnumerator\u003cint\u003c'Measure\u003e\u003e with member b.Current = b.Current :\u003e Object member b.Current = b.Current member b.MoveNext() = b.MoveNext() member b.Reset() = b.Reset() member b.Dispose() = () The BitSetEnumerator type defines three methods for fulfilling the IEnumerable\u003c'T\u003e contract: Current, MoveNext, and Reset. You can see how the BitSetEnumerator fulfills the IEnumerable\u003c'T\u003e interface at the bottom. The type uses the same bit-shifting logic iter uses but breaks it up to support the methods that IEnumerable\u003c'T\u003e expects. We can then have the BitSet collection implement the IEnumerable\u003c'T\u003e interface by returning an instance of the BitSetEnumerator when calling the GetEnumerator method. open System.Collections.Generic [\u003cStruct\u003e] type BitSet\u003c[\u003cMeasure\u003e] 'Measure\u003e(buckets: uint64[]) = // Previous logic here interface System.Collections.IEnumerable with member b.GetEnumerator() = (new BitSetEnumerator\u003c'Measure\u003e(buckets)) :\u003e System.Collections.IEnumerator interface IEnumerable\u003cint\u003c'Measure\u003e\u003e with member s.GetEnumerator() = new BitSetEnumerator\u003c'Measure\u003e(buckets) When we add this approach to the benchmarks, we get the following result: | Method | Mean | Error | StdDev | Gen0 | Allocated | |----------- |----------:|----------:|----------:|-------:|----------:| | HashSet | 23.347 ns | 0.4024 ns | 0.3764 ns | - | - | | Iter | 7.087 ns | 0.1000 ns | 0.0935 ns | - | - | | Enumerable | 55.521 ns | 0.9228 ns | 0.8181 ns | 0.0048 | 40 B | The IEnumerable\u003c'T\u003e approach is twice as slow as using a HashSet. This is less than ideal. It is also allocating on the heap. This is because the interface necessitates the creation of an object on the heap. We’ve negated most, if not all, of the benefits we hope to get from BitSet. What can we do? ","date":"2022-12-01","objectID":"/blog/2022/12/fast-enumeration-of-bitset/:2:0","tags":null,"title":"Fast Iteration of Set Bits in BitSet","uri":"/blog/2022/12/fast-enumeration-of-bitset/"},{"categories":null,"content":"Ducks All The Way Down There’s a feature of .NET that I don’t hear about much but is important in this scenario. The .NET runtime will use duck-typing to implement C# foreach loops and their equivalents. The runtime will look at the type and see if it has a GetEnumerator method that returns a type with the Current field and the MoveNext method. Well, the for...in...do loop in F# is the equivalent to the C# foreach loop. What if instead of implementing IEnumerable\u003c'T\u003e we rely on the .NET duck-typing approach? We can change our enumerator to be a struct so that it doesn’t allocate any memory on the heap, and we’ll avoid the overhead of an interface. Here’s what the BitSetEnumerator looks like as a struct with only the necessary pieces for duck-typing. [\u003cStruct\u003e] type BitSetEnumerator\u003c[\u003cMeasure\u003e] 'Measure\u003e = val mutable BucketIdx: int val mutable CurBucket: uint64 val mutable CurItem: int\u003c'Measure\u003e val Buckets: uint64[] new(buckets: uint64[]) = { BucketIdx = 0 CurBucket = 0UL CurItem = LanguagePrimitives.Int32WithMeasure\u003c'Measure\u003e -1 Buckets = buckets } member b.Current = if b.CurItem \u003c 0\u003c_\u003e then raise (InvalidOperationException \"Enumeration has not started. Call MoveNext.\") else b.CurItem member b.MoveNext() = // Check if we have actually started iteration if b.CurItem \u003c 0\u003c_\u003e then b.CurBucket \u003c- b.Buckets[b.BucketIdx] // There are still items in the Current bucket we should return if b.CurBucket \u003c\u003e 0UL then let r = System.Numerics.BitOperations.TrailingZeroCount b.CurBucket b.CurItem \u003c- LanguagePrimitives.Int32WithMeasure\u003c'Measure\u003e((b.BucketIdx \u003c\u003c\u003c 6) + r) b.CurBucket \u003c- b.CurBucket ^^^ (1UL \u003c\u003c\u003c r) true // We need to move to the next bucket of items else b.BucketIdx \u003c- b.BucketIdx + 1 if b.BucketIdx \u003c b.Buckets.Length then b.CurBucket \u003c- b.Buckets[b.BucketIdx] b.MoveNext() else false Things look a bit different since BitSetEnumerator is now a struct and therefore requires different approaches to handling the internal data. We also change the BitSet type to only have a GetEnumerator() method instead of implementing IEnumerable\u003c'T\u003e. [\u003cStruct\u003e] type BitSet\u003c[\u003cMeasure\u003e] 'Measure\u003e(buckets: uint64[]) = // Previous logic member b.GetEnumerator() = BitSetEnumerator\u003c'Measure\u003e(buckets) When we benchmark this approach, we get the following: | Method | Mean | Error | StdDev | Gen0 | Allocated | |----------- |----------:|----------:|----------:|-------:|----------:| | HashSet | 23.347 ns | 0.4024 ns | 0.3764 ns | - | - | | Iter | 7.087 ns | 0.1000 ns | 0.0935 ns | - | - | | Enumerable | 55.521 ns | 0.9228 ns | 0.8181 ns | 0.0048 | 40 B | | DuckTyping | 28.039 ns | 0.5183 ns | 0.4848 ns | - | - | This is much better. Our performance is almost that of a HashSet. Something to be aware of, the duck-typing approach and the IEnumerable\u003c'T\u003e are not mutually exclusive. If you implement both, the runtime will pick the faster approach in the testing I have done. In the production code, we include both because the IEnumerable\u003c'T\u003e is necessary for using the BitSet with the Seq module. ","date":"2022-12-01","objectID":"/blog/2022/12/fast-enumeration-of-bitset/:3:0","tags":null,"title":"Fast Iteration of Set Bits in BitSet","uri":"/blog/2022/12/fast-enumeration-of-bitset/"},{"categories":null,"content":"Inline All The Things (When it helps) You have probably noticed that our loop-based approach’s performance is still not near the iter method. That’s to be expected. The for-loop approach adds overhead to the iteration. The F# compiler has some special transforms that it does for arrays which makes using a for...in...do loop over the elements of an array incredibly fast, but most other collections do not get that special treatment. There is something we can do to get a little more performance, though. Right now, each time the MoveNext method is called, it creates a new stack frame. This adds overhead to the loop when it has to copy data for each instance of the stack frame. If we could inline the logic of the MoveNext method, we could reduce the number of stack frames created and potentially get a performance boost. If you try to add the inline keyword to Current and MoveNext on BitSetEnumerator, you will have a problem. The compiler will give you an error that looks something like this: D:\\Documents\\GitHub\\FSharpPerformance\\BitSetEnumeration\\DuckTyping.fs(55,17): error FS1114: The value 'BitSetEnumeration.DuckTyping.BitSetEnumerator.MoveNext' was marked inline but was not bound in the optimi zation environment [D:\\Documents\\GitHub\\FSharpPerformance\\BitSetEnumeration\\BitSetEnumeration.fsproj] D:\\Documents\\GitHub\\FSharpPerformance\\BitSetEnumeration\\DuckTyping.fs(37,21): error FS1113: The value 'MoveNext' was marked inline but its implementation makes use of an internal or private function which is not sufficiently accessible [D:\\Documents\\GitHub\\FSharpPerformance\\BitSetEnumeration\\BitSetEnumeration.fsproj] D:\\Documents\\GitHub\\FSharpPerformance\\BitSetEnumeration\\DuckTyping.fs(55,17): warning FS1116: A value marked as 'inline' has an unexpected value [D:\\Documents\\GitHub\\FSharpPerformance\\BitSetEnumeration\\BitSet Enumeration.fsproj] D:\\Documents\\GitHub\\FSharpPerformance\\BitSetEnumeration\\DuckTyping.fs(55,17): error FS1118: Failed to inline the value 'MoveNext' marked 'inline', perhaps because a recursive value was marked 'inline' [D:\\Doc uments\\GitHub\\FSharpPerformance\\BitSetEnumeration\\BitSetEnumeration.fsproj] The build failed. Fix the build errors and run again. That looks like a lot of garbage, but the important part is near the end. It reports an error on line 55 of our DuckTyping.fs, which mentions “perhaps because a recursive value was marked inline.” That’s the clue we need. The MoveNext method is recursive at the moment, so the inlining logic of the F# compiler cannot work. What we need to do is remove this recursion. When we remove the recursion from the MoveNext method, we get the following: member inline b.MoveNext() = // Check if we have actually started iteration if b.CurItem \u003c 0\u003c_\u003e then b.CurBucket \u003c- b.Buckets[b.BucketIdx] // There are still items in the Current bucket we should return if b.CurBucket \u003c\u003e 0UL then let r = System.Numerics.BitOperations.TrailingZeroCount b.CurBucket b.CurItem \u003c- LanguagePrimitives.Int32WithMeasure\u003c'Measure\u003e((b.BucketIdx \u003c\u003c\u003c 6) + r) b.CurBucket \u003c- b.CurBucket ^^^ (1UL \u003c\u003c\u003c r) true // We need to move to the next bucket of items else b.BucketIdx \u003c- b.BucketIdx + 1 let mutable result = false while b.BucketIdx \u003c b.Buckets.Length \u0026\u0026 (not result) do b.CurBucket \u003c- b.Buckets[b.BucketIdx] // There are still items in the Current bucket we should return if b.CurBucket \u003c\u003e 0UL then let r = System.Numerics.BitOperations.TrailingZeroCount b.CurBucket b.CurItem \u003c- LanguagePrimitives.Int32WithMeasure\u003c'Measure\u003e((b.BucketIdx \u003c\u003c\u003c 6) + r) b.CurBucket \u003c- b.CurBucket ^^^ (1UL \u003c\u003c\u003c r) result \u003c- true if not result then b.BucketIdx \u003c- b.BucketIdx + 1 result The logic for moving to the next bucket and checking for values has gotten more complex, but it no longer recurses. This allows us to use the inline keyword to get the F# compiler to inline this logic where it is used. This will reduce the number of stack frames used in our loop. Let’s see what the performance of this version is: | Method | Mean | Error | StdD","date":"2022-12-01","objectID":"/blog/2022/12/fast-enumeration-of-bitset/:4:0","tags":null,"title":"Fast Iteration of Set Bits in BitSet","uri":"/blog/2022/12/fast-enumeration-of-bitset/"},{"categories":null,"content":"Conclusion You’ve learned a little about implementing IEnumerable\u003c'T\u003e for custom collections that you write and how to use the duck-typing of the foreach loop in .NET to get even better performance. We’ve also shown that we can perform even better using the inline keyword to remove stack frames. I recommend that you stick with the built-in looping functions provided by F#: map, iter, iteri, etc. They are highly optimized and will give you great performance out of the box. In rare cases, you should consider other options where the need to capture a Span\u003c'T\u003e or another restriction forces you to use other looping constructs. I hope you find this helpful. Please feel free to reach out with any questions or critiques 😊. ","date":"2022-12-01","objectID":"/blog/2022/12/fast-enumeration-of-bitset/:5:0","tags":null,"title":"Fast Iteration of Set Bits in BitSet","uri":"/blog/2022/12/fast-enumeration-of-bitset/"},{"categories":null,"content":"Part of my work is writing algorithms to analyze networks of nodes representing manufacturing systems. Each node can be one of four different types: Buffer, Constraint, Merge, and Split. A manufacturing system that we would want to simulate is typically made up of no more than 100 of these nodes. A natural way to encode these types would be to use Discriminated Unions (DU). I also use Units of Measure to annotate integers that are the ids for these manufacturing nodes. [\u003cMeasure\u003e] type BufferId [\u003cMeasure\u003e] type ConstraintId [\u003cMeasure\u003e] type SplitId [\u003cMeasure\u003e] type MergeId [\u003cRequireQualifiedAccess\u003e] type Node = | BufferId of bufferId : int\u003cBufferId\u003e | ConstraintId of constraintId : int\u003cConstraintId\u003e | MergeId of mergeId : int\u003cMergeId\u003e | SplitId of splitId : int\u003cSplitId\u003e The default encoding for a DU in F# is as a reference type. This means that if you have a Node array, each array element will be a pointer to where the Node itself is stored in memory. If you need to quickly lookup up Nodes and what type they are, you can run into a phenomenon known as Pointer Chasing. Pointer Chasing is when the CPU is trying to run your code but constantly has to look up new regions of memory because the data is spread out. As .NET developers, we tend not to think about this much, but it can become a severe problem in performance-sensitive code. Fortunately, F# allows us to encode DUs as structs using the [\u003cStruct\u003e] attribute. Here is what that looks like: [\u003cStruct; RequireQualifiedAccess\u003e] type Node = | BufferId of bufferId : int\u003cBufferId\u003e | ConstraintId of constraintId : int\u003cConstraintId\u003e | MergeId of mergeId : int\u003cMergeId\u003e | SplitId of splitId : int\u003cSplitId\u003e Now, if you have a Node array, the data for the node will be stored in the array itself so you can eliminate having to perform an additional lookup. There is a serious downside to this, though. The F# compiler will allocate space for each possible case of the DU instead of only the area necessary for the instantiated individual case. This means that instead of just taking up the space of just two int (one to encode the case and one for the value), this struct Node will take up one int to encode the case and four more int for each possible case. For a deeper explanation of this, I refer you to this excellent post by Bartosz Sypytkowski. If a DU has too many cases, the benefits of the struct layout will quickly be negated by this padding. ","date":"2022-03-20","objectID":"/blog/2022/03/performance-of-dus-and-active-patterns/:0:0","tags":null,"title":"Performance of Discriminated Unions and Active Patterns","uri":"/blog/2022/03/performance-of-dus-and-active-patterns/"},{"categories":null,"content":"Alternative Encoding I was curious if there was another way to approach my problem. I like the elegance of the match ... with syntax in F#, and I am loathed to give it up. Since my Node type is just encoding different possible int values, why not do some bit hacking? Now, I will be the first to say this is non-traditional F# code, but I’m curious, so why not perform the experiment? I’ll define a new version of Node that will use an int to hold the data about which case it represents and the value. I will use the last 4 bits of the Value field to encode which case the Node represents and the top 28 bits will hold the id value. This is cutting off some of the space that int can express, but since our networks are never more than 1,000 nodes, there is no practical loss of modeling space. [\u003cStruct\u003e] type Node = private { Value : int } static member BufferIdCode = 0 static member ConstraintIdCode = 1 static member MergeIdCode = 2 static member SplitIdCode = 3 The static members BufferIdCode, ConstraintIdCode, MergeIdCode, and SplitIdCode, will be the values I use to encode the Node cases. To still use the match...with syntax, I will need to define an Active Pattern for unpacking the case. Active Patterns are an elegant feature of F# for decomposing data into different forms. In this case, I will take the int held in my Node type, check which case it is, and then return the corresponding id. I use a mask and a bitwise AND operation to get the last 4 bits (also known as a nibble) of the Value field, which gives me an int. I compare that int with the possible code values to figure out which type of node it is. I then bit shift the Value field to the right 4 bits to convert it to the id value it stores and multiply the result by the right Unit of Measure to ensure type safety. let (|BufferId|ConstraintId|SplitId|MergeId|) (node: Node) = // Get the nibble which encodes the type of Id let nodeType = node.Value \u0026\u0026\u0026 0x0000000F // Get the value of the Id let idValue = node.Value \u003e\u003e\u003e 4 if nodeType = Node.BufferIdCode then BufferId (idValue * 1\u003cBufferId\u003e) elif nodeType = Node.ConstraintIdCode then ConstraintId (idValue * 1\u003cConstraintId\u003e) elif nodeType = Node.MergeIdCode then MergeId (idValue * 1\u003cMergeId\u003e) elif nodeType = Node.SplitIdCode then SplitId (idValue * 1\u003cSplitId\u003e) else invalidArg (nameof node) \"Node Id type does not match known Node Types\" There is a downside to this technique, though. Active Patterns will cause additional allocations and trigger additional Garbage Collection (GC). Fortunately, F# recognized this and enabled the returning of a struct from a Partial Active Pattern. The syntax is a little odd due to a limitation in the compiler, but it works for our purposes. Here is the equivalent approach using the Partial Active Pattern that returns a ValueStruct to reduce GC pressure. Compared to our first Active Pattern, the downside is that we have to define a separate one for each case we want to match against. [\u003creturn: Struct\u003e] let (|BufferId|_|) (node: Node) = let nodeType = node.Value \u0026\u0026\u0026 0x0000000F if nodeType = Node.BufferIdCode then let idValue = node.Value \u003e\u003e\u003e 4 ValueSome (idValue * 1\u003cBufferId\u003e) else ValueNone [\u003creturn: Struct\u003e] let (|ConstraintId|_|) (node: Node) = let nodeType = node.Value \u0026\u0026\u0026 0x0000000F if nodeType = Node.ConstraintIdCode then let idValue = node.Value \u003e\u003e\u003e 4 ValueSome (idValue * 1\u003cConstraintId\u003e) else ValueNone [\u003creturn: Struct\u003e] let (|MergeId|_|) (node: Node) = let nodeType = node.Value \u0026\u0026\u0026 0x0000000F if nodeType = Node.MergeIdCode then let idValue = node.Value \u003e\u003e\u003e 4 ValueSome (idValue * 1\u003cMergeId\u003e) else ValueNone [\u003creturn: Struct\u003e] let (|SplitId|_|) (node: Node) = let nodeType = node.Value \u0026\u0026\u0026 0x0000000F if nodeType = Node.SplitIdCode then let idValue = node.Value \u003e\u003e\u003e 4 ValueSome (idValue * 1\u003cSplitId\u003e) else ValueNone ","date":"2022-03-20","objectID":"/blog/2022/03/performance-of-dus-and-active-patterns/:1:0","tags":null,"title":"Performance of Discriminated Unions and Active Patterns","uri":"/blog/2022/03/performance-of-dus-and-active-patterns/"},{"categories":null,"content":"Benchmark Setup We will now set up two types of tests. The first set of tests will randomly jump around a Node array, check the Node type, and perform different work based on the case. This is most similar to the workloads I experience when writing algorithms for simulating manufacturing systems. For curiosity, I will also have a tests for a linear traversal of a Node array and perform the same work as the random access. This should illustrate the difference in performance between a predictable access pattern and a random one. The branch predictor in the CPU will have a more challenging time with the random access, and we expect it to be slower. To see the impact of the Active Pattern on memory allocation and GC, we will include the [\u003cMemoryDiagnoser\u003e] attribute on a Benchmarks class that holds our tests. This will tell BenchmarkDotNet to monitor how much allocation is occurring. We should see the Active Pattern approach incur more GC activity. We also include the BranchMispredictions and CacheMisses hardware counters to see how well the CPU can optimize our code. The ideal code has 0 Branch Mispredictions. Whenever we mispredict a branch, we can lose 20 - 30 cycles worth of work depending on the CPU. Cache Misses occur when our data is not in the cache, and the CPU has to go out to memory to retrieve the data. The CPU will do its best to predict what data it needs and fetch it ahead of time. When it guesses wrong, we can incur a severe performance penalty. [\u003cMemoryDiagnoser; HardwareCounters(HardwareCounter.BranchMispredictions, HardwareCounter.CacheMisses)\u003e] type Benchmarks () = let rng = Random 123 let nodeCount = 100 let lookupCount = 100 let loopCount = 100_000 let nodes = [|for i in 0 .. nodeCount - 1 -\u003e match rng.Next (0, 4) with | 0 -\u003e DuEncoding.Node.BufferId 1\u003cBufferId\u003e | 1 -\u003e DuEncoding.Node.ConstraintId 1\u003cConstraintId\u003e | 2 -\u003e DuEncoding.Node.MergeId 1\u003cMergeId\u003e | 3 -\u003e DuEncoding.Node.SplitId 1\u003cSplitId\u003e | _ -\u003e failwith \"The RNG generated a number outside the allowed bounds\" |] At the beginning of the Benchmarks class, we declare a random number generator rng which we will use to produce random nodes and indices to look up. We have a nodeCount of 100, which is the number of nodes we will generate. The lookupCount is the number of node lookups we will perform during each test loop. The loopCount is the number of loops we will perform in each test. nodes is an array of randomly generated nodes for our test. Our first test performs random lookups in the nodes array. It matches against the case of the DU and then adds a different amount to an accumulator based on the case. This is to simulate some amount of work being done for each node that was looked up. [\u003cBenchmark\u003e] member _.DuEncodingRandomAccess () = let mutable acc = 0 for lookupsIndex = 0 to loopCount - 1 do let lookups = randomNodeIndices[lookupsIndex] for lookupIndex = 0 to lookups.Length - 1 do let randomNodeIndex = lookups[lookupIndex] match nodes[randomNodeIndex] with | DuEncoding.Node.BufferId bufferId -\u003e acc \u003c- acc + 1 | DuEncoding.Node.ConstraintId constraintId -\u003e acc \u003c- acc + 2 | DuEncoding.Node.MergeId mergeId -\u003e acc \u003c- acc + 3 | DuEncoding.Node.SplitId splitId -\u003e acc \u003c- acc + 4 acc The second test does the same work, but the Node type is the struct representation instead of the reference-based one. [\u003cBenchmark\u003e] member _.StructDuEncodingRandomAccess () = let mutable acc = 0 for lookupsIndex = 0 to loopCount - 1 do let lookups = randomNodeIndices[lookupsIndex] for lookupIndex = 0 to lookups.Length - 1 do let randomNodeIndex = lookups[lookupIndex] match structNodes[randomNodeIndex] with | StructDuEncoding.Node.BufferId bufferId -\u003e acc \u003c- acc + 1 | StructDuEncoding.Node.ConstraintId constraintId -\u003e acc \u003c- acc + 2 | StructDuEncoding.Node.MergeId mergeId -\u003e acc \u003c- acc + 3 | StructDuEncoding.Node.SplitId splitId -\u003e acc \u003c- acc + 4 acc The third and fourth tests also perform the same work but with the Active Pattern and Partial Active Pattern approaches. Rememb","date":"2022-03-20","objectID":"/blog/2022/03/performance-of-dus-and-active-patterns/:2:0","tags":null,"title":"Performance of Discriminated Unions and Active Patterns","uri":"/blog/2022/03/performance-of-dus-and-active-patterns/"},{"categories":null,"content":"Results Note: Since I am measuring hardware counters, I have to run the terminal as admin; otherwise, I won’t have access to the data. If you want to test this yourself, you need to do the same. When I run the tests, I get the following table: Method Mean Error StdDev BranchMispredictions/Op CacheMisses/Op Gen 0 Allocated DuEncodingRandomAccess 83.09 ms 1.122 ms 0.995 ms 7,414,534 400,225 - 175 B StructDuEncodingRandomAccess 83.56 ms 0.766 ms 0.640 ms 7,415,626 409,418 - 175 B IntEncodingWithActivePatternRandomAccess 134.88 ms 2.650 ms 4.845 ms 8,126,171 1,070,592 28500.0000 240,000,358 B IntEncodingWithPartialActivePatternRandomAccess 86.43 ms 0.841 ms 0.787 ms 7,995,620 406,096 - 191 B DuEncodingLinearAccess 14.86 ms 0.090 ms 0.084 ms 5,073 7,701 - 22 B StructDuEncodingLinearAccess 17.35 ms 0.151 ms 0.142 ms 119,799 13,508 - 36 B IntEncodingWithActivePatternLinearAccess 74.67 ms 1.018 ms 0.903 ms 167,078 677,829 28571.4286 240,000,191 B IntEncodingWithPartialActivePatternLinearAccess 22.83 ms 0.372 ms 0.348 ms 8,151 33,225 - 45 B We see that the normal reference-based DU encoding gives us the best performance for both tests. This honestly surprised me. I would have thought that the Int Encoding would have yielded better results. There is some serious voodoo going on in the F# compiler. I wanted to dig into this more, but my work demands that I cut myself off here. I would like to get this info out for others to look at since I have not been able to find other blog posts which deal with this. I welcome feedback and critique. You can find the code here. Let me know if I missed something obvious. Eventually, I hope to have the time to dig deeper into this. In the meantime, I plan to stick with the default DU until I can figure out if I can make something faster for my use case. Let me know if you have ideas for going faster or other ideas I should test. I can be found on Twitter @McCrews, or you can email matthew@crews.email. ","date":"2022-03-20","objectID":"/blog/2022/03/performance-of-dus-and-active-patterns/:3:0","tags":null,"title":"Performance of Discriminated Unions and Active Patterns","uri":"/blog/2022/03/performance-of-dus-and-active-patterns/"},{"categories":null,"content":"I have been working on a simulation engine that requires a key/value collection for holding the flow rates through a network as part of a Push-Relabel algorithm. This is the most performance-critical code in the engine, so I needed to find the fastest way to perform a lookup, update, and store for a key/value pair. The prevailing wisdom is to use a .NET Dictionary, but I was curious how the performance would compare to the F# Map type. A Map is backed by an AVL Tree and has a read and write performance of $O(log(n))$ while Dictionary is a Hash Table with an algorithmic complexity of $O(1)$ for reads and writes. For my use case, I need to read a value from the collection, perform a minor update, and then update the value for the key in the collection. dictionary[key] \u003c- dictionary[key] + 1.0 // Trivial work example ","date":"2022-03-19","objectID":"/blog/2022/03/performance-of-key-value-lookups-types/:0:0","tags":null,"title":"Performance of Key/Value Collections for Updating","uri":"/blog/2022/03/performance-of-key-value-lookups-types/"},{"categories":null,"content":"Test Setup To make it easier to set up tests across various collection sizes in benchmarkDotNet, I defined an Enum Size that would indicate the size of the collection I wanted to test against. // Enum for the different size [\u003cStruct\u003e] type Size = | ``10`` = 0 | ``100`` = 1 | ``1_000`` = 2 | ``10_000`` = 3 | ``100_000`` = 4 | ``1_000_000`` = 5 The Enum cases will map to the index of the data set that I want to test against. I now define a Benchmark class to hold my tests and generate the necessary data. type Benchmarks () = // The number of lookups I will perform in each test let lookupCount = 10 // A random number generator to create random indices // into the collections. let rng = Random 1337 // Lookup array to map Size -\u003e Count let sizeToCount = [| 10 100 1_000 10_000 100_000 1_000_000 |] // An array of different Maps for each size in Size let maps = sizeToCount |\u003e Array.map (fun count -\u003e Map [for i in 0 .. count - 1 -\u003e string i, 0.0] ) // An array of different Dictionaries for each size in Size let dictionaries = sizeToCount |\u003e Array.map (fun count -\u003e Dictionary [for i in 0 .. count - 1 -\u003e KeyValuePair (string i, 0.0)] ) I then add a member to the Benchmarks class called Size so that benchmarkDotNet can update the field to automatically test across each of the cases in the Size Enum. [\u003cParams(Size.``10``, Size.``100``, Size.``1_000``, Size.``10_000``, Size.``100_000``, Size.``1_000_000``)\u003e] member val Size = Size.``10`` with get, set When benchmarkDotNet runs, it will see that the Size property has been decorated with the different values we want it to test. It will run each of our tests with every value we decorate the Size property with. I now create the test for the Map collections. I index into the maps array and retrieve the Map associated with the case of Size. I then retrieve the keys associated with the Size. This ensures that all of the keys we will lookup can be found in the collection. You will see that I use mutable on the map value and then return it at the end of the method. This is to ensure that the CLR doesn’t compile the work away. This is not how I would typically use a Map. [\u003cBenchmark\u003e] member b.Map () = // We using mutation to ensure the compiler doesn't eliminate unnecessary work let mutable map = maps[int b.Size] let keys = keysForSize[int b.Size] // We are making memory access pattern as predictable as possible // to eliminate cache hits from the work of getting the key. We don't use // IEnumerable to reduce the overhead. for i = 0 to keys.Length - 1 do let key = keys[i] let newValue = map[key] + 1.0 map \u003c- map.Add (key, newValue) // Do a minimal amount of work map We iterate through each key in the keys array associated with the size we are testing. I wanted to try more than one lookup, so I wasn’t getting unexpected performance benefits from the CPU being lucky for a lookup of a single value. I create the same test for the Dictionary type. The work is the same, even though it looks slightly different. This is due to Dictionary having a different API than Map. [\u003cBenchmark\u003e] member b.Dictionary () = let dictionary = dictionaries[int b.Size] let keys = keysForSize[int b.Size] for i = 0 to keys.Length - 1 do let key = keys[i] dictionary[key] \u003c- dictionary[key] + 1.0 // Do a minimal amount of work dictionary I now define my main method and run the benchmarks. [\u003cEntryPoint\u003e] let main _ = // I don't care about what Run returns so I'm ignoring it let _ = BenchmarkRunner.Run\u003cBenchmarks\u003e() 0 Another thing worth mentioning is that I am restricted to the x64 platform, so I update the .fsproj of the project to make sure that I only build and test for x64. \u003cProject Sdk=\"Microsoft.NET.Sdk\"\u003e \u003cPropertyGroup\u003e \u003cOutputType\u003eExe\u003c/OutputType\u003e \u003cTargetFramework\u003enet6.0\u003c/TargetFramework\u003e \u003c!-- Restricts to the x64 platform--\u003e \u003cPlatform\u003ex64\u003c/Platform\u003e \u003c/PropertyGroup\u003e \u003cItemGroup\u003e \u003cCompile Include=\"Program.fs\" /\u003e \u003c/ItemGroup\u003e \u003cItemGroup\u003e \u003cPackageReference Include=\"BenchmarkDotNet\" Version=\"0.13.1\" /\u003e \u003cPackageReference Include","date":"2022-03-19","objectID":"/blog/2022/03/performance-of-key-value-lookups-types/:1:0","tags":null,"title":"Performance of Key/Value Collections for Updating","uri":"/blog/2022/03/performance-of-key-value-lookups-types/"},{"categories":null,"content":"Even Faster? Can we go even faster, though? You may notice that we have to look up the key twice for each update. Once to get the value so that we can add 1.0 to it and a second time to store the updated value. It’s all on this single line of code: // First lookup is here to get the value to add 1.0 to it // ↓ dictionary[key] \u003c- dictionary[key] + 1.0 // ↑ // Second lookup happens here to store the value Wouldn’t it be nice if we didn’t have to do that work twice? What if instead of the Dictionary returning by value, it returns by reference? This way, we only need to perform the lookup once? Now, some of you may start balking, saying that’s dangerous. You can create some hideous bugs if you misuse this. It’s difficult enough that you will not find it in the Dictionary class itself. You need to use a method found on the CollectionsMarshal class, in the System.Runtime.InteropServices namespace. The name of the method I want is GetValueRefOrAddDefault. This method has an unusual function signature, so I want to unpack what is happening. CollectionsMarshal.GetValueRefOrAddDefault\u003c'TKey,'TValue\u003e(dictionary: Dictionary\u003c'TKey,'TValue\u003e, key: 'TKey, exists: byref\u003cbool\u003e) : byref\u003c'TValue F# does some interesting things for you implicitly regarding the ref types: byref, inref, and outref. I highly recommend you read the Microsoft docs on refs. The first time you read it, you may be confused. I was, but the more I work with the ref types, the more it makes sense. Aside: F# is designed as a succinct, expressive, and efficient language. It sometimes gets a reputation for being slow. I will concede if you write entirely idiomatic F#, your performance may not be on the level of a C++ program. BUT, that’s not to say you can’t write fast F# code. F# has defaults and idioms, making it easier to compose correct programs quickly. What people don’t talk about is that you can turn all the safeties in F# off if you want to. If want to drop down to raw native pointers, you can. F# forces you to be more explicit about wanting to violate the safeties which I think is a feature, not a hinderance. The usual way of working with a .NET API which uses a byref as one of the parameters for the method in F# is to use a match...with statement to unpack the values. The most common method I use with this behavior is the TryGetValue method of Dictionary. TryGetValue has the following signature: Dictionary.TryGetValue(key: string, value: byref\u003c'T\u003e) : bool You will see that the method expects you to pass a byref\u003c'T\u003e. If the dictionary has the value, it will put it in the location byref\u003c'T\u003e points and return a true. If it does not find the value, it will not update the value the byref\u003c'T\u003e points to and returns false. Instead of declaring a byref\u003c'T\u003e, we instead use the match ... with syntax, and F# will implicitly do the work of creating the byref\u003c'T\u003e for us. match dictionary.TryGetValue key with | true, value -\u003e () // Do something with value | false, _ -\u003e () // Do something without the value value, in this case, will be the value that was found. It will NOT be a byref\u003c'T\u003e pointing to the value found. F# implicitly dereferences the byref for us. This implicit dereferencing is nice most of the time but, in our case, is the opposite of what we want. Therefore we must define our byrefs and pass them to the method. // dictionary is a Dictionary\u003cint, float\u003e // key is a `int` we are wanting to the look up the float for let mutable wasFound = Unchecked.defaultof\u003c_\u003e let valueRef = \u0026CollectionsMarshal.GetValueRefOrAddDefault (dictionary, key, \u0026wasFound) // ↑ Notice this `\u0026` wasFound is a byref\u003cbool\u003e that we pass to the method. You’ll notice that we are not giving in a byref\u003cfloat\u003e for the method to fill in for us. Instead, we are using the \u0026 operator prepended to the method to tell F# that we want it to return the byref for us, not the value. If we did not prepend the \u0026 to the method call, F# would implicitly dereference the byref for us. This is another case of the F# defaults ","date":"2022-03-19","objectID":"/blog/2022/03/performance-of-key-value-lookups-types/:2:0","tags":null,"title":"Performance of Key/Value Collections for Updating","uri":"/blog/2022/03/performance-of-key-value-lookups-types/"},{"categories":null,"content":"A Safer Approach Instead of getting a reference to the value, we could wrap our values in a ValueWrapper class and store those in the Dictionary. This was proposed in a GitHub discussion where people were debating the addition of the GetValueRefOrAddDefault method. I decided to code one up and compare the performance out of curiosity. type ValueWrapper\u003c'T when 'T : struct\u003e (value: 'T) = member val Value = value with get, set I test this approach; I now have to wrap my values in the ValueWrapper type. let wrappedValueDictionaries = sizeToCount |\u003e Array.map (fun count -\u003e Dictionary [for i in 0 .. count - 1 -\u003e KeyValuePair (string i, ValueWrapper 0.0)] ) And create a test for it… [\u003cBenchmark\u003e] member b.ValueWrappedDictionary () = let valueWrappedDictionary = wrappedValueDictionaries[int b.Size] let keys = keysForSize[int b.Size] for i = 0 to keys.Length - 1 do let key = keys[i] let v = valueWrappedDictionary[key] v.Value \u003c- v.Value + 1.0 // Do a minimal amount of work valueWrappedDictionary We see that this WrappedValue approach is just as fast when we run the benchmarks. Method Size Mean Error StdDev Median Map 10 986.3 ns 19.51 ns 38.06 ns 972.5 ns Dictionary 10 253.2 ns 3.57 ns 3.17 ns 252.4 ns ValueWrappedDictionary 10 104.8 ns 1.99 ns 1.86 ns 104.0 ns DictionaryGetRef 10 119.3 ns 1.58 ns 1.48 ns 119.0 ns Map 100 2,062.4 ns 39.33 ns 36.79 ns 2,053.8 ns Dictionary 100 325.2 ns 6.18 ns 6.34 ns 324.7 ns ValueWrappedDictionary 100 132.3 ns 2.08 ns 1.95 ns 131.7 ns DictionaryGetRef 100 124.5 ns 2.52 ns 3.10 ns 124.2 ns Map 1_000 3,468.0 ns 68.38 ns 130.09 ns 3,394.7 ns Dictionary 1_000 299.8 ns 5.99 ns 10.02 ns 298.1 ns ValueWrappedDictionary 1_000 132.2 ns 1.06 ns 0.94 ns 132.2 ns DictionaryGetRef 1_000 130.9 ns 2.52 ns 2.69 ns 130.1 ns Map 10_000 4,383.1 ns 86.00 ns 80.45 ns 4,393.2 ns Dictionary 10_000 322.4 ns 2.15 ns 1.79 ns 322.6 ns ValueWrappedDictionary 10_000 150.8 ns 0.90 ns 0.80 ns 150.6 ns DictionaryGetRef 10_000 141.4 ns 2.12 ns 1.77 ns 140.9 ns Map 100_000 5,571.8 ns 90.32 ns 84.48 ns 5,569.3 ns Dictionary 100_000 338.0 ns 2.71 ns 2.26 ns 338.6 ns ValueWrappedDictionary 100_000 151.3 ns 0.75 ns 0.63 ns 151.4 ns DictionaryGetRef 100_000 130.5 ns 1.13 ns 1.05 ns 130.2 ns Map 1_000_000 7,695.1 ns 150.65 ns 263.86 ns 7,690.7 ns Dictionary 1_000_000 369.2 ns 2.69 ns 2.10 ns 369.5 ns ValueWrappedDictionary 1_000_000 153.3 ns 1.21 ns 1.13 ns 153.2 ns DictionaryGetRef 1_000_000 152.1 ns 1.09 ns 0.91 ns 152.1 ns The nice thing about this approach is that even when the Dictionary gets reorganized, our ValueWrapper will still point to the correct data piece. The downside is that this will allocate more memory since each ValueWrapper is an object that needs to be allocated on the heap. You also lose any cache locality benefits since the ValueWrapper objects could spread all over memory. We aren’t observing any downsides in this tiny benchmark, but it’s essential to be aware. There could be some performance implications in the context of a larger program. If you feel like playing with the code yourself, you can find all the tests here. Let me know if you have ideas for going faster or other collections I should test. I can be found on Twitter @McCrews, or you can email matthew@crews.email. ","date":"2022-03-19","objectID":"/blog/2022/03/performance-of-key-value-lookups-types/:3:0","tags":null,"title":"Performance of Key/Value Collections for Updating","uri":"/blog/2022/03/performance-of-key-value-lookups-types/"},{"categories":null,"content":"I am writing a graph algorithm at the heart of one of the products we are developing at Simulation Dynamics. A part of the algorithm is tracking whether a node or vertex has been observed yet or not. As the algorithm progresses, it will mark whether it has completed processing a node or a vertex to know it does not need to continue through the graph. This algorithm is part of the inner loop of the simulation and needs to be as fast as possible. Speed is a critical feature for us, so essentially, anything is allowed if it gives us more speed. The only requirement is it be written in .NET (preferably F#) and runs on a modern x86-64 processor. These graphs are almost always small, say 20 nodes and 30 vertices. We are modeling manufacturing processes, and the types of facilities that we simulate can be simplified down to these small graphs. This means we don’t necessarily need to handle N nodes and M vertices, and we need an algorithm that is as fast as possible for these small cases. The F# Set I love the F# Set\u003c'T\u003e collection. When I just need to process sets of value and perform comparisons between them, nothing beats the tried and true Set\u003c'T\u003e. For those curious, the F# Set and Map collections are backed by a variation on the AVL tree. We actually have Victor Baybekov to thank for some massive performance improvements he provided in this pull request. Before I test out various approaches, I create a baseline performance test. I will generate 1,000,000 different indexes in a range of 0 to 50. For each of these indices, I will check whether it is in the Set or not. If it is not in the Set, I will add it. If it is in the Set, I will remove it. This will simulate the type of behavior that the graph algorithm uses while traversing the graph. I will also use the BenchmarkDotNet library to measure my performance. open System open FSharp.NativeInterop open BenchmarkDotNet.Attributes open BenchmarkDotNet.Running open BenchmarkDotNet.Diagnosers [\u003cMemoryDiagnoser\u003e] type Benchmarks () = let testIndexCount = 1_000_000 let indexRange = 50 let rng = Random 123 // Generate a set of random indices to check let testIndexes = [| for _ = 1 to testIndexCount do // Note: Next is exclusive on the upper bound rng.Next (0, indexRange) |] [\u003cBenchmark\u003e] member _.SetTracker () = let mutable tracker = Set.empty for i = 0 to testIndexes.Length - 1 do let testIndex = testIndexes[i] if tracker.Contains testIndex then // Real world we would do work here and then flip the case tracker \u003c- tracker.Remove testIndex else tracker \u003c- tracker.Add testIndex tracker When I run this benchmark, I get the following result. | Method | Mean | Error | StdDev | Gen 0 | Allocated | |----------- |-----------:|----------:|----------:|-----------:|--------------:| | SetTracker | 130.889 ms | 1.7642 ms | 1.4732 ms | 27000.0000 | 226,813,932 B | We now have a baseline to compare against. ","date":"2022-02-06","objectID":"/blog/2022/02/high-performance-observation-tracking/:0:0","tags":null,"title":"Fast Tracking of Item Observations","uri":"/blog/2022/02/high-performance-observation-tracking/"},{"categories":null,"content":"The .NET HashSet The first thing someone will likely point out is, “Matthew, you are using an immutable collection that you are constantly updating and rebinding. Why not use a mutable collection?” Great question! Let’s reach for the next most obvious collection in .NET, the HashSet. HashSet is found in the System.Collections.Generic namespace. Instead of using a tree to keep track of items, it uses a hash table. This should give us constant-time lookup, insertion, and removal. We add a new test to our Benchmarks class… [\u003cBenchmark\u003e] member _.HashSetTracker () = let mutable tracker = Collections.Generic.HashSet () for i = 0 to testIndexes.Length - 1 do let testIndex = testIndexes[i] if tracker.Contains testIndex then // Real world we would do work here and then flip the case tracker.Remove testIndex |\u003e ignore else tracker.Add testIndex |\u003e ignore tracker … and re-run our benchmarks. | Method | Mean | Error | StdDev | Gen 0 | Allocated | |--------------- |-----------:|----------:|----------:|-----------:|--------------:| | SetTracker | 130.889 ms | 1.7642 ms | 1.4732 ms | 27000.0000 | 226,813,932 B | | HashSetTracker | 14.368 ms | 0.0613 ms | 0.0544 ms | - | 2,800 B | We see that the HashSet is giving us a performance boost over the F# Set. This is expected since we don’t have to re-arrange a tree when we perform insertion and deletion. This is NOT to say that Set is inferior to HashSet; please don’t consider that the takeaway. Set can do many things that HashSet cannot. It just happens in this instance, HashSet appears to be a better choice. Oh, but we aren’t done yet… ","date":"2022-02-06","objectID":"/blog/2022/02/high-performance-observation-tracking/:1:0","tags":null,"title":"Fast Tracking of Item Observations","uri":"/blog/2022/02/high-performance-observation-tracking/"},{"categories":null,"content":"Plain Old Arrays One of my favorite talks on the internet is one by Scott Meyers, where he talks about the criticality of understanding memory hierarchies when writing high-performance code. My favorite moment is when he quotes a developer who writes high-frequency trading algorithms. The developer says, “I don’t care how fancy your data structure is; an array will beat it.” I can map all nodes and vertices to int values ahead of time for my use case. This means that I could easily track whether I have visited a node or index by looking up a bool in an array using the value for the node or vertex as an index into the array. Let’s try that and see what we get. I add a new benchmark to my Benchmarks class to see how this approach performs. [\u003cBenchmark\u003e] member _.BoolArrayTracker () = let tracker = Array.create indexRange false for i = 0 to testIndexes.Length - 1 do let testIndex = testIndexes[i] if tracker[testIndex] then // Real world we would do work here and then flip the case tracker[testIndex] \u003c- false else tracker[testIndex] \u003c- true tracker We can now see how using an array\u003cbool\u003e performs against a Set and HashSet. | Method | Mean | Error | StdDev | Gen 0 | Allocated | |----------------- |-----------:|----------:|----------:|-----------:|--------------:| | SetTracker | 130.889 ms | 1.7642 ms | 1.4732 ms | 27000.0000 | 226,813,932 B | | HashSetTracker | 14.368 ms | 0.0613 ms | 0.0544 ms | - | 2,800 B | | BoolArrayTracker | 5.017 ms | 0.0447 ms | 0.0418 ms | - | 84 B | Alright, we see another speed boost. Using just an array, we’ve cut out some cycles that the HashSet has to perform when it performs a lookup. Now we are just taking a pointer to the head of the array and offsetting it to perform the lookup. It’s hard to get much faster than this… or is it? ","date":"2022-02-06","objectID":"/blog/2022/02/high-performance-observation-tracking/:2:0","tags":null,"title":"Fast Tracking of Item Observations","uri":"/blog/2022/02/high-performance-observation-tracking/"},{"categories":null,"content":"Enter Data-Oriented Design I’ve been on a quest to become a better developer, and my latest wanderings have brought me to the world of Data-Oriented Design. I love Mike Acton’s talk at CPP Con, and I highly recommend it for anyone who hasn’t seen it. He mentions that when he sees a bool in a Struct he almost always knows that something has gone wrong. The reason is that a bool only contains 1 bit of information, but it still takes up 8 bits of memory. This means that if you are moving many types back and forth in the cache which contains bools, you are wasting a large amount of memory bandwidth. At the time I watched it, I thought, “Okay, I see your point about wasting memory bandwidth, but what if you really do need to know whether something is true or not?” That sat in the back of my mind for a while, and I didn’t see a solution. The other day though, Jérémie Chassaing gave an excellent talk on writing high-performance F# with no memory allocation. In it, he showed an elegant trick for storing two int32 in a single int64. You can watch the bit here. He holds one of the int32 in the bottom 32-bits of the int64 and the other int32 value in the top 32-bits. That may seem silly, but it provides the compiler a unique optimization that it wouldn’t have otherwise. It can store both of those int32 in a single 64-bit register. This is the fastest form of memory in a CPU. Now, who don’t we take that same trick, and instead of storing 2 int32 values in an int64, what about 64 bools in an int64? Each bit of the int64 can correspond to a node or vertex in the graph. Instead of passing an array\u003cbool\u003e around, we can pass a single int64. Remember that for this use case, it is incredibly rare to see more than 20 nodes and 30 vertices, so 64-bits is plenty to store whether we have observed them or not. Let’s create a new type for wrapping our int64 and provide some convenience methods for working with it. We will define it as a Struct since that provides the .NET compiler a large number of options for optimizing it. [\u003cStruct\u003e] type Int64Tracker = private { mutable Value : int64 } static member Create () = { Value = 0L } // Check whether a position has been set to 1 or not member this.IsSet (position: int) = (this.Value \u0026\u0026\u0026 (1L \u003c\u003c\u003c position)) \u003c\u003e 0L // Sets a bit at a position to 1 member this.Set (position: int) = this.Value \u003c- (1L \u003c\u003c\u003c position) ||| this.Value // Sets a bit at a position to 0 member this.UnSet (position: int) = this.Value \u003c- ~~~ (1 \u003c\u003c\u003c position) \u0026\u0026\u0026 this.Value Let’s unpack this. The Value field is used to track whether something has been observed or not. The Create method gives us a new Int64Tracker where all the bits are set to 0. The IsSet method takes a position and checks whether that position is 0 or not. It does this by taking a value of 1L, which puts a value of 1 in the bottom bit and then left, shifting it to the desired position. It then does a bitwise AND between the Value and the bit-shifted value. This will return 0s for every location except for possibly the position we are interested in. If there is a 1 in the desired position, it will AND with our shifted 1 and return a non-zero number. If there is a 0 in the position of interest, all the bits will return 0, and the resulting value will be equal to 0. The Set method takes a position and sets the bit in that position to 1. It does this by taking a value of 1L, which has a 1 in the bottom bit, left-shifts it to the desired position, and then performs a bitwise OR with Value. If the value in the given position was already 1, it will still be a 1. If it was 0, it will be changed to 1. Here’s a quick video showing the operation. The final method, UnSet, takes 1L and left-shifts it to the desired position. It then does a bitwise negation. This means that all the bits will be 1 except for the position that we want to turn into a 0. We then AND that mask with Value to get our updated Value. Here’s a video showing how this is done. We now have a new type that allows us ","date":"2022-02-06","objectID":"/blog/2022/02/high-performance-observation-tracking/:2:1","tags":null,"title":"Fast Tracking of Item Observations","uri":"/blog/2022/02/high-performance-observation-tracking/"},{"categories":null,"content":"Mutation gets a bad wrap, and some believe Mutation is evil and should be avoided at all costs. For a while, I was also in this camp. As time went by, I realized there is nothing wrong with Mutation. The problem is when Mutation is misapplied. Some of you may be familiar with a paper by Edgar Dijkstra called “Go To Statement Considered Harmful”. In it, Dijkstra argues that the goto statement is a serious source of problems in code and should be avoided. His opening paragraph states that it should be abolished from all “higher level” languages and reserved for machine code. Some would also put Mutation into the same list of things that should be forbidden. Mutation can indeed be a challenging thing to debug. Some would like the pure keyword added to the F# language so that the compiler can ensure that a function does not use Mutation. There was a time that I, too, was caught up in the idea that Mutation should be abolished. I thought that to write high-quality software, we needed to work exclusively with immutable data and data structures. Over time my understanding has evolved, though. Mutation is a tool, just like the Go To statement, and it can be misapplied and overused, but it isn’t bad. Let’s look at some code and contrast a mutation-free approach and Mutation based approach. For the first example, let’s find the maximum element in an array of integers. ","date":"2021-12-05","objectID":"/blog/2021/12/fsharp-loves-mutation/:0:0","tags":null,"title":"F# Loves Mutation","uri":"/blog/2021/12/fsharp-loves-mutation/"},{"categories":null,"content":"Max Item in Array Our first function will not use any mutation to find the maximum value, and I’m assuming that the array passed to the function is non-null and not empty to keep the code simple. The most straightforward approach I could come up with is a recursive function that loops through the values and keeps track of the observed maximum value. // Maximum value in array let mutationFreeMax (x: array\u003cint\u003e) = // I am must going to assume x is non-null and not empty let rec loop (acc: int) (idx: int) (array: array\u003cint\u003e) = if idx \u003c array.Length then let curr = array.[idx] if curr \u003e acc then loop curr (idx + 1) array else loop acc (idx + 1) array else acc loop x.[0] 1 x For someone who has worked with recursion, this is not intimidating, but if you haven’t written many recursive functions, this may confuse you. Only in the last two years did I become comfortable with recursion, so I know firsthand how disorienting code like this can be. Let’s contrast that with an implementation that can use Mutation. let mutationMax (x: array\u003cint\u003e) = // I am must going to assume x is non-null and not empty let mutable acc = x.[0] let mutable idx = 1 while idx \u003c x.Length do let curr = x.[idx] if curr \u003e acc then acc \u003c- curr idx \u003c- idx + 1 acc I believe that most developers would find this easier to follow. We are using two mutable values, acc and idx, but they don’t leak outside the function. The Mutation is isolated to the context in which it is used. In fact, this example comes from the F# source code. Let’s take this a step further and see the performance difference between these two implementations. I’m going to create 100,000 arrays with between 10 and 10,000 random, positive integers and call the two different max function implementations to see the difference in performance. You can check the code and run it yourself here. I am using BenchmarkDotNet for the testing. When I run them I get the following result. Method Mean Error StdDev Median MutationFree 292.7 ms 6.65 ms 18.10 ms 285.4 ms MutationBased 281.8 ms 5.04 ms 4.71 ms 281.5 ms Turns out the mutation-free approach is ever so slightly slower, along with being more confusing for a beginner developer. Now, a more extreme case. ","date":"2021-12-05","objectID":"/blog/2021/12/fsharp-loves-mutation/:1:0","tags":null,"title":"F# Loves Mutation","uri":"/blog/2021/12/fsharp-loves-mutation/"},{"categories":null,"content":"Sorting a List Let’s look at how Quicksort performs on a list of integers. I am referencing Yan Ciu for my implementation. let rec sort (values: list\u003cint\u003e) = match values with | [] -\u003e values | [x] -\u003e values | head::tail -\u003e let less, greater = List.partition ((\u003e=) head) tail List.concat [sort(less); [head]; sort(greater)] Quicksort is easy to express in F# but let’s see how the performance stacks up against the built-in List.sort function. We’ll have each function sort a list of 1,000 positive integers for our testing. You can see the tests here. When we run it, we see the following. Method Mean Error StdDev ImmutableQuicksort 154.46 us 2.939 us 3.717 us BuiltInSort 14.73 us 0.286 us 0.436 us The performance is not even close. Granted, this Quicksort is not optimized. It’s a naive approach, but I would suggest it will never get as fast as the built-in sort. If you look at the source code for List.sort you’ll see this. let sort list = match list with | [] | [_] -\u003e list | _ -\u003e let array = Microsoft.FSharp.Primitives.Basics.List.toArray list Microsoft.FSharp.Primitives.Basics.Array.stableSortInPlace array Microsoft.FSharp.Primitives.Basics.List.ofArray array F# is converting the list to an array, using an optimized sort on the array, and then turning it back into a list for the return value. You will see this kind of behavior all over the F# source. We as users are getting an immutable experience when we work with these functions and types, but under the hood, there is Mutation. ","date":"2021-12-05","objectID":"/blog/2021/12/fsharp-loves-mutation/:2:0","tags":null,"title":"F# Loves Mutation","uri":"/blog/2021/12/fsharp-loves-mutation/"},{"categories":null,"content":"Conclusion Mutation is a powerful tool for when you need to go fast. What I failed to realize early on was that Mutation has a place. The key thing is that the Mutation is isolated and does not leak out into the rest of the program. I am actually working on an update to the SliceMap library for Flips. The underlying data structure will re-arrange itself to best serve the query whenever you query the data. There is constant Mutation occurring under the covers, but the user never sees it. If you liked this post and want to stay in the loop for the work I am doing with Mathematical Programming, please subscribe to my list! ","date":"2021-12-05","objectID":"/blog/2021/12/fsharp-loves-mutation/:3:0","tags":null,"title":"F# Loves Mutation","uri":"/blog/2021/12/fsharp-loves-mutation/"},{"categories":null,"content":"I was recently asked a question on the Flips GitHub page which I felt warranted a full blog post. It is an interesting problem that I have seen several variations on so I wanted to provide a more detailed model. The largest example of this problem I have seen is at Rocket Technology (formerly Quicken Loans) where they must assign desks to thousands of people across several buildings in downtown Detroit, MI. You have a set of People that you need to assign desks to. Each person is a part of a Team, and you would rather that people sat with their Team. Each Desk is a part of a Cluster. A Cluster is a group of desks that are next to each other. Each Person is already assigned to a Desk. You want to come up with a seating assignment that maximizes the number of people who are sitting with their team while also minimizing the number of times that people must move from the desk they are sitting at currently. ","date":"2021-11-21","objectID":"/blog/2021/11/team-desk-assignments/:0:0","tags":null,"title":"Team Desk Assignments","uri":"/blog/2021/11/team-desk-assignments/"},{"categories":null,"content":"The Domain Before we start putting our Mathematical Planning Model together, let’s create simple types to describe our domain. Based on the word description of our problem we already have four types: PersonId, TeamId, DeskId, and ClusterId. For the sake of simplicity, we are going to model all of these as single-case DUs with an int as the value. type TeamId = TeamId of int type DeskId = DeskId of int type PersonId = PersonId of int type ClusterId = ClusterId of int I don’t have access to the original enquirers data set, so I need to create a synthetic data. I’ll start by creating a new System.Random and give it an initial seed value of 123 so I can reproduce my results and define the number of teams, persons, clusters, and desks in my problem. let rng = System.Random 123 let teamCount = 5 let personCount = 22 let clusterCount = 8 let deskCount = 30 Now I’ll create some arrays of my data. let teamIds = [|1 .. teamCount|] |\u003e Array.map TeamId let personIds = [|1 .. personCount|] |\u003e Array.map PersonId let clusterIds = [|1 .. clusterCount|] |\u003e Array.map ClusterId let deskIds = [|1 .. deskCount|] |\u003e Array.map DeskId I need to separate the people into which team they belong to. F# has a handy function in the Array module which makes it easy to divide an array into evenly divided chunks called Array.splitInto. You give the splitInto function the number of chunks you want, and it evenly divides the input array into that many chunks. Let’s now divide the people into which team they belong to. let teams = personIds |\u003e Array.splitInto teamIds.Length |\u003e Array.mapi (fun idx personIds -\u003e let teamId = teamIds[idx] teamId, personIds ) |\u003e readOnlyDict I now have a IReadOnlyDictionary\u003cTeamId, array\u003cPersonId\u003e\u003e which allows me to query which PersonId belong to a given TeamId. I also want to divide the desks into which clusters they belong to. I’ll use the same function as before to evenly divide them. let clusters = deskIds |\u003e Array.splitInto clusterIds.Length |\u003e Array.mapi (fun idx desks -\u003e let clusterId = clusterIds[idx % clusterIds.Length] clusterId, Set desks // Notice I'm creating a Set\u003cDeskId\u003e ) |\u003e readOnlyDict Now I have a IReadOnlyDictionary\u003cClusterId, Set\u003cDeskId\u003e\u003e. I can look up a ClusterId and get a Set which contains all the DeskId that belong to that ClusterId. Notice, I put the DeskId into a Set, not an array. You will see why that matters later. Later in my problem I will need to lookup which ClusterId that a DeskId belongs to so I go ahead and create an IReadOnlyDictionary\u003cDeskId, ClusterId\u003e from the same data. let deskToCluster = clusters |\u003e Seq.collect (fun (KeyValue (clusterId, deskIds)) -\u003e deskIds |\u003e Seq.map (fun deskId -\u003e deskId, clusterId)) |\u003e readOnlyDict Each PersonId is already assigned to a desk. Since I don’t have access to the real data, I’m just going to assign each PersonId to a random DeskId and store that in a array\u003cPersonId * DeskId\u003e. let currentPersonDeskAssignment = let randomDeskOrder = deskIds |\u003e Array.sortBy (fun _ -\u003e rng.NextDouble ()) randomDeskOrder[0 .. personIds.Length - 1] |\u003e Array.zip personIds ","date":"2021-11-21","objectID":"/blog/2021/11/team-desk-assignments/:1:0","tags":null,"title":"Team Desk Assignments","uri":"/blog/2021/11/team-desk-assignments/"},{"categories":null,"content":"The Model We now have a tiny domain and synthetic data. Let’s being building our model! We start with creating the decision variables which represent assigning a given PersonId to a particular DeskId. We will use a Boolean decision variable. A value of 1 will represent true. true means that we do assign a given PersonId to a given DeskId. A value of 0 represents false which means we do NOT assign a PersonId to a given DeskId. We store these decision variables in a SMap2\u003cPersonId, DeskId, Decision\u003e for easy slicing later. let personDeskAssignment = DecisionBuilder \"PersonAssignment\" { for p in personIds do for d in deskIds -\u003e Boolean } |\u003e SMap2 We now create a decision variable which represents assigning a TeamId to a ClusterId. The decision variable will be a Boolean where 1 means we do assign the TeamId to the ClusterId and 0 means we do not. We store these decisions in a SMap2\u003cTeamId, ClusterId, Decision\u003e. let teamClusterAssignment = DecisionBuilder \"TeamClusterAssignment\" { for t in teamIds do for c in clusterIds -\u003e Boolean } |\u003e SMap2 Alright, now we can start creating constraints. First thing we need to do is ensure that each PersonId is assigned to exactly 1 desk. let eachPersonHasDeskConstraints = ConstraintBuilder \"EachPersonHasDesk\" { for p in personIds -\u003e sum personDeskAssignment[p, All] == 1.0 } Notice that the comparison is == and not \u003c==. An == means that the sum MUST equal 1.0. If we had put \u003c==, that would mean that the sum MAY equal 1.0 but it doesn’t have to. This would allow the optimization to not ensure that everyone has a spot. That’s not what we want though so we use == to ensure each PersonId will have a DeskId they are assigned to. Next, we make sure that each DeskId is only assigned once. let eachDeskOnlyOnceConstraints = ConstraintBuilder \"EachDeskOnlyOnce\" { for d in deskIds -\u003e sum personDeskAssignment[All, d] \u003c== 1.0 } Again, please notice the comparison that is being used here. Now we use \u003c== instead of ==. We are saying a DeskId MAY be assigned up to once, but no more. We are not requiring that each DeskId have a PersonId assigned to it. This is an important distinction. Now we want to make sure that each TeamId is assigned a ClusterId. let eachTeamHasClusterConstraints = ConstraintBuilder \"EachTeamHasCluster\" { for t in teamIds -\u003e sum teamClusterAssignment[t, All] == 1.0 } Because we use == in the comparison we are ensuring that each TeamId is assigned a ClusterId. We now must ensure that each ClusterId is assigned only once. let eachClusterOnlyOnceConstraints = ConstraintBuilder \"EachClusterOnlyOnce\" { for c in clusterIds -\u003e sum teamClusterAssignment[All, c] \u003c== 1.0 } We are using \u003c== which is saying that we may assign each ClusterId once but we don’t have to. ","date":"2021-11-21","objectID":"/blog/2021/11/team-desk-assignments/:2:0","tags":null,"title":"Team Desk Assignments","uri":"/blog/2021/11/team-desk-assignments/"},{"categories":null,"content":"Team Co-Location Now we get to the slightly complex part. This is where the original enquirer was stuck, and I completely understand why. Everything up to this point has been straightforward. Now we need to quantify the success of teams sitting together. To do this we are going to use something called Indicator Variables. Indicator Variables are used to indicate whether a certain condition is being met in a model or not. We are doing to use them to model whether a given PersonId is sitting with their assigned TeamId. We will need to create a set of indicator variables for each TeamId, ClusterId, and PersonId combination. let personCoLocated = DecisionBuilder \"TeamCoLocated\" { for t in teamIds do for c in clusterIds do for p in teams[t] -\u003e Boolean } |\u003e SMap3 We now have an SMap3\u003cTeamId, ClusterId, PersonId, Decision\u003e which we will use to model whether a PersonId is sitting with their TeamId at a given ClusterId. You may have noticed that our assignment decisions were along different dimensions. PersonId are assigned to DeskId while TeamId are assigned to ClusterId. Now is when we bring together the dimensions of DeskId and ClusterId. We want to maximize the number of times that a PersonId is assigned to the same ClusterId as their TeamId. Our objective function is going to be the sum of the personCoLocated decisions. To keep the optimizer from just turning all those values to 1.0 though we need to put constraints on them to make sure the necessary conditions are being met. The first necessary condition is that a PersonId is assigned to a DeskId in the given ClusterId. Let’s code that up. let personCoLocatedConstraints = ConstraintBuilder \"TeamPersonCoLocated\" { for t in teamIds do for c in clusterIds do for p in teams[t] -\u003e personCoLocated[t, c, p] \u003c== sum personDeskAssignment[p, In clusters[c]] } Here we are saying that for the solver to turn a given personLocated decision to 1.0, at least one of the personDeskAssignment variables for the PersonId must be 1.0 as well. We are calculating that using this expression: sum personDeskAssignment[p, In clusters[c]] You’ll see that the filter on the second dimensions is an In filter. The In filter takes a Set as an input which is why we had to store the values in the cluster collection as a Set\u003cDeskId\u003e. Now we need to cover the second condition, the TeamId must also be assigned to the ClusterId. let teamCoLocatedConstraints = ConstraintBuilder \"TeamCoLocated\" { for t in teamIds do for c in clusterIds do for p in teams[t] -\u003e personCoLocated[t, c, p] \u003c== teamClusterAssignment[t, c] } Here we are saying that if you want to turn the value of the personCoLocated decision to 1.0, you must have assigned the TeamId to that ClusterId. ","date":"2021-11-21","objectID":"/blog/2021/11/team-desk-assignments/:2:1","tags":null,"title":"Team Desk Assignments","uri":"/blog/2021/11/team-desk-assignments/"},{"categories":null,"content":"Putting it together We can now put together all the components of our model. Let’s create the LinearExpression which is our objective function. let colocationObjectiveExpr = sum personCoLocated let coLocationObjective = Objective.create \"MaximizeCoLocation\" Maximize colocationObjectiveExpr We are saying that we want to maximize the number of times that a PersonId is assigned to the same ClusterId as the TeamId they are assigned to. We have a secondary objective which is to minimize the number of times people are moved from their current seating assignment. To do this we create an expression which is the sum of whether a PersonId is assigned to the DeskId they are currently at. let maxRetentionExpr = seq { for (p, d) in currentPersonDeskAssignment -\u003e 1.0 * personDeskAssignment[p, d] } |\u003e Seq.sum let maxRetentionObjective = Objective.create \"MaxRetention\" Maximize maxRetentionExpr With our two objectives and various constraints we can compose our full model. let model = Model.create coLocationObjective |\u003e Model.addObjective maxRetentionObjective |\u003e Model.addConstraints eachPersonHasDeskConstraints |\u003e Model.addConstraints eachDeskOnlyOnceConstraints |\u003e Model.addConstraints eachTeamHasClusterConstraints |\u003e Model.addConstraints eachClusterOnlyOnceConstraints |\u003e Model.addConstraints personCoLocatedConstraints |\u003e Model.addConstraints teamCoLocatedConstraints We aren’t doing anything fancy so we will just use our default settings and attempt to solve. let settings = Settings.basic let result = Solver.solve settings model Now let’s create a simple function to print out the results if we find a solution. This will just extract which assignments the solver is recommending we use and print that out to a simple table in the console. match result with | Optimal sln -\u003e let personDeskAssignmentValues = Solution.getValues sln personDeskAssignment let selectedDeskAssignments = personDeskAssignmentValues |\u003e Map.toSeq |\u003e Seq.filter (fun (_, value) -\u003e value = 1.0) |\u003e Seq.map fst |\u003e readOnlyDict let teamClusterAssignmentValues = Solution.getValues sln teamClusterAssignment let selectedTeamClusterAssignment = teamClusterAssignmentValues |\u003e Map.toSeq |\u003e Seq.filter (fun (_, value) -\u003e value = 1.0) |\u003e Seq.map fst printfn \"Team/Cluster Assignments\" for (teamId, clusterId) in selectedTeamClusterAssignment do printfn $\"{teamId}\" printfn $\"{clusterId}\" printfn \"=== People ===\" for personId in teams.[teamId] do let deskId = selectedDeskAssignments[personId] let clusterId = deskToCluster[deskId] printfn $\"{personId} | {deskId} | {clusterId}\" | _ -\u003e printfn \"Uh Oh\" When we run all of this, we get the following result. Team/Cluster Assignments TeamId 1 ClusterId 2 === People === PersonId 1 | DeskId 5 | ClusterId 2 PersonId 2 | DeskId 4 | ClusterId 1 PersonId 3 | DeskId 7 | ClusterId 2 PersonId 4 | DeskId 8 | ClusterId 2 PersonId 5 | DeskId 6 | ClusterId 2 TeamId 2 ClusterId 4 === People === PersonId 6 | DeskId 15 | ClusterId 4 PersonId 7 | DeskId 14 | ClusterId 4 PersonId 8 | DeskId 16 | ClusterId 4 PersonId 9 | DeskId 1 | ClusterId 1 PersonId 10 | DeskId 13 | ClusterId 4 TeamId 3 ClusterId 3 === People === PersonId 11 | DeskId 9 | ClusterId 3 PersonId 12 | DeskId 10 | ClusterId 3 PersonId 13 | DeskId 11 | ClusterId 3 PersonId 14 | DeskId 12 | ClusterId 3 TeamId 4 ClusterId 5 === People === PersonId 15 | DeskId 20 | ClusterId 5 PersonId 16 | DeskId 17 | ClusterId 5 PersonId 17 | DeskId 19 | ClusterId 5 PersonId 18 | DeskId 18 | ClusterId 5 TeamId 5 ClusterId 6 === People === PersonId 19 | DeskId 23 | ClusterId 6 PersonId 20 | DeskId 24 | ClusterId 6 PersonId 21 | DeskId 22 | ClusterId 6 PersonId 22 | DeskId 21 | ClusterId 6 You will see that in most cases people are sitting with their teams but in some cases they are not. This is a product of a mismatch between the number of people on each team and the number of desks in each cluster. You can play with this model yourself here. Change the variables and see what results you get! This was a fun problem to put tog","date":"2021-11-21","objectID":"/blog/2021/11/team-desk-assignments/:3:0","tags":null,"title":"Team Desk Assignments","uri":"/blog/2021/11/team-desk-assignments/"},{"categories":null,"content":"I have been working on creating some types which allow me to wrap an array and index it with an int which has a Unit of Measure (UoM). Right now, if you want to index into an array with an int that has a UoM, you need to remove the units. [\u003cMeasure\u003e] type ItemIdx let a = [|1.0 .. 10.0|] let idx = 1\u003cItemIdx\u003e let x = a[idx] // This will raise an error saying that the type `int\u003cItemIdx\u003e` is not correct let y = a[int idx] // This will work because the units are removed when calling `int` You may think, “Matthew, that call to int is going to cause a problem, isn’t it?” That’s a great question. Let’s put together an experiment and see what assembly is generated. This code… let test (a: array\u003cfloat\u003e) = let x = 1 a[x] Will generate this assembly. ; Core CLR 6.0.21.52210 on amd64 _.test(System.Double[]) L0000: sub rsp, 0x28 L0004: vzeroupper L0007: cmp dword ptr [rcx+8], 1 L000b: jbe short L0017 L000d: vmovsd xmm0, [rcx+0x18] L0012: add rsp, 0x28 L0016: ret L0017: call 0x00007ffd109ee750 L001c: int3 Let’s add a UoM to the index and see what happens when we use it to index into the array while using int to remove the units. [\u003cMeasure\u003e] type ItemIdx let test (a: array\u003cfloat\u003e) = let x = 1\u003cItemIdx\u003e a[int x] // Does calling `int` here incur a performance penalty? And here is the assembly… ; Core CLR 6.0.21.52210 on amd64 _.test(System.Double[]) L0000: sub rsp, 0x28 L0004: vzeroupper L0007: cmp dword ptr [rcx+8], 1 L000b: jbe short L0017 L000d: vmovsd xmm0, [rcx+0x18] L0012: add rsp, 0x28 L0016: ret L0017: call 0x00007ffd109ee750 L001c: int3 You should notice that we are getting the exact same result. The F# compiler is smart enough to see that we are calling the int conversion function on a type that is already an int so it removes it. This kind of thing is annoying to have to do manually all the time and I really wanted a wrapper around an array which had a UoM type associated with the index. I decided to code up something simple. type ClassWrapper\u003c[\u003cMeasure\u003e] 'Measure, 'Value\u003e(values: array\u003c'Value\u003e) = member _.Values = values member this.Item with get (idx: int\u003c'Measure\u003e) = this.Values.[int idx] member this.Length = LanguagePrimitives.Int32WithMeasure\u003c'Measure\u003e this.Values.Length We now have a class which is taking an array\u003c'Value\u003e as part of its constructor and it is giving us a view of the underlying array which is forcing the use of an int with a UoM to retrieve values. This allows us to do the following. [\u003cMeasure\u003e] type ItemIdx let classWrapper = [|1.0 .. (float numberCount)|] |\u003e ClassWrapper\u003cItemIdx, float\u003e ClassWrapper will now force us to use an int\u003cItemIdx\u003e to retrieve values. You may think this is cumbersome but if you are working with many arrays simultaneously it can be easy to mix up which index is meant to be associated with which array. I like the compiler to be able to help me out so the idea of using UoM as a way provide some guarantees is nice provided there is not a speed penalty. I also thought, “You know, why not use a Struct instead of a Class to wrap the value? Using a Struct means the reference to the array will be on the stack, right? That should save you chasing a reference before getting to the array.” Rather than assuming that was the case I decided to put together a test using BenchmarkDotNet to verify my assumption was correct. ","date":"2021-11-16","objectID":"/blog/2021/11/benchmarkdotnet-gotcha/:0:0","tags":null,"title":"BenchmarkDotNet Gotcha with F#","uri":"/blog/2021/11/benchmarkdotnet-gotcha/"},{"categories":null,"content":"The Setup The first thing I need to do is define a Struct for wrapping my array. [\u003cStruct\u003e] type StructWrapper\u003c[\u003cMeasure\u003e] 'Measure, 'Value\u003e = val Values : array\u003c'Value\u003e new (values: array\u003c'Value\u003e) = { Values = values } member inline this.Item with inline get (idx: int\u003c'Measure\u003e) = this.Values[int idx] member this.Length = LanguagePrimitives.Int32WithMeasure\u003c'Measure\u003e this.Values.Length Then I setup some test data. I typically am working with small arrays so I’m just going to be summing up the values from 1.0 to 100.0 and I’ll perform that 100,000 times. I create my three different types for my testing. let iterations = 100_000 let numberCount = 100 let rawArray = [|1.0 .. (float numberCount)|] let classWrapper = [|1.0 .. (float numberCount)|] |\u003e ClassWrapper\u003cItemIdx, float\u003e let structWrapper = [|1.0 .. (float numberCount)|] |\u003e StructWrapper\u003cItemIdx, _\u003e Alright, data prepared, time to create some tests. I open the namespaces I need from BenchmarkDotNet and create my Benchmark class. I create three tests to see which approach is faster. I’m assuming that the raw array is the absolute limit (short of SIMD). type Benchmarks () = [\u003cBenchmark\u003e] member _.RawArray () = let mutable iterationIdx = 0 let mutable result = 0.0 while iterationIdx \u003c iterations do let mutable idx = 0 let len = rawArray.Length while idx \u003c len do result \u003c- result + rawArray[idx] idx \u003c- idx + 1 result \u003c- 0.0 // Reset iterationIdx \u003c- iterationIdx + 1 result [\u003cBenchmark\u003e] member _.ClassWrapper () = let mutable iterationIdx = 0 let mutable result = 0.0 while iterationIdx \u003c iterations do let mutable idx = 0\u003cItemIdx\u003e let len = classWrapper.Length while idx \u003c len do result \u003c- result + classWrapper[idx] idx \u003c- idx + 1\u003cItemIdx\u003e result \u003c- 0.0 // Reset iterationIdx \u003c- iterationIdx + 1 result [\u003cBenchmark\u003e] member _.StructWrapper () = let mutable iterationIdx = 0 let mutable result = 0.0 while iterationIdx \u003c iterations do let mutable idx = 0\u003cItemIdx\u003e let len = structWrapper.Length while idx \u003c len do result \u003c- result + structWrapper[idx] idx \u003c- idx + 1\u003cItemIdx\u003e result \u003c- 0.0 // Reset iterationIdx \u003c- iterationIdx + 1 result I run the benchmarks and get an unexpected result. Method Mean Error StdDev RawArray 6.434 ms 0.1241 ms 0.1100 ms ClassWrapper 6.969 ms 0.1177 ms 0.1101 ms StructWrapper 23.983 ms 0.2800 ms 0.2619 ms I am shocked that the StructWrapper performed so much more poorly that either the RawArray or ClassWrapper. This does not make any sense to me. If anything, StructWrapper should be faster than ClassWrapper but these numbers aren’t lying. The .NET Runtime has some special optimizations it can perform for Struct. In .NET 6.0 this includes keeping the values of the struct in the registers. You can check out the work here ","date":"2021-11-16","objectID":"/blog/2021/11/benchmarkdotnet-gotcha/:1:0","tags":null,"title":"BenchmarkDotNet Gotcha with F#","uri":"/blog/2021/11/benchmarkdotnet-gotcha/"},{"categories":null,"content":"The Fix I go to StackOverflow and Twitter to see if anyone had insight into what is going on. Upon the recommendation of Phillip Carter I move the code for generating the test data to inside the Benchmark class. When I do this, I get these results. Method Mean Error StdDev InternalRawArray 5.788 ms 0.0241 ms 0.0225 ms InternalClassWrapper 5.983 ms 0.0174 ms 0.0154 ms InternalStructWrapper 5.980 ms 0.0423 ms 0.0396 ms Now the performance is roughly equivalent. Apparently, there are some gotchas with the BenchmarkDotNet library and F# modules. I go ahead and define some additional types for wrapping an array. I wrap an array using a Record and a Record with the [\u003cStruct\u003e] attribute. I create tests where the data is defined inside the Benchmark class and tests where the data is defined in a separate module. Here is what I ended up finding. Method Mean Error StdDev InternalRawArray 5.839 ms 0.0517 ms 0.0431 ms ExternalRawArray 6.537 ms 0.0429 ms 0.0401 ms InternalClassWrapper 5.866 ms 0.0183 ms 0.0203 ms ExternalClassWrapper 6.903 ms 0.0734 ms 0.0686 ms InternalStructWrapper 6.032 ms 0.0933 ms 0.0827 ms ExternalStructWrapper 21.042 ms 0.0932 ms 0.0826 ms InternalRecordApproach 5.920 ms 0.0728 ms 0.0608 ms ExternalRecordApproach 6.899 ms 0.0760 ms 0.0674 ms InternalStructRecordApproach 5.899 ms 0.0947 ms 0.1297 ms ExternalStructRecordApproach 5.841 ms 0.0576 ms 0.0450 ms As you can see, I stumbled upon what appears to be a single outlier. You can also see that across the board the tests that are operating on data defined inside the Benchmark class outperform those where the data is defined externally. The only exception is the Struct Record but the difference in means is withen the noise of the tests. I think it’s important for an F# developer whose looking for performance to be aware that where data is declared can affect your benchmarks and could lead to incorrect conclusions. The guidance I received from Phillip was to declare the data in the Benchmark class. Bartosz Adamczewski recommends writing the library code in F# and the benchmarks in C#. This makes sense as I believe the BenchmarkDotNet library considers the C# use case primarily. If you would like to see the full set of tests you can check out the repo here. Until next time, stay safe out there and have fun with your benchmarking! Please send me an email at matthewcrews@gmail.com if you have any questions and subscribe so you can stay on top new posts and products I am offering. ","date":"2021-11-16","objectID":"/blog/2021/11/benchmarkdotnet-gotcha/:2:0","tags":null,"title":"BenchmarkDotNet Gotcha with F#","uri":"/blog/2021/11/benchmarkdotnet-gotcha/"},{"categories":null,"content":" NOTE: All the code can be found here. Feel free to follow along! ","date":"2021-11-13","objectID":"/blog/2021/11/records-as-keys-for-dictionaries/:0:0","tags":null,"title":"Accelerating Dictionary Lookup with Records as Keys","uri":"/blog/2021/11/records-as-keys-for-dictionaries/"},{"categories":null,"content":"The Problem I ran into an interesting problem a week ago and I think others may find some value in it. I am currently working on writing a small Discrete-Event Simulation engine for a manufacturing facility. This problem required a bespoke solution since the current products that are available on the market are built with Modelers in mind and not deployment. In our ideal scenario this engine gets embedded in a decision-making product that is run by a Plant Manager. The goal is to take a Discrete-Event model of the facility and then wrap it in an optimization loop to find the best possible schedule for the facility. For optimization to be effective, you need to be able to run the model as fast as possible. At each time step we need to evaluate how the processes in the facility are going to respond to the updated state. Let’s call this evaluation CalculateChange. The input for the CalculateChange is a type called Settings. Settings holds the parameters for the pieces of equipment in the facility. The result of calling CalculateChange will be a Changes type which holds the changes we will make to the model. In our case, for a given Settings we will always get the same Changes back from CalculateChange. This means that CalculateChange is a natural candidate for memoization. The easiest way to memoize a function, that I am aware of, is to store results in a Dictionary and check to see if a result is already stored in it. A silly example of memoization is the following. // Create a Dictionary to hold our results let cache = Dictionary () let memoizedFunction (a: int) = match cache.TryFindValue a with | true, result -\u003e // Yay, the result for `a` was in our cache result | false, _ -\u003e // Booooo! The result was not in our cache :/ // Now we have to call the expensive function let result = reallyExpensiveFunctionToCall a // We store the result so that it will be in the cache next time cache.[a] \u003c- result // Now return the result result Here we are trying to keep from having to evaluate reallyExpensiveFunctionToCall each time by storing the result. Memoization can be a powerful way to speed up your programs. You are trading memory footprint for speed. This means this technique should only be used in an environment where you have plenty of memory available and what you really need is speed. ","date":"2021-11-13","objectID":"/blog/2021/11/records-as-keys-for-dictionaries/:1:0","tags":null,"title":"Accelerating Dictionary Lookup with Records as Keys","uri":"/blog/2021/11/records-as-keys-for-dictionaries/"},{"categories":null,"content":"The Challenge This is where things take an interesting turn. Our Settings type is made up of a few arrays. Two of them are array\u003cfloat\u003e and one of them is an array\u003cBufferState\u003e. BufferState is a Discriminated Union with 3 cases: Full, Partial, and Empty. type BufferState = | Full | Partial | Empty type Settings = { Levels : array\u003cfloat\u003e MaxRates : array\u003cfloat\u003e Buffers : array\u003cBufferState\u003e } I am wanting to use the Settings type as a Key in a Dictionary. Now, some of you may already have alarms going off in your head. We are wanting to hash and check equality of something that has float in it. If you ask a question about this on StackOverflow, the first responses will typically be people telling you not to do that. I will echo their caution but add some context about when it is okay. Equality of any floating-point number is a notoriously difficult problem. Floating point math has rounding error built in which means that something that works mathematically may not work computationally. For example. Try putting this into an F# Interactive session and you will get false. // This will return FALSE 0.1 + 0.2 = 0.3 If you were taking a math class, the answer would be true, obviously. Using floating point math though 0.3 cannot be perfectly represented so what you get it when you put in 0.3 is 0.299999999999999988897769753748434595763683319091796875 Likewise, when you add 0.1 and 0.2 together you get 0.3000000000000000444089209850062616169452667236328125000 Those two numbers are very, very close but they are technically not the same. Okay, so I agree with the StackOverflow mob that testing equality of floats is fraught with danger. There are situations where it IS okay though. Those situations are when you are needing to compare float values that no math has been performed on. In my use case, the values in Settings are parameters that are being set on the condition of other parts of the facility. They are not the result of any computation. This means that I can reliably test their equality because no math has been performed on them. Note: I highly recommend reading the paper What Every Computer Scientist Should Know About Floating-Point Arithmetic by David Goldberg. It’s probably way more detail than you need at this time but if you are working with floating-point math it’s worth your time. What is nice is that F# is going to give us structural equality for free. This means using Settings as a key for a record should be no problem, right? Well, as with everything the answer is, “It depends…”. Earlier this year, Isaac Abraham wrote a great blog post for Compositional IT which went into the performance gains that can be had from writing custom equality and comparison in F#. I highly recommend you go read that article first before continuing. In that article Isaac showed that there can be significant speed gains from implementing your own equality on a Record. In his tests the speed up for a Dictionary lookup was anywhere from 2 to 3 times faster. That has significant implications on the runtime of your algorithm if there is a Dictionary lookup in the heart of an inner loop. In my use case I want to lookup results in a Dictionary at every single time step of the simulation because a Dictionary lookup is still several orders of magnitude cheaper than running the expensive calculation logic. Before we go doing anything though, let’s establish the baseline of our performance so we don’t flail around blindly. For this domain the values of Levels field can range from 0.0 to 100.0 and it can contain anywhere from 10 to 100 values. The values in MaxRates range from 0.0 to 10.0 and contain anywhere from 10 to 100 values. The values of the Buffers field are evenly distributed between the three possible states of BufferState and can be from 10 to 100 values. We’ll need to generate some data for us to test on. open System // Parameters for generating test data let rng = Random (123) let maxLevelValue = 100.0 let maxRateValue = 100.0 // How many loo","date":"2021-11-13","objectID":"/blog/2021/11/records-as-keys-for-dictionaries/:2:0","tags":null,"title":"Accelerating Dictionary Lookup with Records as Keys","uri":"/blog/2021/11/records-as-keys-for-dictionaries/"},{"categories":null,"content":"Simple Override Let’s do the next, obvious thing and try defining a new version of Settings in a new module called Simple where we will use the easiest means of overriding the default equality behavior. I am going to put this new type and its data in a separate module to isolate it to prevent type collision headaches. module Simple = [\u003cCustomEquality; NoComparison\u003e] type Settings = { Levels : array\u003cfloat\u003e MaxRates : array\u003cfloat\u003e Buffers : array\u003cBufferState\u003e } override this.Equals b = match b with | :? Settings as other -\u003e this.Levels = other.Levels \u0026\u0026 this.MaxRates = other.MaxRates \u0026\u0026 this.Buffers = other.Buffers | _ -\u003e false override this.GetHashCode () = hash (this.Levels, this.MaxRates, this.Buffers) Nothing exotic going on here. We’ve added the attributes [\u003cCustomEquality\u003e] and [\u003cNoComparison\u003e]. [\u003cCustomEquality\u003e] is telling the F# compiler that we are going to provide our own overloads for Equals and GetHashCode. We have to also add [\u003cNoComparison\u003e] because the compiler wants us to also define the necessary methods for performing comparisons. We aren’t testing that right now, so we are telling the compiler, “I don’t want to define comparison so just don’t allow me to compare it.” This is one of the downsides of overriding default behavior. Now that we’ve told the compiler we are defining equality, we have taken on more responsibility for how this type behaves. Let’s generate the necessary test data and update our Benchmark class with a new testing method. I just copy and paste the code that we used for the default Settings type into the module where we defined our new Settings with overrides. module Simple = // Type definition here... // This is why we defined that array of indices before. So we could generate new Settings types // that were populated with the same data. // We now generate the random Settings which uses our custom overrides let settings = seq { for vi in valueIndexes -\u003e { Levels = levels[vi.LevelsIdx] MaxRates = maxRates[vi.MaxRatesIdx] Buffers = buffers[vi.BufferStatesIdx] } // Making sure we are getting the type we want } |\u003e Array.ofSeq // The values we will test looking up in a Dictionary let settings = testIndexes |\u003e Array.map (fun idx -\u003e settings[idx]) // Create the dictionary for looking up Settings let settingsDictionary = settings |\u003e Array.mapi (fun i settings -\u003e KeyValuePair (settings, i)) |\u003e Dictionary And now we add our test to the Benchmarks class. We also moved the default Settings into a module of its own so now we need to prefix the initial test with Default. Our Benchmark class will now look like this. // Type to contain our performance tests type Benchmarks () = [\u003cBenchmark\u003e] member _.Default () = let mutable idx = 0 let mutable result = 0 while idx \u003c Default.settingsKeys.Length do let testKey = Default.settingsKeys[idx] result \u003c- Default.settingsDictionary[testKey] idx \u003c- idx + 1 result [\u003cBenchmark\u003e] member _.Simple () = let mutable idx = 0 let mutable result = 0 while idx \u003c Simple.settingsKeys.Length do let testKey = Simple.settingsKeys[idx] result \u003c- Simple.settingsDictionary[testKey] idx \u003c- idx + 1 result We do another build and re-run our benchmarks. Upon completion we get these results. Method Mean Error StdDev Default 37.69 ms 0.428 ms 0.357 ms Simple 37.82 ms 0.271 ms 0.241 ms This is wildly disappointing. This had no affect which honestly, is probably a good thing. What we just implemented is essentially the default behavior of F# so we shouldn’t expect a big change. At this point though, I got curious as to how exactly is F# generating hash codes for array\u003cfloat\u003e. I went digging into the F# source and found my way to the prim-types.fs file. One thing I notice in my sleuthing is that F# has some specialized functions for generating hashcodes for array\u003cint\u003e, array\u003cint64\u003e, and array\u003cbyte\u003e but I didn’t see any for array\u003cfloat\u003e. Hmm, what if we created one and used that for generating our hashcodes? ","date":"2021-11-13","objectID":"/blog/2021/11/records-as-keys-for-dictionaries/:3:0","tags":null,"title":"Accelerating Dictionary Lookup with Records as Keys","uri":"/blog/2021/11/records-as-keys-for-dictionaries/"},{"categories":null,"content":"HashCode for array\u003cfloat\u003e I decided to steal borrow the function that F# is using for combining hashcodes found here. I also copy and paste the specialized function for hashing array\u003cint\u003e found here and change it up a little to work with array\u003cfloat\u003e. I put this new functions into a new module FloatHash. module FloatHash = // Source: https://github.com/dotnet/fsharp/blob/dc81e22205550f0cedf4295b06c3a1e338c1cfa1/src/fsharp/FSharp.Core/prim-types.fs#L1625 let inline HashCombine nr x y = (x \u003c\u003c\u003c 1) + y + 631 * nr let HashFloatArray (x: array\u003cfloat\u003e) : int = let len = x.Length let mutable i = len - 1 let mutable acc = 0 while (i \u003e= 0) do acc \u003c- HashCombine i acc (int x.[i]) i \u003c- i - 1 acc Now we have the ingredients to make our own hashing function for our Settings type. Let’s define a new Settings using this new functionality. module FloatHash = // ...new functions here [\u003cCustomEquality; NoComparison\u003e] type Settings = { Levels : array\u003cfloat\u003e MaxRates : array\u003cfloat\u003e Buffers : array\u003cBufferState\u003e } override this.Equals b = match b with | :? Settings as other -\u003e this.Levels = other.Levels \u0026\u0026 this.MaxRates = other.MaxRates \u0026\u0026 this.Buffers = other.Buffers | _ -\u003e false override this.GetHashCode () = // We now use our `hashFloatArray` let levelsHash = HashFloatArray this.Levels let maxRatesHash = HashFloatArray this.MaxRates let buffersHash = this.Buffers.GetHashCode() hash (levelsHash, maxRatesHash, buffersHash) We again copy and paste the data generating code into this new module to create our test data with our new type. We then add a third method to our Benchmarks class to test this new approach. type Benchmarks () = // Previous benchmarks are still here. I'm just not posting the code for brevity [\u003cBenchmark\u003e] member _.FloatHash () = let mutable idx = 0 let mutable result = 0 while idx \u003c FloatHash.settingsKeys.Length do let testKey = FloatHash.settingsKeys[idx] result \u003c- FloatHash.settingsDictionary[testKey] idx \u003c- idx + 1 result After we compile and run our benchmarks, we get the following. Method Mean Error StdDev Default 37.12 ms 0.648 ms 0.606 ms Simple 36.84 ms 0.270 ms 0.253 ms FloatHash 10.12 ms 0.093 ms 0.087 ms Whoa! Now that’s interesting. We get a significant speed up when using a function that is specifically made for a array\u003cfloat\u003e. Now we are making progress. ","date":"2021-11-13","objectID":"/blog/2021/11/records-as-keys-for-dictionaries/:4:0","tags":null,"title":"Accelerating Dictionary Lookup with Records as Keys","uri":"/blog/2021/11/records-as-keys-for-dictionaries/"},{"categories":null,"content":"Faster by Ignoring Now, if you are one of the few people who clicked the links to the F# source code, you may notice that the version of HashFloatArray I wrote isn’t quite like the version that is in the F# source code. The F# source code only hashes up to the first 18 values. It ignores the elements after that. I’m assuming the rationale is that the first 18 elements are sufficient for generating a unique enough hashcode. Let’s create a new module FloatHashSort where we define a new Settings type that uses this abbreviated hashing. module FloatHashShort = // To limit the number of elements we use for hashing let defaultHashNodes = 18 let inline HashCombine nr x y = (x \u003c\u003c\u003c 1) + y + 631 * nr let HashFloatArray (x: array\u003cfloat\u003e) : int = let len = x.Length let mutable i = len - 1 if i \u003e defaultHashNodes then i \u003c- defaultHashNodes // limit the hash let mutable acc = 0 while (i \u003e= 0) do acc \u003c- HashCombine i acc (int x.[i]) i \u003c- i - 1 acc [\u003cCustomEquality; NoComparison\u003e] type Settings = { Levels : array\u003cfloat\u003e MaxRates : array\u003cfloat\u003e Buffers : array\u003cBufferState\u003e } override this.Equals b = match b with | :? Settings as other -\u003e this.Levels = other.Levels \u0026\u0026 this.MaxRates = other.MaxRates \u0026\u0026 this.Buffers = other.Buffers | _ -\u003e false override this.GetHashCode () = let levelsHash = HashFloatArray this.Levels let maxRatesHash = HashFloatArray this.MaxRates let buffersHash = this.Buffers.GetHashCode() hash (levelsHash, maxRatesHash, buffersHash) // There is also the data creation code here but I'm leaving it out because it's a repeat of // what you have seen already. And we add a fourth benchmark to Benchmarks. type Benchmarks () = // ...previous benchmarks here [\u003cBenchmark\u003e] member _.FloatHashShort () = let mutable idx = 0 let mutable result = 0 while idx \u003c FloatHashShort.settingsKeys.Length do let testKey = FloatHashShort.settingsKeys[idx] result \u003c- FloatHashShort.settingsDictionary[testKey] idx \u003c- idx + 1 result We recompile and run those benchmarks! Let’s see what we get. Method Mean Error StdDev Default 35.546 ms 0.5105 ms 0.3985 ms Simple 38.245 ms 0.3848 ms 0.3600 ms FloatHash 9.861 ms 0.0399 ms 0.0333 ms FloatHashShort 9.459 ms 0.0965 ms 0.0903 ms Alright! A little more progress. Nothing crazy but we’ll take everything we can get. ","date":"2021-11-13","objectID":"/blog/2021/11/records-as-keys-for-dictionaries/:5:0","tags":null,"title":"Accelerating Dictionary Lookup with Records as Keys","uri":"/blog/2021/11/records-as-keys-for-dictionaries/"},{"categories":null,"content":"Faster array\u003cfloat\u003e Equality We made a little progress now on the GetHashCode side of our problem. Let’s look at the Equals now. We would like to speed this up. Right now, we are using the built in functionality of F# to evaluate the equality of the arrays. Let’s write our own to see if we get more performance. We are going to write a function which is going to take two array\u003cfloat\u003e and test whether all the values are the same. We will be using a while loop in this case. From my understanding, all loops get compiled down to while loops in IL. Using a while will get early termination and the most compact assembly that I am aware of. For more info on optimizing .NET I refer you to Federico Andres Lois (@federicolois) and Bartosz Adamczewski (@badamczewski01). We create a new module, FloatArrayEquals, to hold our new approach. module FloatArrayEquals = // ...our updated hashing code is still here. Not shown for brevity // The function we will use to compare the values in two float arrays let FloatArrayEquals (a: array\u003cfloat\u003e) (b: array\u003cfloat\u003e) = if a.Length \u003c\u003e b.Length then invalidArg (nameof b) \"Cannot check equality on arrays of different lengths\" let mutable idx = 0 let mutable result = true // Use a while loop to create better assembly while idx \u003c a.Length \u0026\u0026 result do if a.[idx] \u003c\u003e b.[idx] then result \u003c- false idx \u003c- idx + 1 result [\u003cCustomEquality; NoComparison\u003e] type Settings = { Levels : array\u003cfloat\u003e MaxRates : array\u003cfloat\u003e Buffers : array\u003cBufferState\u003e } override this.Equals b = match b with | :? Settings as other -\u003e // We are using our new function to compare the values // in our array\u003cfloat\u003e (FloatArrayEquals this.Levels other.Levels) \u0026\u0026 (FloatArrayEquals this.MaxRates other.MaxRates) \u0026\u0026 this.Buffers = other.Buffers | _ -\u003e false override this.GetHashCode () = let levelsHash = HashFloatArray this.Levels let maxRatesHash = HashFloatArray this.MaxRates let buffersHash = this.Buffers.GetHashCode() hash (levelsHash, maxRatesHash, buffersHash) We add a new method to Benchmark… type Benchmarks () = // Previous benchmarks are still here... [\u003cBenchmark\u003e] member _.ArrayEquals () = let mutable idx = 0 let mutable result = 0 while idx \u003c ArrayEquals.settingsKeys.Length do let testKey = ArrayEquals.settingsKeys[idx] result \u003c- ArrayEquals.settingsDictionary[testKey] idx \u003c- idx + 1 result Then compile and run our tests to get… Method Mean Error StdDev Default 37.085 ms 0.1836 ms 0.1627 ms Simple 35.839 ms 0.2870 ms 0.2544 ms FloatHash 10.189 ms 0.1942 ms 0.1907 ms FloatHashShort 9.454 ms 0.1645 ms 0.1539 ms ArrayEquals 7.100 ms 0.1289 ms 0.1324 ms This is great! We are still making progress. Before F# had to figure out what method to call in order to evaluate equality. Here we can skip that and use a loop tuned for this exact problem. ","date":"2021-11-13","objectID":"/blog/2021/11/records-as-keys-for-dictionaries/:6:0","tags":null,"title":"Accelerating Dictionary Lookup with Records as Keys","uri":"/blog/2021/11/records-as-keys-for-dictionaries/"},{"categories":null,"content":"Enter the SIMD Now, I skipped something at the very beginning of this whole post. There was a version of my simulation code where the Settings type did not contain arrays. It was a more complex set of types. I’ve been on a journey to learn how to write faster code because speed is a huge differentiator in my field. I work in a .NET shop, but we still want to have fast simulations. During my wanderings I have found Data-Oriented Design and Entity Component Systems. These are broad topics, but they are really about arranging your data to achieve high performance. I have been building code with a more Entity Component style. I’m not building full Entity Component systems but I’m tending toward Structs of Arrays (SoA) instead of Arrays of Structs (AoS) as a way to organize data (AoS vs SoA). Settings is a type that has gone through that transformation. By arranging data in a way that is sympathetic to how computers work, we can achieve much better performance. One of the tools that becomes available to you when you arrange your data in nice contiguous blocks of data is SIMD instructions. Practically all modern processors have special instructions which operate on multiple pieces of data simultaneously. This is exactly what we are doing with the equality checks in our Equals methods. For our first step into SIMD, let’s use some SSE2 instructions to test the equality of array\u003cfloat\u003e. SSE2 allows us to operate on 128 bits at a time. This means we can test the equality of 2 float at the same time. Note: The Intrinsics libraries are designed as zero safeties type of library. It is incumbent on the developer to check whether the CPU actually has the required intrinsics available. Most modern x86 CPUs will have everything that I am showing. Best practice is to have code check as to whether the instructions are available and providing a fallback function if they are not. A fallback for us would simply be the code earlier in this post. Production code should contain checks and fallbacks. You have been warned. I am going to show you the function we will use for stepping through two arrays simultaneously and then break it down for you line by line. In this case I think it’s easier for you to see the whole picture first and then have me explain what is happening to you. module SseFloatArrayEquals = // Open some new namespace we will need open FSharp.NativeInterop open System.Runtime.Intrinsics.X86 open System.Runtime.Intrinsics // Note, we are still using the same hashing functions. There just not shown for brevity // This is the new function we will use for comparing values let equals (a: array\u003cfloat\u003e) (b: array\u003cfloat\u003e) = if a.Length \u003c\u003e b.Length then invalidArg (nameof b) \"Cannot check equality on arrays of different lengths\" let mutable result = true let mutable idx = 0 let lastBlockIdx = a.Length - (a.Length % Vector128\u003cfloat\u003e.Count) use aPointer = fixed a use bPointer = fixed b while idx \u003c lastBlockIdx \u0026\u0026 result do let aVector = Sse2.LoadVector128 (NativePtr.add aPointer idx) let bVector = Sse2.LoadVector128 (NativePtr.add bPointer idx) let comparison = Sse2.CompareEqual (aVector, bVector) let mask = Sse2.MoveMask comparison result \u003c- (mask = 3) idx \u003c- idx + Vector128\u003cfloat\u003e.Count while idx \u003c a.Length \u0026\u0026 result do if a.[idx] \u003c\u003e b.[idx] then result \u003c- false idx \u003c- idx + 1 result Okay, let’s unpack what is happening in equals. In the first couple of lines, I’m making sure that the two arrays are the same length. if a.Length \u003c\u003e b.Length then invalidArg (nameof b) \"Cannot check equality on arrays of different lengths\" Next I create some values I will be using to track my progress through the loop. let mutable result = true let mutable idx = 0 I then need to calculate the last index at which I can use the SIMD instructions. Past this index I will need to fall back to scalar operations since I don’t have a big enough chunk of data to work on. let lastBlockIdx = a.Length - (a.Length % Vector128\u003cfloat\u003e.Count) We now need to get a pointer to the a","date":"2021-11-13","objectID":"/blog/2021/11/records-as-keys-for-dictionaries/:7:0","tags":null,"title":"Accelerating Dictionary Lookup with Records as Keys","uri":"/blog/2021/11/records-as-keys-for-dictionaries/"},{"categories":null,"content":"Bytes all the way down There is a comparison that we haven’t looked at yet. It’s the equality of the Buffers field. You may remember that this field is an array\u003cBufferState\u003e. BufferState is a discriminated union and so far has been using the built in F# equality. Something important to note, the default DU is a reference type. This means that it is passed by reference and if you want to compare the equality of two different DUs, you’ll need to follow there references to get the underlying data. Fortunately, F# has added the ability to make struct DUs with the [\u003cStruct\u003e] keyword. This is great for high performance scenarios but there are downsides. I don’t have the space to go into it here but will refer you to Bartosz Sypytkowski’s excellent blog post here. It’s a great read and will provide you significant insight into how to get the most performance out of F#. So why do I care about the fact that I can make the BufferState DU into a struct? Well, if I’m storing all the data in an array and that data is a struct, then everything I need to compare will be contiguous. Now of course there are no hardware intrinsics in an x86 CPU for comparing F# types, but there are intrinsics for comparing raw bytes. If I can get a pointer to the array\u003cBufferState\u003e and simply compare the bytes for equality I can accelerate the equality check of the Buffers field as well. Let’s get to work. We’ll need to define a new type BufferStateStruct which is exactly the same as a BufferState but with the [\u003cStruct\u003e] attribute. We’ll put all this new code in a module called SseByteArrayEquals. module SseByteArrayEquals = [\u003cStruct\u003e] type BufferStateStruct = | Full | Partial | Empty Now we need to define a new equals function that can take two array\u003c'T\u003e and check their equality based on the bytes matching. Again, I’m going to throw the whole function at you and then break it down. It’s like what we went through in the previous section with some minor tweaks. module SseByteArrayEquals = let private equals\u003c'T when 'T : unmanaged\u003e (a: array\u003c'T\u003e) (b: array\u003c'T\u003e) = if a.Length \u003c\u003e b.Length then invalidArg (nameof b) \"Cannot perform equals on arrays of different lengths\" let len = a.Length * sizeof\u003c'T\u003e / sizeof\u003cbyte\u003e let mutable result = true let mutable idx = 0 let lastBlockIdx = len - (len % Vector128\u003cbyte\u003e.Count) use pointerA = fixed a use pointerB = fixed b let bytePointerA = pointerA |\u003e NativePtr.toNativeInt |\u003e NativePtr.ofNativeInt\u003cbyte\u003e let bytePointerB = pointerB |\u003e NativePtr.toNativeInt |\u003e NativePtr.ofNativeInt\u003cbyte\u003e while idx \u003clastBlockIdx \u0026\u0026 result do let aVector = Sse2.LoadVector128 (NativePtr.add bytePointerA idx) let bVector = Sse2.LoadVector128 (NativePtr.add bytePointerB idx) let comparison = Sse2.CompareEqual (aVector, bVector) let mask = Sse2.MoveMask (comparison) result \u003c- (mask = 65535) idx \u003c- idx + Vector128\u003cbyte\u003e.Count while idx \u003c len \u0026\u0026 result do result \u003c- ((NativePtr.get bytePointerA idx) = (NativePtr.get bytePointerB idx)) idx \u003c- idx + 1 result Our function declarations have changed slightly. We now take an array\u003c'T\u003e but we added the restriction unmanaged. You can read more about type restrictions here but this restriction is necessary for us to be able to view the array as just a set of bytes. At the beginning we have the same check to ensure that the arrays are the same length. if a.Length \u003c\u003e b.Length then invalidArg (nameof b) \"Cannot perform equals on arrays of different lengths\" We then need to know how long our arrays are in terms of the number of bytes since we will be operating on them as just bytes. let len = a.Length * sizeof\u003c'T\u003e / sizeof\u003cbyte\u003e We then create result to track what we have found, create an int to track our progress and calculate the index of the last block we will be able to process using vector operations. let mutable result = true let mutable idx = 0 let lastBlockIdx = len - (len % Vector128\u003cbyte\u003e.Count) We now get our pointers to pin the array down to prevent GC from moving things from underneath us. use pointerA =","date":"2021-11-13","objectID":"/blog/2021/11/records-as-keys-for-dictionaries/:8:0","tags":null,"title":"Accelerating Dictionary Lookup with Records as Keys","uri":"/blog/2021/11/records-as-keys-for-dictionaries/"},{"categories":null,"content":"Wrap Up I hope you enjoyed this journey. I had a lot of fun figuring out just how fast I could make this lookup since it is at the heart of some of the most performance sensitive code I wrote. I still have a lot to learn and if you have feedback, I would love to hear it! You can check out the code here. You can run the benchmarks yourself and see what you find. Please send me an email at matthewcrews@gmail.com if you have any questions and subscribe so you can stay on top new posts and products I am offering. ","date":"2021-11-13","objectID":"/blog/2021/11/records-as-keys-for-dictionaries/:9:0","tags":null,"title":"Accelerating Dictionary Lookup with Records as Keys","uri":"/blog/2021/11/records-as-keys-for-dictionaries/"},{"categories":null,"content":"Welcome to part 2 of this series. In the previous post we setup our problem which is to speed up the SliceMap family of types for sparse data. We created benchmarks and measured the performance of the current implementation. I gave a brief overview of a new approach I had come up with and showed how it failed miserably. We were in a depressing place at the end of the last post but hope burns eternal! I have already been researching approaches for this problem on and off for a year, so I didn’t expect the problem to be slain in a day. Rather than giving up, I went searching for answers. ","date":"2021-08-23","objectID":"/blog/2021/08/slicemap-rework-part-2/:0:0","tags":null,"title":"SliceMap Rework - Part 2","uri":"/blog/2021/08/slicemap-rework-part-2/"},{"categories":null,"content":"Enter Data-Oriented Design Recently I have been researching Data-Oriented Design. My first introduction to it was a great talk by Mike Acton as CppCon. I regularly watch talks on other languages and paradigms to grow my understanding of the field and this talk in particular struck a chord. While some may find Mike’s delivery a little brusque, I found it refreshing. The talk is littered with great lines, but the following is one of my favorite. Reality is not a hack you’re forced to deal with to solve your abstract, theoretical problem. Reality is the actual problem. Mike Acton Overall, Data-Oriented Design emphasizes data and its transformation as the key thing to design around. It generally eschews Object Orientation as a means of decomposing problems and instead looks at what data layouts and access patterns allow us to extract the maximum performance. This talk sent me deep down a rabbit hole. Eventually I found my way to Jonathan Blow who has given many great talks online. I decided to pick up the book “Data-Oriented Design” by Richard Fabian. I’m still struggling with how I could use Data-Oriented Design to solve my slicing problem when I came to chapter 6 which discusses Searching. On page 114 of the paperback Richard describes how we can have data structures for looking up data that keep track of the query patterns being used. Once a threshold is met, the data could be re-ordered to better suit how the data is being accessed. This was the moment of insight for me. “Wait!” I said to myself. “In real world use cases, you are often slicing across 1 dimension of the data many times in a row. Then you may start slicing across another dimension many times in a tight loop. Why not have the SliceMap re-order it’s data to be optimal for the types of lookups that are being performed!” ","date":"2021-08-23","objectID":"/blog/2021/08/slicemap-rework-part-2/:1:0","tags":null,"title":"SliceMap Rework - Part 2","uri":"/blog/2021/08/slicemap-rework-part-2/"},{"categories":null,"content":"Idea 3: Reorganizing Internals I went back to the drawing board and reworked how data was being stored in the SliceMap types. The internal fields of the 1 dimensional SliceMap remained simple. We give the SliceMap a comparer for comparing the keys when performing the Hadamard Product. keys is just a chunk of memory that is sorted. values is contiguous memory where the position is what determines the key it goes with. type SliceMap\u003c'k, 'v when 'k : comparison\u003e (comparer: IComparer\u003c'k\u003e, keys: ReadOnlyMemory\u003c'k\u003e, values: ReadOnlyMemory\u003c'v\u003e) = let comparer = comparer let keys = keys let values = values SliceMap2D gets a little more interesting. We need to remember that a SliceMap2D can be thought of as a table in a database where the primary key is made up of two fields: Key1 and Key2. Here is what some example data could look like. Key1 Key2 Value 1 “A” 2.0 1 “B” 8.0 1 “C” 3.0 2 “B” 1.7 2 “C” 1.7 3 “A” 9.4 3 “B” 4.6 … … Since we are trying to optimize the speed of slicing the data, we are willing to do some work up front to organize the data. When we initially create the SliceMap2D, we will sort the data by Key1 then Key2. This will allow us to use Run Length Encoding on the outer keys, Key1 in this case. We will store the length of the runs of the outer key in an IndexRange type. [\u003cStruct\u003e] type IndexRange = { Start : int Length : int } We will use two arrays for storing Key1 data. One array for the values of Key1, another for the IndexRange that corresponds to the key. We will call these fields OuterKeyValues and OuterKeyRanges respectively. Key2 and Values will be stored in a ReadOnlyMemory of their respective types. Key2 and Values have a 1 to 1 matching based on their location in their containers. We can now define SliceMap2DInternals for storing this information. [\u003cStruct\u003e] type SliceMap2DInternals\u003c'k1, 'k2, 'v when 'k1 : comparison and 'k2 : comparison\u003e = { OuterComparer : IComparer\u003c'k1\u003e InnerComparer : IComparer\u003c'k2\u003e OuterKeyValues : 'k1[] OuterKeyRanges : IndexRange[] InnerKeyValues : ReadOnlyMemory\u003c'k2\u003e Values : ReadOnlyMemory\u003c'v\u003e } Now, you may notice that I was talking about Key1 and Key2 but then switched to talking about OuterKey and InnerKey. This is where things may get confusing but trust me, we’ll get there! We need SliceMap2D to be able to restructure itself in order to provide fast slicing across Key1 or Key2. If Key1 data is stored in the OuterKey fields, then it is much faster to slice along Key1 because all we need to do it find the range of values it applies to and simply just slice the memory for InnerKeyValues and Values to create a SliceMap. If Key2 is stored in the InnerKeyValues field, it is difficult to slice because a particular value of Key2 could occur in multiple places in InnerKeyValues. But what if we were able to flip which key was stored in the OuterKeyValues and OuterKeyRanges fields and which one was stored in InnerKeyValues? Well, then we could slice along the Key2 dimension quickly since all its values would be contiguous after flipping. The “problem” is that F# is statically typed and doesn’t like you changing the type of fields. Fortunately, every problem in F# is solved with another type. Enter the SliceMap2DState. type SliceMap2DState\u003c'k1, 'k2, 'v when 'k1 : comparison and 'k2 : comparison\u003e = | Key1Key2 of SliceMap2DInternals\u003c'k1, 'k2, 'v\u003e | Key2Key1 of SliceMap2DInternals\u003c'k2, 'k1, 'v\u003e What this Discriminated Union is doing is containing the information for how the keys are stored in the SliceMap2DInternals. It tells us if Key1 is in the outer fields or if Key2 is. Now we can define SliceMap2D. type SliceMap2D\u003c'k1, 'k2, 'v when 'k1 : comparison and 'k2 : comparison\u003e (internalState: SliceMap2DState\u003c_, _, _\u003e) = let mutable internalState = internalState Notice, SliceMap2D is storing its state in a mutable field so it can change it when it wants. When you go to slice along a dimension, it will check how the data is laid out. If the data is not laid out for efficient slicing, it will swap","date":"2021-08-23","objectID":"/blog/2021/08/slicemap-rework-part-2/:2:0","tags":null,"title":"SliceMap Rework - Part 2","uri":"/blog/2021/08/slicemap-rework-part-2/"},{"categories":null,"content":"Did We Get Faster? In the previous post we ran our benchmarks against the current implementation, and we got the following timings. Method Mean Error StdDev DenseData 7.993 s 0.0748 s 0.0700 s MediumSparsity 2.154 s 0.0176 s 0.0156 s HighSparsity 1.209 s 0.0134 s 0.0126 s These are the timings we get for our new version of SliceMap2D with self-adjusting internals. Method Mean Error StdDev DenseData 379.05 ms 5.281 ms 4.940 ms MediumSparsity 113.99 ms 0.647 ms 0.574 ms HighSparsity 71.89 ms 0.636 ms 0.595 ms It looks a little better when we plot the performance against each other. So, it got a little faster 😊. I almost cried when I saw this. The fact that this problem has been tormenting me for over a year problem had something to do with it. We have even more gains on the horizon! There are several other things we can do to speed this up. Feel free to check out this repo and branch to see what all of the code looks like and run the benchmarks for yourself! I welcome feedback and ideas! Please send me an email at matthewcrews@gmail.com if you have any questions and subscribe so you can stay on top new posts and products I am offering. ","date":"2021-08-23","objectID":"/blog/2021/08/slicemap-rework-part-2/:3:0","tags":null,"title":"SliceMap Rework - Part 2","uri":"/blog/2021/08/slicemap-rework-part-2/"},{"categories":null,"content":"I have been on a journey to overhaul the underpinnings of the Flips library and it has been a humbling experience. Part of what I believe provides a unique value add compared to other libraries for Mathematical Planning is the SliceMap types. They provide a convenient method for subsetting your data that is intuitive for someone who writes optimization models. The SliceMap types are heavily influenced by the TupleDict type in the Gurobi library for Python. The major problem with SliceMap is that it does not scale well to sparse data sets. I knew this would be the case when I wrote it originally, but the performance was good enough at the time, so I decided to ship. Since then, this performance degradation has been a splinter in my mind that I simply could not shake. There had to be a way to do this well, I just didn’t know it yet. I have spent a year on and off searching for answers. I now have a shelf in my office that is full of books on algorithms, data structures, and multi-dimensional data structures. It’s all the best resources I could find that could possibly help me with this problem. While it was fun reading them, it ultimately didn’t help much. All these fancy data structures didn’t support what appeared to be a unique use case. This series of posts is going to be me taking you through the journey to where I had the breakthrough and then kept on pushing. I have kept a repo set aside with branches which illustrate each of the milestones so you can see the impact. It includes code for easy benchmarking with benchmarkDotNet and code that you can profile yourself if you wish. My goal is for you to see the mistakes and understand why some approaches are inefficient, even if the Big O notation says they should be faster. Also, I am not a “Pro” performance person. I’m actively learning to diagnose and address performance issues so if you see that I missed something, please let me know! Let’s start our journey with understanding the domain we are working in and what the requirements of SliceMap are. ","date":"2021-08-16","objectID":"/blog/2021/08/slicemap-rework-part-1/:0:0","tags":null,"title":"SliceMap Rework - Part 1","uri":"/blog/2021/08/slicemap-rework-part-1/"},{"categories":null,"content":"The Linear Programming Domain SliceMap is a set of types specifically for working in the Linear Programming and Mixed-Integer Programming domains. I often refer to these domains as “Mathematical Planning” because they are primarily concerned with using mathematics to find the best possible plans. The word “Program” used to mean “Plan” but now the word “Program” has become overloaded since the advent of Computer Science, so I don’t like to use it anymore. The academic literature still calls it “Linear Programming” and “Mixed-Integer Programming” though so if you go searching for more resources, you’ll need to use the original terms. The foundation of the LP and MIP domains is the humble Linear Expression. The term “Linear” in mathematics carries a special meaning that I won’t go into here. The key thing you need to know is that these expressions are straight. They are not curved in any way. Here are some examples of Linear Expressions: $$1.0 + x_{1} + 2 \\times x_{2}$$ $$3.5 \\times x_{1} + 5.7 \\times x_{2}$$ $$3.5 \\times x_{1} + 14.0$$ The most important thing to note is that you never see two $x$ multiplied together and there is no division. Here the $x$ values correspond to Decisions that we want to evaluate using Mathematical Planning. We can model this small domain using a simple set of types in F#. type DecisionType = | Boolean | Integer of LowerBound:float * UpperBound:float | Continuous of LowerBound:float * UpperBound:float type DecisionName = DecisionName of string type Decision = { Name : DecisionName Type : DecisionType } type LinearExpr = | Float of float | Decision of Decision | Scale of scale: float * expr: LinearExpr | Add of lExpr: LinearExpr * rExpr: LinearExpr This doesn’t show you how these can interact though. If we add the operators for these types, we get the following. type DecisionType = | Boolean | Integer of LowerBound:float * UpperBound:float | Continuous of LowerBound:float * UpperBound:float type DecisionName = DecisionName of string type Decision = { Name : DecisionName Type : DecisionType } with static member ( + ) (l: float, r: Decision) = LinearExpr.Add (LinearExpr.Float l, LinearExpr.Decision r) static member ( + ) (l: Decision, r: Decision) = LinearExpr.Add (LinearExpr.Decision l, LinearExpr.Decision r) static member ( * ) (l: float, r: Decision) = LinearExpr.Scale (l, LinearExpr.Decision r) [\u003cRequireQualifiedAccess\u003e] type LinearExpr = | Float of float | Decision of Decision | Scale of scale: float * expr: LinearExpr | Add of lExpr: LinearExpr * rExpr: LinearExpr static member ( + ) (l: float, r: LinearExpr) = LinearExpr.Add (LinearExpr.Float l, r) static member ( + ) (l: Decision, r: LinearExpr) = LinearExpr.Add (LinearExpr.Decision l, r) static member ( + ) (l: LinearExpr, r: LinearExpr) = LinearExpr.Add (l, r) static member ( * ) (l: float, r: LinearExpr) = LinearExpr.Scale (l, r) static member ( * ) (l: LinearExpr, r: float) = LinearExpr.Scale (r, l) static member Zero = LinearExpr.Float 0.0 The reason this is important is that we want to use SliceMap to be able to subset these types, multiply them together, and then sum the result. We aren’t just working with float values. It’s a more complex domain than that. If it was just float, I would have been trying to use SIMD or AVX instructions. ","date":"2021-08-16","objectID":"/blog/2021/08/slicemap-rework-part-1/:1:0","tags":null,"title":"SliceMap Rework - Part 1","uri":"/blog/2021/08/slicemap-rework-part-1/"},{"categories":null,"content":"SliceMap Requirements Okay, we’ve talked a little about the primitives we are working with, let’s talk about what SliceMap needs to be able to do. Now, SliceMap is like tuples in F#. Tuples are not a single type; they are a family of types. A 2-element tuple is not the same as a 3-element tuple. In the same way SliceMap is a family of types and each has a dimensionality to it. The dimensionality corresponds to the keys being used to index the value. A SliceMap has a 1-dimensional key. A SliceMap2D has a 2-dimensional key and so on. Right now, I only plan on supporting up to a 5D key. I have not come across a real-world problem that needed a higher dimensional key. I think the best analogy for explaining what SliceMaps are meant to do is with database tables. Think of each SliceMap as a table where the 'key is unique and there is a corresponding value 'value. The table would look something like this. Key Value 1 2.0 2 1.7 3 2.5 … … Now, a SliceMap2D would have two columns for the keys like so. Key1 Key2 Value 1 “A” 2.0 1 “B” 8.0 1 “C” 3.0 2 “B” 1.7 2 “C” 1.7 3 “A” 9.4 3 “B” 4.6 … … Now, we would like to be able to subset these datasets quickly and easily. To simplify this for the developer, the SliceMap types provide several overloads for the Item method. Let’s look at an example. We will fill a SliceMap2D with some values and then select only the values where the first key is equal to 1 and we can slice it again but only take the values where the second key equals 2. let key1 = [1..5] let key2 = [1..2] let sm2 = SliceMap2D [ for k1 in key1 do for k2 in key2 -\u003e k1, k2, k1 + k2 ] // slice will only contain the values where the first index = 1 let slice1 = sm2[1, All] // slice2 will only contain values where the second index = 2 let slice2 = sm2[All, 2] This “slicing” provides a terse way to quickly filter down to the values that we care about. We then add the ability to take the Hadamard Product of two SliceMap. Think of this as an inner join on two tables. Where the keys match, the value in the two tables is multiplied. In Matlab and other languages it is the .* operator. It’s meant to signify element by element multiplication. In our case, whenever the keys match the values are multiplied. Here’s a quick example of what it looks like let a = SliceMap [for i in 1..5 -\u003e i, i] let b = SliceMap [for i in 1..3 -\u003e i, i * i] let c = a .* b // The key/value pairs in `c` will be // [(1, 1); (2, 8); (3, 27)] The Hadamard Product of two SliceMap allow us to quickly multiply values together for the same key which is one of the cornerstones of Mathematical Planning. Finally, we then need to sum of these values to get the final result. We define a sum function which knows how to take any SliceMap and return the total of the values that it contains. let a = SliceMap [for i in 1..5 -\u003e i, i] let b = SliceMap [for i in 1..3 -\u003e i, i * i] let c = a .* b let total = sum c // total is 36 If you want a deeper explanation of SliceMap and how they are meant to work, you can check out this page. ","date":"2021-08-16","objectID":"/blog/2021/08/slicemap-rework-part-1/:2:0","tags":null,"title":"SliceMap Rework - Part 1","uri":"/blog/2021/08/slicemap-rework-part-1/"},{"categories":null,"content":"Baseline Test Note: If you want to see this code all at once, check out this repo and branch Now that we just had a whirlwind introduction to the Mathematical Planning domain and SliceMap, we can start talking about the implementation. We want to rewrite the SliceMap implementations to support fast slicing, Hadamard Product, and summing of values. Those are the critical operations. We also want it to be performance as the data becomes more sparse. Before we go about rewriting though, we should know what our current performance is. The current implementation of SliceMap assumes that data is dense across all dimensions. This means that when it sums up values or performs a Hadamard Product, it will check every combination of keys to see if there is a value. Currently the actual values of the SliceMap are stored in a Dictionary and are accessed using a TryGetValue. For dense data, this is not a problem because every entry will have a value. As data gets sparse though, you end up doing a lot of unnecessary work. For our baseline benchmarks we are going to test 3 different scenarios: Dense, Medium Sparsity, and High Sparsity. Dense for us will mean there is a value for every key. Medium sparsity will only have values for 10% of the possible keys. High Sparsity will have values only 1% of the time. Here is the code for setting up our data. We will have a set of Cities modeled as an int with a Unit of Measure and a set of Trucks also modeled as an int with Unit of Measure. [\u003cMeasure\u003e] type City [\u003cMeasure\u003e] type Truck let rng = Random 123 let numberOfCities = 1_000 let numberOfTrucks = 1_000 let cities = [| for c in 1 .. numberOfCities -\u003e LanguagePrimitives.Int32WithMeasure\u003cCity\u003e c |] let trucks = [| for t in 1 .. numberOfTrucks -\u003e LanguagePrimitives.Int32WithMeasure\u003cTruck\u003e t|] We then create some random Cost and Capacity data that we will use in our SliceMaps. let costs = SliceMap [| for t in trucks -\u003e t, 100.0 + 10.0 * rng.NextDouble() |] let capacity = SliceMap [| for t in trucks -\u003e t, 1_000.0 + 10.0 * rng.NextDouble () |] We now create a Decision for the pairs of City and Truck that we are modeling. let cityTruckPairs = [| for c in cities do for t in trucks -\u003e c, t |] // For the Dense case let decisions = SliceMap2D [| for (c, t) in cityTruckPairs -\u003e let decisionName = DecisionName ($\"{c.ToString()}_{t.ToString()}\") c, t, { Name = decisionName; Type = DecisionType.Boolean } |] For the Medium and Highly sparse cases we generate our SliceMap2D of Decision differently. We randomly omit pairs of City and Truck to make the data sparse. // Medium Density case let densityPercent = 0.10 let cityTruckPairs = [| for c in cities do for t in trucks -\u003e if rng.NextDouble() \u003c densityPercent then Some (c, t) else None |] |\u003e Array.choose id // Medium Density decisions let decisions = SliceMap2D [| for (c, t) in cityTruckPairs -\u003e let decisionName = DecisionName ($\"{c.ToString()}_{t.ToString()}\") c, t, { Name = decisionName; Type = DecisionType.Boolean } |] And for Sparse we only accept 1% of the pairs. let densityPercent = 0.01 let cityTruckPairs = [| for c in cities do for t in trucks -\u003e if rng.NextDouble() \u003c densityPercent then Some (c, t) else None |] |\u003e Array.choose id let decisions = SliceMap2D [| for (c, t) in cityTruckPairs -\u003e let decisionName = DecisionName ($\"{c.ToString()}_{t.ToString()}\") c, t, { Name = decisionName; Type = DecisionType.Boolean } |] Now that we have setup our data, we can run some benchmarks! We will use benchmarkDotNet which makes this type of work incredibly easy. For each case we create a loop function which iterates through each of the Cities in our data and computes a linear expression using the SliceMaps we populated, the Hadamard Product, and summing them. We loop through all the Cities 10 times so that our test is sufficiently long. Anything under 100 ms causes a warning from benchmarkDotNet that the test may be too short and the results not valid. We are assigning to a mutable value and returning it to ensure tha","date":"2021-08-16","objectID":"/blog/2021/08/slicemap-rework-part-1/:3:0","tags":null,"title":"SliceMap Rework - Part 1","uri":"/blog/2021/08/slicemap-rework-part-1/"},{"categories":null,"content":"First failure: Clever IEnumerable The first thing I tried when I came back to this problem was to try to be cleverer about which keys I iterated through. The idea was to encode the outer keys with a Run-Length Encoding and the inner keys would have an index for the next time that value showed up. This meant if I slice the outer dimension, I will only look at the small section for that run. If I slice the inner dimensions, I can find the first instance of a value and then just skip from row to row. I know that’s not a detailed explanation, but I don’t think it’s worth going into the details. You can see the code here if you want. The reason it’s not worth the explanation can be found in the performance results. | Method | Mean | Error | StdDev | |--------------- |---------:|---------:|---------:| | DenseData | 86.070 s | 0.4374 s | 0.4091 s | | MediumSparsity | 10.592 s | 0.0938 s | 0.0832 s | | HighSparsity | 3.343 s | 0.0289 s | 0.0270 s | This was deeply disappointing. Even though I was able to skip so many values, the overhead of the IEnumerable and the fact that I was skipping all over these arrays caused havoc. This is a case of trying to be too clever and it can bite you. I may have theoretically been doing less work, but the algorithm was so much less friendly for the CPU and I suffered the consequences. Now, I didn’t want to give up right away with this approach. I took a long time coming up with it. I thought that maybe I had done something silly and had made some error. I decided to profile to find what I was doing wrong. I like to use dotTrace and dotMemory for this kind of work. I’ve recently also started using Superluminal which shows a lot of promise. ","date":"2021-08-16","objectID":"/blog/2021/08/slicemap-rework-part-1/:4:0","tags":null,"title":"SliceMap Rework - Part 1","uri":"/blog/2021/08/slicemap-rework-part-1/"},{"categories":null,"content":"Second Failure: Struct Tuples Isn’t Enough When I ran the code through dotMemory I saw that I was generating a huge number of tuples. These were coming from the calls to Seq.unfold which was using a normal F# tuple which is a ref type. I knew I could cut down on the GC if I converted these to struct tuples. I went ahead and made the change which you can check out here. Sadly, this helped some, but not enough. | Method | Mean | Error | StdDev | |--------------- |---------:|---------:|---------:| | DenseData | 51.158 s | 0.1429 s | 0.1337 s | | MediumSparsity | 7.619 s | 0.0367 s | 0.0344 s | | HighSparsity | 2.934 s | 0.0223 s | 0.0209 s | There were some additional things that I could have done to try to eek out some more performance but I knew none of them had the potential to give me a 10x or more improvement which was what I was really looking for. I sat with this for a day trying to see if I had missed something obvious but nothing came to me. It looks like the clever enumeration approach is a dead end. ","date":"2021-08-16","objectID":"/blog/2021/08/slicemap-rework-part-1/:5:0","tags":null,"title":"SliceMap Rework - Part 1","uri":"/blog/2021/08/slicemap-rework-part-1/"},{"categories":null,"content":"Where from here? Fortunately, this story does have a good ending but it will have to wait for my next post. I had to go back to the drawing board and rethink my approach. Fortunately, I had picked up Data-Oriented Design and had been reading through it for inspiration. In it I found an idea that proved to be the key to unlocking this problem. Sorry to leave you hanging but this dad needs some sleep 😴. Please send me an email at matthewcrews@gmail.com if you have any questions and subscribe so you can stay on top new posts and products I am offering. ","date":"2021-08-16","objectID":"/blog/2021/08/slicemap-rework-part-1/:6:0","tags":null,"title":"SliceMap Rework - Part 1","uri":"/blog/2021/08/slicemap-rework-part-1/"},{"categories":null,"content":"I was recently asked about what resources I would recommend for Linear Programming. My response was, “Are you interested in problem formulation? Solution techniques? Algorithmic implementation?” The answer was, “Yes!” Here are the resources that I would recommend for those wanting to dive deep into Linear Programming (LP). Note, I will be focusing strictly on LP instead of Mixed-Integer Programming (MIP). MIP is a logical evolution of LP but brings in a whole host of other challenges. LP is the foundation for any MIP solver, so it is worth starting with LP. ","date":"2021-05-04","objectID":"/blog/2021/05/2021-05-04/:0:0","tags":null,"title":"Resources for Linear Programming","uri":"/blog/2021/05/2021-05-04/"},{"categories":null,"content":"Formulation I believe the foundation of Linear Programming begins with model formulation. Many textbooks start with Simplex Algorithm and explain how it works. I believe this is backwards. As a practitioner, I spend little time implementing the Simplex Algorithm and all of my time on writing models. Therefore, I believe understanding what kinds of problems can be modeled with LP should be where someone starts. I have found no better book for this than Model Building in Mathematical Programming by H. Paul Williams. It does a fantastic job of talking through real world problems and explaining how to model them. I have referred to this book more than any other in my time actually “doing” Linear Programming. Another great intro book would be An Illustrated Guide to Linear Programming by Saul I. Gass. This is possibly the gentlest introduction to the subject outside of a workshop. It is short, sweet, and to point. The example problems are straightforward and easy to follow. As the title suggests, there are lots of illustrations. Saul is trying to make the tool of LP as approachable as possible. ","date":"2021-05-04","objectID":"/blog/2021/05/2021-05-04/:1:0","tags":null,"title":"Resources for Linear Programming","uri":"/blog/2021/05/2021-05-04/"},{"categories":null,"content":"Solution Techniques I think of the workflow around Mathematical Planning as being like writing queries for a database. You can use a database if you can write a query for it, typically in SQL. You don’t have to understand the internals of how a database works to get tremendous value out of it. In the same way, you can derive significant value from Mathematical Planning without needing to understand how a Solver works. For the curious though, it’s fun to understand how things work. Modern LP Solvers are miracles of engineering and the math behind them is still comprehensible for someone with a background in Linear Algebra. If you want to understand the variety of ways that LPs can be solved I would recommend Linear Programming by Vasek Chvatal and Introduction to Linear Optimization by Dimitris Bertsimas and John N. Tsitsiklis. I have gotten more out of Chvatal’s book, but it is rarer and more difficult to get ahold of. You may need to search around for a good price on it. It’s worth it if you can though. There are nuances covered in it that I haven’t found in any other textbook. I also offered up “Introduction to Linear Optimization” as an easier book to get ahold of. It covers much of the same material, but I found their pseudocode a little more difficult to follow. I eventually got it, but it was a struggle at times. This is in part due to multiple ways of stating the Linear Programming problem. Every book claims they are presenting the “standard form” of the problem. This is not true. There is no “standard form”. Standard Form only exists within a given book. I would often keep notes for each book on what they meant by “standard form”. ","date":"2021-05-04","objectID":"/blog/2021/05/2021-05-04/:2:0","tags":null,"title":"Resources for Linear Programming","uri":"/blog/2021/05/2021-05-04/"},{"categories":null,"content":"Algorithmic Implementation We’ve offered some resources on formulation and solution techniques, but what about actually building a solver? First, I would suggest against it unless your motivation is purely out of curiosity. Solvers are complex and there are many edge cases that you must deal with. Let’s say you are committed to the task though and you really wanted to write one. First, make sure you grasp the resources in the previous sections. Chvatal’s book has some refinements on the Simplex and Dual Simplex that will be critical if you want to write a high-performance solver. One book that I have really enjoyed that explicitly focuses on writing a LP Solver is Computational Techniques of the Simplex Method by Istvan Maros. Unfortunately, it is expensive. You could probably get away without this one, but I found it useful for finding the appropriate research papers. When I was at university it was available as a PDF from the library. When I left university, I bought a copy once I had some extra cash for books. If you plan on writing a solver, you will be doing an enormous amount of sparse linear algebra. Almost all real-world problems involve sparse, if not hyper-sparse, matrices. A fast solver must take advantage of the sparsity of the data. The best resource I have found is Direct Methods for Sparse Linear Systems by Timothy Davis. You can even find Dr. Davis’s lectures on YouTube. The example code is all in C but if you have come this far, understanding C will not be much of an obstacle for you. Alright, I may have saved the best for last. Dr. Robert Bixby is one of the founders of Gurobi and the first author of CPLEX. He’s a bit of a legend when it comes to writing LP solvers. I randomly came across a lecture he gave on how to implement a high-quality LP solver on YouTube. It’s three parts and if you are serious about writing a solver, they are gold. You can find the videos here: Part1, Part2, and Part3. These are extremely valuable in giving you an overview of what it takes to write a good solver and where the big pain points are. You will come to admire the amazing engineering that has gone into modern solvers. ","date":"2021-05-04","objectID":"/blog/2021/05/2021-05-04/:3:0","tags":null,"title":"Resources for Linear Programming","uri":"/blog/2021/05/2021-05-04/"},{"categories":null,"content":"Wrap Up So that’s what I would recommend. I know that was a lot of books but that’s where the resources are for understanding LP. To go deeper you will need to start diving into research papers. This blog will also continue to feature modeling examples. I took a short break from blogging as I was changing jobs, but I am picking it back up again. You will see plenty of examples on how to formulate problems on this blog. Please send me an email at matthewcrews@gmail.com if you have any questions and subscribe so you can stay on top new posts and products I am offering. ","date":"2021-05-04","objectID":"/blog/2021/05/2021-05-04/:4:0","tags":null,"title":"Resources for Linear Programming","uri":"/blog/2021/05/2021-05-04/"},{"categories":null,"content":"I’ve continued to consult with my friend on the job assignments problem that I have been discussing in post 1 and post 2. At first, he was excited about what we had come up with but I knew there were likely more complexities that had not been uncovered yet. He went back to the client and came away with some new information. He told me, “Mathew, it turns out that machines have limited capacity. We have to limit how much work is assigned to them.” “Not a problem,” I respond. I tell me friend that it is straightforward to add constraints to the model which limit how much work is assigned to a machine. Let me walk you through how I update the model we created in post 1 and post 2 to take this new limitation into consideration. Note: The full code for this post can be found here ","date":"2021-02-03","objectID":"/blog/2021/02/2021-02-04/:0:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 3","uri":"/blog/2021/02/2021-02-04/"},{"categories":null,"content":"Machine Capacity Constraints The first thing we need to know is how much capacity machines have. My friend tells me that they are limited to 24.0 units of work. Our jobs are coming in sizes of 1, 2, or 3. Let’s create a constraint for each machine which states that the total loading cannot exceed this amount. // Limit on how much work a machine can be assigned let maxMachineCapacity = 24.0 // Machines have a limited capacity let maxMachineCapacityConstraints = ConstraintBuilder \"MachineCapacity\" { for machine in machines -\u003e sum (assignments[machine, All, All] .* jobSizes) \u003c== maxMachineCapacity } Remember, machines is the list of machines available for us to assign work to. We are looping through each machine and creating a constraint which says the sum of the work assigned to the machine cannot exceed the max capacity, maxMachineCapacity. assignments is a SliceMap indexed by machine, job-type, and job where the value is a Boolean decision. 1 indicates that we are assigning the job to the machine and 0 indicates that we are not. The notation assignments.[machine, All, All] is a “slice” which says, “Give me the assignments for this machine across all job-types and jobs.” jobSizes is another SliceMap where the key is a job and the value is the size of the job. We multiply the decisions by the size of the job using the Hadamard Product .*. Now that we have created some capacity constraints for the machines, let’s add them to our model and try to solve. // Compose the model let model = Model.addObjective minSetupsObjective |\u003e Model.addConstraints maxWorkConstraints |\u003e Model.addConstraints minWorkConstraints |\u003e Model.addConstraint maxWorkDifferenceConstraint |\u003e Model.addConstraints setupConstraints |\u003e Model.addConstraints jobsAssignmentConstraints |\u003e Model.addConstraints maxJobTypeDConstraints |\u003e Model.addConstraints maxMachineCapacityConstraints // \u003c- New constraints // Give the solver plenty of time to find a solution let settings = { Settings.basic with MaxDuration = 60_000L } let result = Solver.solve settings model We’ve cleaned up some of our code from earlier posts. All the solving is now abstracted behind the Scheduler.schedule function. It returns a new type if the solver can find a solution, MachineAssignments. This type contains a list machines and the jobs that are assigned to it. type MachineAssignment = { Machine : Machine Jobs : Job list } type MachineAssignments = MachineAssignments of MachineAssignment list We now call Scheduler.schedule to see if we can find a plan which fits our requirements. let scheduleResult = Scheduler.schedule maxWorkDifference maxJobTypeDPercentage maxMachineCapacity jobs machines We would like to have the script print out some nice output. We created a function, Printer.MachineAssignments.print, which provides nice clean output if we are able to solve the problem. Let’s call this function in the case that our solver successfully solved. match scheduleResult with | Result.Ok assignments -\u003e Printer.MachineAssignments.print assignments | Result.Error msg -\u003e printfn $\"{msg}\" What do we get? \u003e match scheduleResult with - | Result.Ok assignments -\u003e Printer.MachineAssignments.print assignments - | Result.Error msg -\u003e printfn $\"{msg}\";; Unable to solve val it : unit = () Uh oh, the solver failed to find a solution to our problem. What went wrong? ","date":"2021-02-03","objectID":"/blog/2021/02/2021-02-04/:1:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 3","uri":"/blog/2021/02/2021-02-04/"},{"categories":null,"content":"When the Solver Fails The solver was not able to find a solution. It is reporting “Unable to solve”. How can this be? We were able to solve this problem before. What has changed? Let’s think about it. We have added constraints which state that a machine cannot be overloaded. Overloaded in this case means anything over 24.0. Previously we were loading the machines up to 28.0, 29.0, or 30.0. We need to introduce a new concept to our vocabulary, “Infeasible”. Infeasible is a term you will find frequently in the optimization literature. In this context what it means is that there is no solution to the problem. Our problem is overly constrained. What other constraints combined with our new machine capacity constraints could be causing this problem? I’ll give you a hint, it’s the machine assignment constraints. Previously we defined a set of constraints, jobsAssignmentConstraints, which stated that every job must be assigned to a machine. In this new world though, that is not possible. There is simply too much work given the capacity of the machines. Therefore, the solver cannot find a solution. This is when we need to go back to the business and discuss priorities. What is truly the most important thing? In this scenario, I was able to discuss the problem with my friend. We agreed that the first priority is to fully utilize the machines. After that, we want to minimize the number of different jobs that a machine processes. This is an example of multi-objective optimization. The idea is that there is a series of objective in order of importance. You iteratively solve for each objective. The mechanics of how this works will need to wait for another post. Fortunately, multi-objective models are simple to express with Flips. We add the objectives to the model in the order of their priority. ","date":"2021-02-03","objectID":"/blog/2021/02/2021-02-04/:2:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 3","uri":"/blog/2021/02/2021-02-04/"},{"categories":null,"content":"Mult-Objective Formulation We need to create a new objective for maximizing the loading of machines. Let’s do that by first creating an expression which evaluates the total machine loading. // Maximize Utilization expression let maxUtilizationExpression = sum (assignments .* jobSizes) The maxUtilizationExpression expression evaluates just how much we we have assigned to all machines. We can use this to create an objective. let maxUtilizationObjective = Objective.create \"MaxUtilization\" Maximize maxUtilizationExpression This objective states that we would like to maximize the loading of the machines. We will use this new objective as the first objective of our model. We will also omit the jobsAssignmentConstraints that existed before since we no longer anticipate being able to assign all of the jobs to machines. Let’s compose our new model. // Compose the model let model = Model.create maxUtilizationObjective // First priority objective |\u003e Model.addObjective minSetupsObjective // Second priority objective |\u003e Model.addConstraints maxWorkConstraints |\u003e Model.addConstraints minWorkConstraints |\u003e Model.addConstraint maxWorkDifferenceConstraint |\u003e Model.addConstraints setupConstraints |\u003e Model.addConstraints maxJobTypeDConstraints |\u003e Model.addConstraints maxMachineCapacityConstraints Note that we create the initial model using the maxUtilizationObjective objective then add the minSetupsObjective to the model. This means that the solver will find a solution which maximizes the machine utilization first and then search for a solution that minimizes the number of different job-types. Let’s try to solve this and see what we get. This code comes from the Scheduler.schedule function. If the solver is successful, it returns a Result.Ok with the machine assignments. If it fails to find a solution, it returns a Result.Error with “Unable to solve” as the message. // Give the solver plenty of time to find a solution let settings = { Settings.basic with MaxDuration = 60_000L } let result = Solver.solve settings model match result with | Optimal solution -\u003e getMachineAssignments solution assignments |\u003e MachineAssignments |\u003e Result.Ok | _ -\u003e Result.Error \"Unable to solve\" If we use our pretty printer function, we get the following. let scheduleResult = Scheduler.schedule maxWorkDifference maxJobTypeDPercentage maxMachineCapacity jobs machines match scheduleResult with | Result.Ok assignments -\u003e Printer.MachineAssignments.print assignments | Result.Error msg -\u003e printfn $\"{msg}\" I use the Specture.Console library for printing these tables to the console. Machine Loading: ┌─────────┬────────────┬─────────────────────┬────────────────────┐ │ Machine │ Total Work │ Percent Type D Work │ Distinct Job Count │ ├─────────┼────────────┼─────────────────────┼────────────────────┤ │ 1 │ 24 │ 0.00% │ 1 │ │ 2 │ 24 │ 0.00% │ 1 │ │ 3 │ 24 │ 0.00% │ 1 │ │ 4 │ 24 │ 0.00% │ 1 │ │ 5 │ 24 │ 0.00% │ 1 │ └─────────┴────────────┴─────────────────────┴────────────────────┘ We see that the solver is filling up each machine with the maximum capacity available. Each machine is also only processing a single job-type. None of job-type D is being processed on any of these machines, interesting. Is that what we want? Maybe we want a policy which prioritizes some of the jobs above others? Maybe when work carries over from the previous day, it needs to be prioritized over new work coming in? These are some interesting questions that we will explore in the next post! I hope you are enjoying this series and it is giving you insight into how Mathematical Planning can be used to deal with many different scheduling challenges. More posts to come! Please send me an email at matthewcrews@gmail.com if you have any questions and subscribe so you can stay on top new posts and products I am offering. ","date":"2021-02-03","objectID":"/blog/2021/02/2021-02-04/:3:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 3","uri":"/blog/2021/02/2021-02-04/"},{"categories":null,"content":"In my previous post I introduced a scheduling problem where I needed to assign jobs to machines to achieve the maximum efficiency. We say efficiency is calculated as the number of times a machine must change the job-type it is working on. I want to continue exploring this problem by adding some nuance. Note: Full code for this post can be found here ","date":"2021-01-28","objectID":"/blog/2021/01/2021-01-28/:0:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 2","uri":"/blog/2021/01/2021-01-28/"},{"categories":null,"content":"Not Too Many Bad Jobs As my conversation continued with my friend regarding this problem a new constraint came up. It turns out there is a fourth job-type, let’s call it job-type D, that can cause significant wear on a machine if it is run for too long. He wanted to add a constraint to the problem which would limit that amount of job-type D assigned to any given machine. In his case, he wanted a machine to have no more than 50% of the total work assigned to it to be of job-type D. Fortunately this is a relatively simple update to our model. ","date":"2021-01-28","objectID":"/blog/2021/01/2021-01-28/:1:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 2","uri":"/blog/2021/01/2021-01-28/"},{"categories":null,"content":"Refactoring the Domain The great thing about F# is that it is easy to refactor our domain. In our case the Job type and the Machine type don’t need to change. What does need to be updated is the JobType type. We will add another case to the discriminated union to represent job-type D. I have also decided to do some refactoring and clean up how the code is organized. We are also going to move all the type definitions into their own Module. module Types = // The Domain [\u003cRequireQualifiedAccess\u003e] type JobType = | A | B | C | D // This is the new case we have added type Job = { Id : int JobType : JobType Size : float } with override this.ToString () = $\"Job_{this.Id}\" type Machine = { Id : int JobTypes : Set\u003cJobType\u003e } with override this.ToString () = $\"Machine_{this.Id}\" Next, we need to adjust our data generation. Again, we are going to do some code cleanup and move all the code for generating random jobs and machines into its own module. We are adjusting two thing from the previous post. The jobTypes Array had the new JobType case. We are also going to adjust the jobTypeSets. This is the possible job qualifications for a machine. In our new problem, job-type A is the most difficult and therefore fewer machines are qualified. All machines are capable of job-type D, even though it is not preferred. module DataGeneration = open System open Types // Set of JobTypes for iterating over and sampling from let jobTypes = [| JobType.A JobType.B JobType.C JobType.D // The new DU case we added |] // Some theoretical JobTypeSets to be used in generating // random Machines let jobTypeSets = [| Set jobTypes Set jobTypes[1..] Set jobTypes[2..] |] let minJobSize = 1 let maxJobSize = 3 let randomJobSize (rng: Random) = rng.Next(minJobSize, maxJobSize) |\u003e float let randomJobType (rng: Random) = jobTypes.[rng.Next(0, jobTypes.Length)] let randomJobTypeSet (rng: Random) = jobTypeSets.[rng.Next(0, jobTypeSets.Length)] let randomJob (rng: Random) (id: int) = { Id = id JobType = randomJobType rng Size = randomJobSize rng } let randomMachine (rng: Random) (id: int) = { Id = id JobTypes = randomJobTypeSet rng } ","date":"2021-01-28","objectID":"/blog/2021/01/2021-01-28/:2:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 2","uri":"/blog/2021/01/2021-01-28/"},{"categories":null,"content":"Updating Our Model I won’t go over all the model code that we created before. I am just going to show the new constraints that we need to add to the original formulation. One of the reasons I love Mathematical Planning is that it makes it relatively easy to tweak and update models over time. If the code is well organized, it’s trivial to turn features on and off. To add our limits on the amount of job-type D that a machine has, let’s define a value which is the maximum percent of D allowed. // Limit on the amount of JobType D on any given machine let maxJobTypeDPercentage = 0.30 Now we want to create a constraint for each of our machines which says the the percent of the total work assigned to the machine is no more than this percentage. Fortunately, this is relatively easy with Flips. let maxJobTypeDConstraints = ConstraintBuilder \"MaxTypeD\" { for machine in machines -\u003e let totalWork = sum (assignments[machine, All, All] .* jobSizes) let jobTypeDWork = sum (assignments[machine, JobType.D, All] .* jobSizes) jobTypeDWork \u003c== maxJobTypeDPercentage * totalWork } Okay, let’s unpack this. We are using the ConstraintBuilder Computation Expression to create a constraint for each machine in machines. We then calculate the total amount of work assigned to a machine by using the assignments SliceMap and selecting all the assignments for our machine and performing elementwise multiplication, .*, by the jobSizes. We then sum that up to get the total amount of work assigned to the machine. We store that expression in the totalWork value. To get the total amount of job-type D work assigned to the machine, we need to sub-select the assignments SliceMap for the machine and JobType.D then elementwise multiply by the jobSizes. We sum these values up to get the jobTypeDWork expression. totalWork is an expression which represents the total amount of work assigned to the machine. jobTypeDWork represent the total amount of job-type D assigned to the machine. We can now create our constraint expression. We state that jobTypeDWork must be less or equal to the totalWork expression multiplied by the max allowed percentage of job-type D, maxJobTypeDPercentage. This constraint will limit just how much work of job-type D that is allowed on the machine. That’s all we must do to accommodate this new restriction from my friend. ","date":"2021-01-28","objectID":"/blog/2021/01/2021-01-28/:3:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 2","uri":"/blog/2021/01/2021-01-28/"},{"categories":null,"content":"Unpacking the Results The only other change my friend asked for was to increase the number of jobs up to 100 because it would be more represented of the size of the real-world problem. With that adjustment, we can now compose our new model with these new constraints included. let model = Model.create minSetupsObjective |\u003e Model.addConstraints maxWorkConstraints |\u003e Model.addConstraints minWorkConstraints |\u003e Model.addConstraint maxWorkDifferenceConstraint |\u003e Model.addConstraints setupConstraints |\u003e Model.addConstraints jobsAssignmentConstraints |\u003e Model.addConstraints maxJobTypeDConstraints // Our new constraints We setup our solver settings and attempt to solve. // Give the solver plenty of time to find a solution let settings = { Settings.basic with MaxDuration = 60_000L } let result = Solver.solve settings model We now inspect the result. We add a couple of functions for getting the job assignments for each machine and summarizing the total loading of the machines. // This will return a list\u003cMachine * list\u003cJob\u003e\u003e let getMachineAssignments (solution: Solution) (assignments: SMap3\u003cMachine, JobType, Job, Decision\u003e) = Solution.getValues solution assignments |\u003e Map.filter (fun _ v -\u003e v = 1.0) |\u003e Map.toList |\u003e List.map (fun ((machine, _, job), _) -\u003e machine, job) |\u003e List.sortBy (fun (machine, job) -\u003e machine.Id, job.Id) |\u003e List.groupBy fst |\u003e List.map (fun (machine, jobs) -\u003e machine, jobs |\u003e List.map snd) // This create an anonymous record which holds the Machine, // the total loading and the job-type D loading let getMachineLoading jobAssignments = jobAssignments |\u003e List.map (fun (machine, jobs) -\u003e {| Machine = machine TotalWork = jobs |\u003e List.sumBy (fun j -\u003e j.Size) JobTypeDWork = jobs |\u003e List.filter (fun j -\u003e j.JobType = JobType.D) |\u003e List.sumBy (fun j -\u003e j.Size) |}) We then use these to analyze the result and print out what we found. match result with | Optimal solution -\u003e // Get which jobs are assigned to each machine let machineAssignments = getMachineAssignments solution assignments // Calculate the total work for each machine and the amount of job-type D let machineLoads = getMachineLoading machineAssignments printfn \"\" printfn \"Machine Loading:\" for (m) in machineLoads do // Print out the loading for each machine and the percent of job-type D work printfn $\"Machine: {m.Machine.Id} | Total Work: {m.TotalWork} | Type D Work %.2f{m.JobTypeDWork / m.TotalWork} %%\" // Find the min and max loads and calculate the difference let maxDifference = let loads = machineLoads |\u003e List.map (fun m -\u003e m.TotalWork) (List.max loads) - (List.min loads) printfn \"\" printfn $\"Max Difference in Loading: { maxDifference }\" | _ -\u003e printfn \"%A\" result This will show the following results. Machine Loading: Machine: 1 | Total Work: 28 | Type D Work 0.00 % Machine: 2 | Total Work: 28 | Type D Work 0.50 % Machine: 3 | Total Work: 29 | Type D Work 0.00 % Machine: 4 | Total Work: 30 | Type D Work 0.50 % Machine: 5 | Total Work: 29 | Type D Work 0.00 % Max Difference in Loading: 2 You can see that the machines are evenly loaded according to the maximum allowable difference and that no machine has more than 50% loading of job-type D. I would say that we have a success! ","date":"2021-01-28","objectID":"/blog/2021/01/2021-01-28/:4:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 2","uri":"/blog/2021/01/2021-01-28/"},{"categories":null,"content":"Takeaways We are only beginning to look at variations of this problem. Hopefully, what you have been able to observe was that it was simple to update our model code to add this new requirement my friend found. I think that is part of the beautify of Mathematical Planning. Updating and adjusting the logic can be simple. In our next post we are going to look at what happens when we add capacity for the machines and our problem becomes unsolvable! Please send me an email at matthewcrews@gmail.com if you have any questions and subscribe so you can stay on top new posts and products I am offering. ","date":"2021-01-28","objectID":"/blog/2021/01/2021-01-28/:5:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 2","uri":"/blog/2021/01/2021-01-28/"},{"categories":null,"content":" I was recently having a discussion with a friend when they brought up a new problem they were looking into. He asked me if it was a good candidate for Mathematical Planning and I said, “Absolutely!” I am abstracting away the specific domain, but this is the essence of the problem. There are a set of machines which can process jobs. The jobs are of different types and sizes. There are three job-types: A, B, and C. Each machine has different job-types that it can process. Some machines can process any job-type while other machines can only work on one or two. At the beginning of the day, we are given a set of jobs to assign to the machines. We want to assign jobs to machines such that a) the machines are evenly loaded and b) we minimize the number of different job-types each machine must process. An ideal plan would have each machine with the same amount of work and only processing a single job-type. The reason we want a machine to only process a single job-type is to minimize the waste associated with changing between job-types. Switching between job-types is fast, it just creates unwanted waste. Let’s start with creating a small domain model and generate some example data. Note: You can find the full code example for this post here ","date":"2021-01-25","objectID":"/blog/2021/01/2021-01-25/:0:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 1","uri":"/blog/2021/01/2021-01-25/"},{"categories":null,"content":"A Domain for Job Assignments Based on the description there are some clear types that we need to define: Machine, Job, and JobType. [\u003cRequireQualifiedAccess\u003e] type JobType = | A | B | C type Job = { Id : int JobType : JobType Size : float } with override this.ToString () = $\"Job_{this.Id}\" type Machine = { Id : int JobTypes : Set\u003cJobType\u003e } with override this.ToString () = $\"Machine_{this.Id}\" I like to override the ToString method because the ConstraintBuilder and DecisionBuilder in Flips use ToString for the naming of constraints and decisions. JobType is a straightforward Discriminated Union with three different cases A, B, and C. Job has an Id field for identifying a particular Job, a JobType which describes the type of job that it is, and the Size. The Machine type has an Id field and a JobTypes field. The JobTypes field is a Set of JobType. This represents the jobs the Machine can process. We now want to setup some data for us to be able to play with. These will be the parameters which will help us generate random data for us to work with. // Set of JobTypes for iterating over and sampling from let jobTypes = [| JobType.A JobType.B JobType.C |] // Some theoretical JobTypeSets to be used in generating // random Machines let jobTypeSets = [| Set jobTypes Set jobTypes.[1..] Set jobTypes.[..1] |] // Setting up parameters for the example let rng = System.Random(123) let numberOfJobs = 20 let numberOfMachines = 5 let minJobSize = 1 let maxJobSize = 3 let maxWorkDifference = 2.0 jobTypes is an Array which holds each of the possible JobType cases. We will use this to create random jobs. The jobTypeSets value is an Array of Set\u003cJobType\u003e. These are the possible values for the JobTypes field of Machine that we will use for generating random machines. For this example, we will have 20 jobs and 5 machines to assign them to. minJobSize will control how small a job can be and maxJobSize will determine how large. The maxWorkDifference parameter will determine how different the loading of machines that we will allow. We now create some convenience functions for generating random data. We will also add a function for looking up a key in a Map but returning a default value when the key is not present. let randomJobSize (rng: System.Random) = rng.Next(minJobSize, maxJobSize) |\u003e float let randomJobType (rng: System.Random) = jobTypes.[rng.Next(0, jobTypes.Length)] let randomJobTypeSet (rng: System.Random) = jobTypeSets.[rng.Next(0, jobTypeSets.Length)] module Map = // Useful when you want to look up a key in a Map but you want it to provide // a default value if the key is missing let tryFindDefault (key: 'a) (defaultValue: 'v) (m: Map\u003c'a, 'v\u003e) = match Map.tryFind key m with | Some v -\u003e v | None -\u003e defaultValue We can now generate a random set of jobs and machines to work with. // Create some examples jobs let jobs = [1..numberOfJobs] |\u003e List.map (fun id -\u003e { Id = id JobType = randomJobType rng Length = randomJobSize rng }) // Create some test machines let machines = [1..numberOfMachines] |\u003e List.map (fun id -\u003e { Id = id JobTypes = randomJobTypeSet rng }) ","date":"2021-01-25","objectID":"/blog/2021/01/2021-01-25/:1:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 1","uri":"/blog/2021/01/2021-01-25/"},{"categories":null,"content":"Formulating the Problem Now that we have some data to work with, we can get to formulating our model. Let’s go ahead and open Flips. I love working with VS Code, Ionide, and .fsx files for this kind of exploration. The new #r \"nuget: \u003clibrary name\u003e\" syntax for using Nuget packages has been a game changer. #r \"nuget: Flips\" open Flips open Flips.Types open Flips.SliceMap We want to create a Map where they key is a JobType and the value is a list of Job that are of that type. This will make it easy for us to lookup the Jobs of a given type. // A Map from JobType to the Jobs which are of that type let jobsForJobType = jobs |\u003e List.groupBy (fun job -\u003e job.JobType) |\u003e Map We now want to create a 1-dimensional SliceMap where the 'key is a Job and the 'value is the size of the job. This will make it easy for us to sum up how much work has been assigned to a Machine. // A SliceMap where the key is a Job and the value is the size of the Job let jobSizes = jobs |\u003e List.map (fun job -\u003e job, job.Size) |\u003e SMap Now let’s create the set of Decisions which will represent us assigning a Job to a Machine. We will store this in a 3-dimensional SliceMap keyed by the Machine, the JobType, and finally the Job. The reason we key by the JobType will become apparent later in the formulation. We will use a Boolean decision where 1.0 indicates that we are assigning the Job to a Machine and 0.0 indicates not. // The Decisions which represent assigning a Job to a Machine // The JobType index allows us to slice along the job type // which is useful in some of the constraints let assignments = DecisionBuilder \"Assignment\" { for machine in machines do for jobType in machine.JobTypes do for job in Map.tryFindDefault jobType [] jobsForJobType -\u003e Boolean } |\u003e SMap3 Now that we have these decisions which represent assigning a job to a machine, we can formulate our first and most obvious constraints. Each job must be assigned to one machine. For each job we say that the sum of assignments for a given job across all machines and all job-types must be 1.0. This forces the solver to find a solution where each job is assigned once. // Each job must be assigned let jobsAssignmentConstraints = ConstraintBuilder \"JobAssignment\" { for job in jobs -\u003e sum assignments.[All, All, job] == 1.0 } ","date":"2021-01-25","objectID":"/blog/2021/01/2021-01-25/:2:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 1","uri":"/blog/2021/01/2021-01-25/"},{"categories":null,"content":"Constraint or Objective? We now come to an interesting question. Our original problem statement said that we want to minimize the number of different job-types a machine must deal with. Ideally each machine only works on a single job-type. We also said that we want the machines evenly loaded. When I was chatting with my friend I dug into this point. Which one of these objectives is more important because we can’t optimize for both? This is where a modeler needs to work with their client to help them understand what the most important thing is truly. In our case, minimizing the different job-types for machines was the most important, so long as the machines were not too unevenly loaded. This means that the goal for even loading becomes a constraint and the objective remains the minimization of different job-types for machines. We will explore variations of this problem in future posts. ","date":"2021-01-25","objectID":"/blog/2021/01/2021-01-25/:3:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 1","uri":"/blog/2021/01/2021-01-25/"},{"categories":null,"content":"Controlling the Difference in Loading Now that we have decided that even machine loading needs to be a constraint, we need to create some Decisions to control for it. We will create two Decisions. One will represent the value of the machine with the greatest loading and the other the machine with the least loading. // A Decision which is meant to represent the MaxWork value across all Machines let maxWork = Decision.createContinuous \"MaxWork\" 0.0 infinity // A Decision which is meant to represent the MinWork value across all Machines let minWork = Decision.createContinuous \"MinWork\" 0.0 infinity Now, we want to create a Constraint which states that the difference between these two values is not greater than the maximum allowed difference. // We constrain the difference between the most heavily loaded machine // and the least loaded let maxWorkDifferenceConstraint = Constraint.create \"MaxWorkDifferent\" (maxWork - minWork \u003c== maxWorkDifference) Okay, that’s great but will it do anything? Right now, there is nothing that forces the maxWork decision to be equal to the loading of the most heavily loaded machine. There’s also nothing which forces minWork to be equal to the loading of the most lightly loaded machine. The solver could set the values to 0.0 and be done with it. We need to create some constraints which will force maxWork and minWork to take on the loading of the most heavily and most lightly loaded machines. Let’s create some constraints which state that the value of the maxWork decision must be greater than or equal to the loading of all the machines. This will force it to be a value above or equal to the maximum loading. // The maxWork Decision must be greater or equal to all of the total work // for each Machine let maxWorkConstraints = ConstraintBuilder \"MaxWork\" { for machine in machines -\u003e maxWork \u003e== sum (assignments[machine, All, All] .* jobSizes) } We do a similar thing for the minWork decision. In this case we will say that minWork must be less than or equal to all the loadings of the machines. // The minWork Decision must be less or equal to all of the total work // for each Machine let minWorkConstraints = ConstraintBuilder \"MinWork\" { for machine in machines -\u003e minWork \u003c== sum (assignments[machine, All, All] .* jobSizes) } maxWorkConstraints and minWorkConstraints will force maxWork and minWork to take on the values of the most heavily and lightly loaded machines respectively. maxWorkDifferenceConstraint states that the difference between maxWork and minWork must be within the permissable bounds. All together these constraints will prevent the solver from distributing jobs across machines unevenly. ","date":"2021-01-25","objectID":"/blog/2021/01/2021-01-25/:4:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 1","uri":"/blog/2021/01/2021-01-25/"},{"categories":null,"content":"Minimizing the Job-Types for Machines We now need to quantify how many different job-types are being assigned to machines. To do this, we will create a set of Boolean decisions which will indicate whether we have decided to assign a job of a given job-type to a machine. We will store these in a 2-dimensional SliceMap where the keys are the Machine and the JobType. 1 will represent that we have decided to assign a given JobType to a Machine. 0 will indicate that we did not. I like to think of this as “turning on” or “turning off” the job-type for the machine. We will call these decisions the setups decisions. // A Decision which indicates whether we setup a given Machine for a // JobType at any point let setups = DecisionBuilder \"Setups\" { for machine in machines do for jobType in jobTypes -\u003e Boolean } |\u003e SMap2 We now want to create some constraints which will force the solver to turn on the decision to allow the assigning of a job-type to a machine. We will do this by saying that the sum of jobs of a given job-type must be less than or equal to our decision to assign that job-type to the machine multiplied by a large number. This will force the solver to “turn on” the job-type for the machine. In our case the “large number” will be the total number of jobs. // We must turn the setups value for a given Machine and JobType to 1 // if we assign a Job of the given JobType to the Machine let setupConstraints = ConstraintBuilder \"SetupRequired\" { for machine in machines do for jobType in jobTypes -\u003e sum (assignments[machine, jobType, All]) \u003c== (float numberOfJobs) * setups[machine, jobType] } We can now create an expression which represents the number of different job-types that are assigned to machines. // An expression which is the sum of the Setups that will need to be performed let numberSetupsExpression = sum setups We use this expression to create our objective of minimizing the number of different job-types assigned to a machine. // We want to minimize the number of setups let minSetupsObjective = Objective.create \"MinSetups\" Minimize numberSetupsExpression We can now compose our model from the parts that we have created. // Compose the model let model = Model.create minSetupsObjective |\u003e Model.addConstraints jobsAssignmentConstraints |\u003e Model.addConstraints maxWorkConstraints |\u003e Model.addConstraints minWorkConstraints |\u003e Model.addConstraint maxWorkDifferenceConstraint |\u003e Model.addConstraints setupConstraints We now ask the solver to find us a solution. // Give the solver plenty of time to find a solution let settings = { Settings.basic with MaxDuration = 60_000L } let result = Solver.solve settings model If you want to see the code that prints out the results you can check it out here. This is the solution the solver found. Assignments: Machine: 1 Job: 4 Job: 7 Job: 8 Job: 11 Machine: 2 Job: 16 Job: 18 Job: 20 Machine: 3 Job: 1 Job: 3 Job: 6 Job: 12 Job: 17 Job: 19 Machine: 4 Job: 2 Job: 13 Job: 14 Machine: 5 Job: 5 Job: 9 Job: 10 Job: 15 Machine Loading: Machine: 1 | Total Load: 5 Machine: 2 | Total Load: 5 Machine: 3 | Total Load: 7 Machine: 4 | Total Load: 6 Machine: 5 | Total Load: 6 Max Diffence In Loading: 2 ","date":"2021-01-25","objectID":"/blog/2021/01/2021-01-25/:5:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 1","uri":"/blog/2021/01/2021-01-25/"},{"categories":null,"content":"Next Steps We’ve only begone to explore this model. There are quite a few variations and nuances that I will dive into in the posts to come. In the future we will discuss adding machine capacity and dealing with infeasible models. We will also explore adding restrictions on just how much of a job-type can be assigned to a given machine. Some job-types cause more wear and therefore we do not want too much assigned to a single machine. We will also look at needing to re-plan part way through the day and look at scheduling over a longer time horizon. These types of scheduling problems are common and therefore it’s valuable for us to explore how we can play and tweak with this model to make it suit our needs. Feel free to reach out with questions and ideas for modeling challenges in the future! Please send me an email at matthewcrews@gmail.com if you have any questions and subscribe so you can stay on top new posts and products I am offering. ","date":"2021-01-25","objectID":"/blog/2021/01/2021-01-25/:6:0","tags":null,"title":"Scheduling Jobs for Maximum Efficiency - Part 1","uri":"/blog/2021/01/2021-01-25/"},{"categories":null,"content":"I was recently posed the question, “Can you use Mathematical Planning to optimize the Cutting Stock problem?” For those who are not familiar with this problem, you can find the Wikipedia article here. In summary, you have a stock size of paper material from which you need to produce smaller sizes. In the example provided on Wikipedia, the stock size is 5600mm. You are asked to produce a variety of sizes between 1380mm and 2200mm. The ideal plan is one which minimizes the amount of waste. This is a classic planning problem that can actually be reduced to the knapsack problem. Note: Full code for this post can be found here These are the cut lengths and quantities you need to produce in the example problem. Width [mm] Number of Items 1380 22 1520 25 1560 12 1710 14 1820 18 1880 18 1930 20 2000 10 2050 12 2100 14 2140 16 2150 18 2200 20 There are a variety of different ways you can cut the stock size into the smaller sizes. For example, you could produce 3 x 1820mm cuts from a 5600mm stock roll. You could also do 2200mm, 1820mm, and 1560mm. In total there are 308 possible combinations of cuts, not including the empty combination which has zero cuts. The most important thing to realize when approaching this problem is that the order you make the cuts does not matter. A more technical term would be that the order of cuts is commutative. ","date":"2021-01-11","objectID":"/blog/2021/01/2021-01-11/:0:0","tags":null,"title":"Minimizing Waste for the Cutting Stock Problem","uri":"/blog/2021/01/2021-01-11/"},{"categories":null,"content":"Generating the Possible Cuts The most difficult part of this problem turned out to be the generating of the possible cuts. Before we dive right into that though, let’s create some simple domain types to describe our problem. type Cut = Cut of float type Plan = Plan of Map\u003cCut, int\u003e A Cut is a length we want to create from our stock rolls. A Plan is a set of cuts. We want an algorithm which will generate the possible Plans for a set of Cuts for our stock roll. To make our lives easier, I am going to go ahead and write some functions which allow us to work with these types more easily. module Cut = /// Take a Cut and return the length as a float let length (Cut length) = length module Plan = /// Give me a Plan with no cuts let empty : Plan = Plan Map.empty /// Give me the total length of cuts in the plan let length (Plan plan) = plan |\u003e Seq.sumBy (fun (KeyValue(Cut cut, count)) -\u003e cut * float count) /// Add a Cut to a Plan and return a new Plan let addCut (cut: Cut) (Plan plan) = match Map.tryFind cut plan with | Some count -\u003e Plan (Map.add cut (count + 1) plan) | None -\u003e Plan (Map.add cut 1 plan) /// Give me the count of each distinct cut in a given Plan let cutCounts (Plan plan) = plan |\u003e Seq.map (fun (KeyValue(cut, count)) -\u003e cut, count) We now have our domain for working in this space. Let’s talk about the function which will generate the possible Plans given a set of Cuts and a Stock Length. We want something like this: let generatePlans (stockLength: float) (cuts: Cut list) : Plan list = // Do some magic here?? Now, I’m going to show you the answer that I came up with. What you are not seeing though is the couple of hours I spent with my notebook sketching out how this would work. It was not intuitive to me, so I don’t want you to think that this stuff just materializes out of thin air. I had to struggle. It was not intuitive but by the time I was done, I felt immense satisfaction. The first thing I am going to do is sort cuts from the shortest length to the longest and ensure that I only have distinct cuts. let sortedCuts = cuts |\u003e List.distinct |\u003e List.sortBy (fun (Cut length) -\u003e length) This algorithm is going to take advantage of the fact that the cuts are sorted from shortest to longest so that it can terminate early. Now I want to write a recursive function which is going to take an initially empty Plan and try adding cuts to it. It will keep adding cuts until it exceeds the Stock Length. You can think of this as a sort of Constructive Heuristic. I’m going to show you the full function but then we will unpack it. let rec generate (candidates: (Plan * Cut list) list) (approved: Plan list) = match candidates with | [] -\u003e approved | testCandidate::remainingCandidates -\u003e let plan, cuts = testCandidate match cuts with | [] -\u003e let newApproved = plan::approved generate remainingCandidates newApproved | nextCut::remainingCuts -\u003e if Plan.length plan + Cut.length nextCut \u003c= stockLength then let newPlan = Plan.addCut nextCut plan let newCandidates = (newPlan, cuts)::(plan, remainingCuts)::remainingCandidates generate newCandidates approved else let newApproved = plan::approved generate remainingCandidates newApproved We have a list of plans and possible cuts which we are exploring called candidates. As candidates are approved, they are added to the approved list of plans. Keep in mind, this function will be initially called with an empty Plan and the full list of Cuts available. Let’s go through the cases step by step. match candidates with | [] -\u003e approved This is the terminal case. We have evaluated all the Plans that were generated, and we return the Plans in the approved list. Now for the case where we still have remaining candidates. | testCandidate::remainingCandidates -\u003e let plan, cuts = testCandidate In this case, there is at least one remaining candidate to evaluate, testCandidate. We create plan and cuts values using structural unpacking of testCandidate. plan is the Plan we are testing. cuts is the list of possible c","date":"2021-01-11","objectID":"/blog/2021/01/2021-01-11/:1:0","tags":null,"title":"Minimizing Waste for the Cutting Stock Problem","uri":"/blog/2021/01/2021-01-11/"},{"categories":null,"content":"The Optimization Problem The optimization model for this is rather simple. We will create the list of possible Plans using the function we just described. We will associate an integer Decision with each Plan which is to indicate how many of each of those plans we will schedule. Let’s setup the data for our model so that we can build it. All this data is taken from the Wikipedia example. let cuts = [ 1380.0 1520.0 1560.0 1710.0 1820.0 1880.0 1930.0 2000.0 2050.0 2100.0 2140.0 2150.0 2200.0 ] |\u003e List.map Cut let cutRequirements = [ Cut 1380.0 , 22.0 Cut 1520.0 , 25.0 Cut 1560.0 , 12.0 Cut 1710.0 , 14.0 Cut 1820.0 , 18.0 Cut 1880.0 , 18.0 Cut 1930.0 , 20.0 Cut 2000.0 , 10.0 Cut 2050.0 , 12.0 Cut 2100.0 , 14.0 Cut 2140.0 , 16.0 Cut 2150.0 , 18.0 Cut 2200.0 , 20.0 ] |\u003e Map let stockLength = 5600.0 let plans = generatePlans stockLength cuts We now want to start building our model. We’ll open the namespaces we need and create our set of Decisions associated with each Plan in plans. We are using SliceMaps to simplify formulation. open Flips open Flips.Types open Flips.SliceMap let planDecs = DecisionBuilder \"PlanCount\" { for plan in plans -\u003e Integer (0.0, infinity) } |\u003e SMap We then need to calculate the number of each Cut that is associated with each Plan. This will be important for us to formulate the constraints around meeting the minimum cut requirements. We will store this information in a 2-D SliceMap where the first index is the Plan and the second index is the Cut. The value in the SliceMap is the number of a given Cut in the Plan. let planCutCounts = plans |\u003e Seq.collect (fun plan -\u003e Plan.cutCounts plan |\u003e Seq.map (fun (cut, count) -\u003e (plan, cut), float count) ) |\u003e SMap2 It’s now actually simple to create our constraints. We will create a constraint for each Cut in our data stating that the solution must meet the minimum quantity of each Cut. let cutRequirementConstraints = ConstraintBuilder \"CutRequirements\" { for cut in cuts -\u003e sum (planDecs .* planCutCounts.[All, cut]) \u003e== cutRequirements.[cut] } Our objective is to minimize the total number of stock rolls required to meet the demand for each Cut. let objective = Objective.create \"MinRolls\" Minimize (sum planDecs) We combine these into our model and solve. let model = Model.create objective |\u003e Model.addConstraints cutRequirementConstraints let result = Solver.solve Settings.basic model Let’s go ahead and provide some nice printing of the results to the console. match result with | Optimal solution -\u003e let values = Solution.getValues solution planDecs |\u003e Map.filter (fun _ quantity -\u003e quantity \u003e 0.0) let totalNumberOfRolls = values |\u003e Seq.sumBy (fun (KeyValue(_, count)) -\u003e count) printfn \"Quantity | Plan\" for KeyValue(plan, quantity) in values do printfn $\"%8.0f{quantity} | {plan}\" printfn \"==========================================\" printfn $\"Total Number of Rolls: {totalNumberOfRolls}\" printfn \"==========================================\" | _ -\u003e failwith \"Unable to solve\" When you run the full script, you will see the following printed out. Quantity | Plan 8 | Plan (map [(Cut 1380.0, 1); (Cut 2000.0, 1); (Cut 2200.0, 1)]) 7 | Plan (map [(Cut 1380.0, 1); (Cut 2050.0, 1); (Cut 2150.0, 1)]) 7 | Plan (map [(Cut 1380.0, 1); (Cut 2100.0, 2)]) 10 | Plan (map [(Cut 1520.0, 1); (Cut 1880.0, 1); (Cut 2200.0, 1)]) 10 | Plan (map [(Cut 1520.0, 1); (Cut 1930.0, 1); (Cut 2140.0, 1)]) 3 | Plan (map [(Cut 1520.0, 1); (Cut 1930.0, 1); (Cut 2150.0, 1)]) 2 | Plan (map [(Cut 1520.0, 1); (Cut 2000.0, 1); (Cut 2050.0, 1)]) 2 | Plan (map [(Cut 1560.0, 1); (Cut 1820.0, 1); (Cut 2200.0, 1)]) 8 | Plan (map [(Cut 1560.0, 1); (Cut 1880.0, 1); (Cut 2150.0, 1)]) 1 | Plan (map [(Cut 1560.0, 2); (Cut 2050.0, 1)]) 2 | Plan (map [(Cut 1710.0, 1); (Cut 1820.0, 1); (Cut 2050.0, 1)]) 6 | Plan (map [(Cut 1710.0, 2); (Cut 2140.0, 1)]) 7 | Plan (map [(Cut 1820.0, 2); (Cut 1930.0, 1)]) ========================================== Total Number of Cuts: 73 ========================================== If you check the Wik","date":"2021-01-11","objectID":"/blog/2021/01/2021-01-11/:2:0","tags":null,"title":"Minimizing Waste for the Cutting Stock Problem","uri":"/blog/2021/01/2021-01-11/"},{"categories":null,"content":"Next Steps This was a fun challenge and was a bit of a brain teaser. These types of problems are everywhere in manufacturing planning and scheduling. Minimizing the amount of raw resources required is incredibly important but can be brutally difficult. It’s often done by domain experts spending hours with Excel finding a plan that meets all the requirements. These are some of my favorite problems to turn into Mathematical Planning models. Thank you for your time and I look forward to chatting next week! Please send me an email at matthewcrews@gmail.com if you have any questions and subscribe so you can stay on top new posts and products I am offering. ","date":"2021-01-11","objectID":"/blog/2021/01/2021-01-11/:3:0","tags":null,"title":"Minimizing Waste for the Cutting Stock Problem","uri":"/blog/2021/01/2021-01-11/"},{"categories":null,"content":" One of the questions that I get quite a bit is, “How did you learn F#?” I keep getting this question, so I decided to write a short post where I laid out what I have found to be the most useful resources to develop my F# skills. ","date":"2021-01-09","objectID":"/blog/2021/01/2021-01-09/:0:0","tags":null,"title":"Learning Resources for F#","uri":"/blog/2021/01/2021-01-09/"},{"categories":null,"content":"Practice This first piece of advice that I give to people is to choose an algorithm they already know and write it in F#. Don’t try to learn a new concept and a new language at the same time. I am familiar with the Simulated Annealing algorithm, so I use it as my first algorithm that I try to write in any new language. The process of trying to write something you already know but in a new way is great for exposing your underlying presuppositions. It forces you out of your comfort zone and challenges you to rethink problems. ","date":"2021-01-09","objectID":"/blog/2021/01/2021-01-09/:1:0","tags":null,"title":"Learning Resources for F#","uri":"/blog/2021/01/2021-01-09/"},{"categories":null,"content":"Books I, for one, love reading books. I enjoy having a physical copy of something that I can mark up with notes or put sticky notes into for quick reference. There is something about a person taking the time to organize their thoughts that creates a cohesive narrative. Each of these books are highly recommended. They are the foundation of all my F# understanding. ","date":"2021-01-09","objectID":"/blog/2021/01/2021-01-09/:2:0","tags":null,"title":"Learning Resources for F#","uri":"/blog/2021/01/2021-01-09/"},{"categories":null,"content":"Get Programming With F# Link: Get Programing with F# This is the first book I recommend to an experienced developer coming to F#. It does an excellent job of providing a comparison and contrast between C# and F#. I have found this is incredibly helpful in providing a bridge for developers coming to F#. I did not spend much time in other languages before learning F# so I had not developed many paradigms around Object-Oriented or procedural programming. I’ve seen people who have been steeped in OO struggle transitioning to functional style. This book provides some useful bridges between paradigms. ","date":"2021-01-09","objectID":"/blog/2021/01/2021-01-09/:2:1","tags":null,"title":"Learning Resources for F#","uri":"/blog/2021/01/2021-01-09/"},{"categories":null,"content":"Stylish F# Link: Stylish F# This is one of the most formative books for me. I knew how to write F# before reading this book but what this book did was help me understand why I was writing code in a particular way. Before this book, I kept writing code that felt clunky. It did the job, but it was not as clean and elegant as I would like. Kit provides some valuable insight into how to form F# code that is not only effective but delightful. I found that I enjoyed writing F# more after reading this book. This is the first book I recommend to new developers. If they ingest and absorb these principles, they will avoid many heartaches. This book really needs to be promoted more. ","date":"2021-01-09","objectID":"/blog/2021/01/2021-01-09/:2:2","tags":null,"title":"Learning Resources for F#","uri":"/blog/2021/01/2021-01-09/"},{"categories":null,"content":"Domain Modeling Made Functional Link: Domain Modeling Made Functional A must read for anyone really, but especially F# developers. There is a ridiculous amount of wisdom packed into these pages. Scott has spent years writing code and the lessons he shares in this book are incredible. I constantly try to instill new developers with the idea that abstractions are not just to hide details of implementation. A good abstraction provides a barrier around context. We often just think of an abstraction as obscuring the underlying implementation. Abstractions should also remove the need to be concerned about services adjacent to ours. I believe this is the idea of a bounded context and it is so critical for writing code that can be scaled, maintained, and reasoned about. Healthy systems having strong reasoning boundaries. Read this book, no matter what language you are writing. ","date":"2021-01-09","objectID":"/blog/2021/01/2021-01-09/:2:3","tags":null,"title":"Learning Resources for F#","uri":"/blog/2021/01/2021-01-09/"},{"categories":null,"content":"Machine Learning Projects for .NET Developers Link: Machine Learning Projects for .NET Developers This one may be domain specific, but I recommend it to anyone who is wanting to understand how to translate algorithms to F#. What Mathias does in this book is implement ML algorithms from the ground up. Many other ML books simply tell you how to use an existing library. Mathias takes a different tactic and takes you behind the curtain and shows you how they are built. I think it really shows the elegance of F# for expressing algorithms. Highly recommended for someone interested in F#, ML, and algorithm implementation. ","date":"2021-01-09","objectID":"/blog/2021/01/2021-01-09/:2:4","tags":null,"title":"Learning Resources for F#","uri":"/blog/2021/01/2021-01-09/"},{"categories":null,"content":"Blogs ","date":"2021-01-09","objectID":"/blog/2021/01/2021-01-09/:3:0","tags":null,"title":"Learning Resources for F#","uri":"/blog/2021/01/2021-01-09/"},{"categories":null,"content":"F# for Fun and for Profit Link At some point, every F# developer finds their way to these hallowed halls. I haven’t found an F# developer yet who hasn’t mentioned Scott Wlaschin’s blog when they describe their journey to F#. There is simply a cornucopia of knowledge in these pages. The only warning is that some of the content is old and may be out of style. I can’t recommend this blog enough. Scott has poured his years of experience into these pages and it is a blessing to us developers following in his footsteps. ","date":"2021-01-09","objectID":"/blog/2021/01/2021-01-09/:3:1","tags":null,"title":"Learning Resources for F#","uri":"/blog/2021/01/2021-01-09/"},{"categories":null,"content":"Mark Seemann’s Blog Link Mark has been speaking and teaching on functional programming for years. His blog is sure to inspire. I have had my mind regularly expanded reading through his posts. He has a way of reframing how you see the world which can fill you with awe. One of my best memories was sitting with him for an hour talking about programming as art. He is a deep thinker and how you think about code will be expanded if you spend any amount of time on his blog. Please send me an email at matthewcrews@gmail.com if you have any questions and subscribe so you can stay on top new posts and products I am offering. ","date":"2021-01-09","objectID":"/blog/2021/01/2021-01-09/:3:2","tags":null,"title":"Learning Resources for F#","uri":"/blog/2021/01/2021-01-09/"},{"categories":null,"content":"In the previous two posts in this series we introduced the Food Cart Problem. We want a plan for stocking our Food Cart which will maximizes our revenue. In the first post we discussed the foods that we can stock and the restrictions we are operating under. We introduced a simple heuristic for stocking the food cart and created a simulation in order to validate what our expected revenue is. In the second post we formulated a Mathematical Planning model to create a plan to maximize our expected revenue. We validated that the plan found with Mathematical Planning is superior to the simple heuristic in the first post through simulations and statistical tests. In this final post of the series, we will create a simple Machine Learning model to make predictions of expected demand using the weather conditions as predictors. We will combine the tools of Mathematical Planning and Machine Learning to create an even more profitable algorithm for stocking the food cart. Note: To see all the code for this post, go here ","date":"2021-01-01","objectID":"/blog/2021/01/2020-01-01/:0:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 3","uri":"/blog/2021/01/2020-01-01/"},{"categories":null,"content":"Generating Test Data Since this problem is a hypothetical, we need to generate data to train a Machine Learning model on. The demand for food will be modeled using a Poisson Distribution but the mean of the demand, the $\\lambda$, depend on the weather. We are going to generate data that follows a simple model. $$ \\begin{align} \\text{Demand Mean} = \\ \u0026\\text{Baseline Demand} \\ + \\\\ \u0026\\text{Temperature Coefficient} \\times \\text{temperature} \\ + \\\\ \u0026\\text{Condition Offset} \\end{align} $$ Each food will have a baseline amount of demand, Baseline Demand. The demand for the food will go up or down depending on the temperature. The weather condition (Sunny, Cloudy, Rainy) will also bump the demand up or down in a stepwise manner. We will generate several months’ worth of weather conditions and then use them to generate our historical demand data. We want to make our code clear and maintainable so we create a domain model. Let’s create types to describe a day, temperature, conditions, and weather. type Day = Day of int type Temperature = Temperature of float type Condition = | Sunny | Cloudy | Rainy type Weather = { Condition : Condition Temperature : Temperature } Now we can create the type which holds the parameters for our demand model. type DemandModelParameters = { BaselineDemand : float TemperatureCoefficient : float ConditionOffsets : Map\u003cCondition, float\u003e } Our model will take the Weather as an input and simulate a random Demand based on the statistical model we just proposed. Let’s create a type which describes this simulation result. [\u003cMeasure\u003e] type serving type Demand = Demand of float\u003cserving\u003e type DemandSimulationResult = { Day : Day Weather : Weather Demand : Demand } Now we can write a function which takes the DemandModelParameters, Weather and gives us a random Demand that follows our model. In statistics this is often called “sampling from the distribution” so we will call our function sample. module Demand = let sample (rng: Random) (parameters: DemandModelParameters) (weather: Weather) = let (Temperature temperature) = weather.Temperature let lambda = parameters.BaselineDemand + temperature * parameters.TemperatureCoefficient + parameters.ConditionOffsets.[weather.Condition] let result = Poisson.Sample (rng, lambda) Demand (float result * 1.0\u003cserving\u003e) The Poisson.Sample function returns an int which isn’t what we will want for training our Machine Learning model, so we go ahead and turn it into a float and attach Units of Measure. Units of Measure are incredibly valuable in tracking what numbers mean but they can add a little bit of boilerplate in some cases. I find it worth the clarity they bring. We want to add a couple more functions for generating random Temperature, Condition and Weather. module Condition = let private conditions = [ 0, Sunny 1, Cloudy 2, Rainy ] |\u003e Map let sample (rng: Random) = conditions.[rng.Next(0, 2)] module Temperature = let sample (rng: Random) (Temperature minTemperature) (Temperature maxTemperature) = rng.NextDouble() * (maxTemperature - minTemperature) + minTemperature |\u003e Math.Round |\u003e Temperature module Weather = let sample (rng: Random) (minTemperature: Temperature) (maxTemperature: Temperature) : Weather = let condition = Condition.sample rng let temperature = Temperature.sample rng minTemperature maxTemperature { Condition = condition Temperature = temperature } Well, this is great. We now have a way of generating data for which we can train a ML model on. Let’s do this for Burgers. // Parameters for generating samples data let burgerParameters = { BaselineDemand = 337.5 TemperatureCoefficient = 3.5 ConditionOffsets = Map [ Sunny, -30.0; Cloudy, 0.0; Rainy, 30.0 ] } let rng = System.Random(123) // Number of days for which to generate data let numberOfDays = 100 let minTemp = Temperature 40.0 let maxTemp = Temperature 110.0 let pastDays = [1..numberOfDays] |\u003e List.map Day let pastWeather = pastDays |\u003e List.map (fun day -\u003e {| Day = day; Weather = Simulation.Weather.sample rng minTe","date":"2021-01-01","objectID":"/blog/2021/01/2020-01-01/:1:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 3","uri":"/blog/2021/01/2020-01-01/"},{"categories":null,"content":"Training a Machine Learning Model We will use ML.NET to create a simple ML Model to predict demand. The ML.NET API can use many different data sources as inputs. For our case, we will save our historical weather and demand data to a local .csv file. I am not giong to show the boilerplate for saving the data but you can find it in the full code example. The ML.NET API has you define types for your training data that it uses for parsing the input. In our case we will have delimited records in a .csv file. [\u003cCLIMutable\u003e] type DemandData = { [\u003cLoadColumn(1)\u003e] Temperature : single [\u003cLoadColumn(2)\u003e] Condition : string [\u003cLoadColumn(3); ColumnName(\"Label\")\u003e] Demand : single } The [\u003cLoadColumn(n)\u003e] attribute tells ML.NET which column in the source data corresponds to that field. Our saved data looks like this: Day Temperature Condition Demand 1 104 Cloudy 706 2 97 Cloudy 645 3 43 Cloudy 492 4 50 Sunny 457 We don’t need the data in the Day column so we are ignoring it. The [\u003cColumnName(\"Label\")\u003e] is what tells ML.NET the field that I am trying to predict. It will use the other fields as inputs to try to predict Demand. We want to create a function which takes an input file, trains a model, and then saves the model locally. ML.NET allows you to save trained models as .zip files. This makes it easy to load a pre-existing model to use in your code. We will be working with several different types of files and I find it useful to create some simple types to represent that. type DataFile = DataFile of string type ModelFile = ModelFile of string type OutputDirectory = OutputDirectory of string A DataFile is a filepath to the .csv which holds our data. A ModelFile is the .zip which holds the persisted form of our ML model for predicting demand. The OutputDirectory is just a directory where we want to put data and models. We can now put together a simple train function which will take our input data, train an ML model, and then save it for us to use later. We will also report the metrics from the training. open Microsoft.ML open Microsoft.ML.Data open Types let private reportMetrics (metrics: RegressionMetrics) = // show the metrics printfn \"Model metrics:\" printfn \" RMSE:%f\" metrics.RootMeanSquaredError printfn \" MSE: %f\" metrics.MeanSquaredError printfn \" MAE: %f\" metrics.MeanAbsoluteError let train (OutputDirectory outputDir) (modelName: string) (inputFile) = let context = MLContext() let dataView = context.Data.LoadFromTextFile\u003cDemandData\u003e (inputFile, hasHeader = true, separatorChar = ',') let partitions = context.Data.TrainTestSplit(dataView, testFraction = 0.2) let pipeline = EstimatorChain() .Append(context.Transforms.Categorical.OneHotEncoding(\"Condition\")) .Append(context.Transforms.NormalizeMeanVariance(\"Temperature\")) .Append(context.Transforms.Concatenate(\"Features\", \"Condition\", \"Temperature\")) .Append(context.Regression.Trainers.Sdca()) let model = partitions.TrainSet |\u003e pipeline.Fit let metrics = partitions.TestSet |\u003e model.Transform |\u003e context.Regression.Evaluate reportMetrics metrics |\u003e ignore System.IO.Directory.CreateDirectory outputDir |\u003e ignore let outputFile = $\"{outputDir}/%O{modelName}.zip\" context.Model.Save (model, dataView.Schema, outputFile) ModelFile outputFile The part where we define the shape of our model is in the definition of pipeline. let pipeline = EstimatorChain() .Append(context.Transforms.Categorical.OneHotEncoding(\"Condition\")) .Append(context.Transforms.NormalizeMeanVariance(\"Temperature\")) .Append(context.Transforms.Concatenate(\"Features\", \"Condition\", \"Temperature\")) .Append(context.Regression.Trainers.Sdca()) We know that the Condition data is categorical so we need to encode it in a way that the training code understands. One-hot Encoding represents categories as a set of Boolean values. It’s a similar concept to binary encoding of numbers but they are not the same, so don’t mix them up! We then take the step to normalize the mean and variance of the Temperature data. ML training really likes to have","date":"2021-01-01","objectID":"/blog/2021/01/2020-01-01/:2:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 3","uri":"/blog/2021/01/2020-01-01/"},{"categories":null,"content":"MP + ML = $$$ To evaluate how our new predictor performs, we will combine it with our Mathematical Planning model. Let’s simulate 30 days into the future and compare the three approaches: Simple Heuristic, Mathematical Planning, Mathematical Planning + Machine Learning. We generate 30 days into the future. For each day we sample from the possible weather conditions. We then take the Weather for each day and generate a sample of the demand for each food from our underlying statistical model. We store the result in a Map\u003cFood, Demand\u003e to simplify some of our later code. To make our code cleaner we are going to add a new function to our Simulation module for the demand for all the foods in each day. module DayDemand = let sample (rng: Random) (parameters: Map\u003cFood, DemandModelParameters\u003e) (weather: Weather) = parameters |\u003e Map.map (fun food demandParameters -\u003e Demand.sample rng demandParameters weather) It’s a small thing, but I have rarely regretted making my code clean and simple. Now we will generate 30 days’ worth of weather data. let numberFutureDays = 30 // Let's create some future data that we will evaluate our different techniques against let futureDays = [1..numberFutureDays] |\u003e List.map (fun d -\u003e {| Day = Day d; Weather = Simulation.Weather.sample rng minTemp maxTemp |}) With 30 days’ worth of future weather data, we will now generate theoretical demand on those days. We are “cheating” in that we have control of the generation of demand data. In real world scenarios there are some additional complexities that need to be dealt with. My desire is to show the concepts. Future posts can deal with the idiosyncrasies. We now generate the demand that would be observed on those days. // Let's create some future data that we will evaluate our different techniques against let futureDays = futureWeather |\u003e List.map (fun d -\u003e {| d with FoodDemand = Simulation.DayDemand.sample rng foodParameters d.Weather |}) We now have a list of days where we have the weather on that day and the demand. We can now calculate what our revenue would have been using the simple heuristic from the first blog post. We create a function which takes the planned inventory and the demand on a day to calculate the revenue. module RevenueModel = open Types let evaluate (revenuePerServing: Map\u003cFood, RevenuePerServing\u003e) (inventoryPlan: Map\u003cFood, Inventory\u003e) (demand: Map\u003cFood, Demand\u003e) = inventoryPlan |\u003e Map.toSeq |\u003e Seq.sumBy (fun (food, inventory) -\u003e revenuePerServing.[food] * (Sales.calculate inventory demand.[food])) The RevenueModel.evaluate function calculates what the revenue would have been on a day given the inventory plan that was used and the demand that was realized. We now use our simple heuristic plan for each of the future 30 days to see what our total revenue would have been. let simpleHueristicPlan = Map [ Burger, Inventory 0.0\u003cserving\u003e Pizza, Inventory 900.0\u003cserving\u003e Taco, Inventory 466.0\u003cserving\u003e ] let simpleHeuristicRevenue = futureDays |\u003e List.sumBy (fun d -\u003e RevenueModel.evaluate revenuePerServing simpleHueristicPlan d.FoodDemand) simpleHeuristicRevenue comes to $62,247.20. This will be our baseline. We now use the inventory plan we found using Mathematical Planning and find the revenue we would achieve. let optimizedPlan = Map [ Burger, Inventory 572.0\u003cserving\u003e Pizza, Inventory 355.0\u003cserving\u003e Taco, Inventory 669.0\u003cserving\u003e ] let optimizedPlanRevenue = futureDays |\u003e List.sumBy (fun d -\u003e RevenueModel.evaluate revenuePerServing optimizedPlan d.FoodDemand) optimizedPlanRevenue comes to $65,462.50, a 2.58% improvement. That will add up to significant additional revenue as the business scales. We now want to bring Mathematical Planning and Machine Learning together. We will use our Machine Learning models to predict the expected demand given the weather as input. The result of the prediction will be used as the input to the Mathematical Planning model which will suggest the ideal amount of inventory to carry. I am going to gloss over some details","date":"2021-01-01","objectID":"/blog/2021/01/2020-01-01/:3:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 3","uri":"/blog/2021/01/2020-01-01/"},{"categories":null,"content":"Next Steps I hope this illustrates a simple use case for combining the power of Mathematical Planning and Machine Learning. The techniques are highly complementary and lead to highly effective solutions which are far better than either technique in isolation. We can improve this solution in many ways. We spent no time on tuning the Machine Learning model. It would be nice to have real world data for this problem. The challenges is that the real world data sets I have access to are proprietary which is why I generated my own. In future posts I hope to explore how to tune Machine Learning models to protect against specific failure modes by tuning for different metrics. We are also assuming that we restock the food cart every morning. We likely need to make inventory decisions several days ahead of time which introduces some interesting optimization challenges we will explore in the future. Let me know if there are any problems you would like me to explore! Please send me an email at matthewcrews@gmail.com if you have any questions and subscribe so you can stay on top new posts and products I am offering. ","date":"2021-01-01","objectID":"/blog/2021/01/2020-01-01/:4:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 3","uri":"/blog/2021/01/2020-01-01/"},{"categories":null,"content":"In a previous post we discussed the problem of stocking our food cart to maximize our profitability. We created a simple heuristic and then performed simulations to evaluate the expected profitability. We discussed that knowing the expected profitability was not enough, we calculated Confidence Intervals to understand the where the true expected profitability lies. This week we want to find a better plan for packing the food cart. We will use Mathematical Planning to find an answer which maximize the expected profitability which outperforms the heuristic used in the first post. Note: To see all the code for this post, go here ","date":"2020-12-21","objectID":"/blog/2020/12/2020-12-21/:0:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 2","uri":"/blog/2020/12/2020-12-21/"},{"categories":null,"content":"The Probability of Selling Math Warning: I like to explain the math behind how we calculate the probablility of us selling items. If you would like to skip the math, feel free to jump to the next section There’s a nuance to this problem that the heuristic from the previous post did not take into consideration. We know that the number of sales of a food can be modeled by a Poisson Distribution. The Poisson Distribution takes a single parameter, $\\lambda$, which is the average arrival rate of a process. In our case, the “process” is people placing orders for a given food. Our historical data shows that the average daily demand for food is 600 burgers, 900 pizzas, and 700 tacos. Burger Pizza Taco Average Demand $600.0 \\frac{\\text{item}}{\\text{day}}$ $900.0 \\frac{\\text{item}}{\\text{day}}$ $700.0 \\frac{\\text{item}}{\\text{day}}$ Why do we care about this? Why do we care that the demand is actually a Poisson Distribution? Let’s perform a thought experiment. What are the odds that we sell at least 1 pizza in a day? Essentially 100%, right? What about 2 pizzas? Still 100% but maybe infinitesimally smaller. What are the odds of us selling 900 pizzas? Well, ~50% because historically we have seen that is the average demand. On rare days we will only sell 800 pizzas and on some we may sell over 1,000 if we do not run out of inventory. We can use the Poisson Distribution to calculate the odds of us selling a particular number of pizzas on any given day. Before we go any farther, lets plot the distribution of pizza demand between 800 and 1,000. You notice that the peak is at 900 which is what we expect since that is the average. Visually we can tell that the probability of demand being between 850 and 950 is high but drops off rapidly at the tails. We need to figure out how to use this information to help us find the ideal number of each food we should pack. What would be really useful is being able to calculate what the probability is of us completely selling our inventory. If we pack 950 pizzas, how likely are we to sell through all of them? We would rather not waste space on foods that are not going to sell. We need a function which allows us to calculate the probability of demand exceeding some value of $x$. In math terms we would write: $$ P(demand \u003e x) $$ One the ingredients for this calculation is called the Cumulative Distribution Function (CDF). The CDF for the Poisson Distribution is: $$ \\text{CDF}(x) = P(X \\le x) = \\frac{\\Gamma(x + 1,\\ \\lambda)}{\\lfloor x \\rfloor !} \\qquad \\qquad x = 0,1,2 , \\ldots $$ That’s some scary looking math so let’s unpack what this means. The function $F(x)$ calculates the probability of a random observation from our Poisson Distribution being less than or equal to the value of $x$. In our case, $x$ is the number of servings we pack of a food. The CDF isn’t quite what we need. What we want is to know the probability of demand exceeding $x$, not being less or equal to $x$. It is pretty simple to adjust the formula though. The probability of demand being greater than $x$ is equal to 1.0 minus the probability of demand being less than $x$ $$ P(demand \u003e x) = 1 - P(demand \\leq x) $$ We can swap in our CDF for the Poisson distribution to get a new formula which allows us to calculate the demand exceeding $x$. $$ P(demand \u003e x) = 1 - \\frac{\\Gamma(x + 1,\\ \\lambda)}{\\lfloor x \\rfloor !} $$ Fortunately, the MathNET library includes a function for calculating these values so we will not need to worry about implementing them. ","date":"2020-12-21","objectID":"/blog/2020/12/2020-12-21/:1:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 2","uri":"/blog/2020/12/2020-12-21/"},{"categories":null,"content":"Applying to Food Cart Problem How do we apply this to the food cart problem? Let’s say we want to know the odds of us being able to sell at least 800 pizzas. We can plug the value into the function, and we get a probability of 99.963% chance of demand exceeding 800. We can also put 1,000 in and find that there is only a 0.049% chance of demand for pizza being that high. We will use this in our objective function. We want to maximize the expected revenue so we will weight the revenue we receive for an item by the probability that we actually sell it. This means that the revenue we receive for the $n^\\text{th}$ item of a given food will be multiplied by the probability that demand meets are exceeds that amount. This gives us the following formula for expected revenue. $$ \\text{Expected Revenue} = \\sum_{f \\in food}\\sum_{n} P(\\text{demand} \\ge n) * \\text{Revenue}_{f} $$ We are using the same domain as in the previous post and we will be extending it. Let’s write a function which will gives us the probabilities of selling the $n^\\text{th}$ number of a food. To keep track of the $n^\\text{th}$ item we are going to add a new type to our domain. type NthItem = NthItem of int Now we create the function which calculates the probabilites for a given food for 1 item up to some max number of items. let createIncrementProbability (foodDemands: seq\u003cFood * DemandRate\u003e) (maxItems: int) = seq { for (food, DemandRate demandRate) in foodDemands do for i in 1..maxItems -\u003e let probability = 1.0 - (Poisson.CDF (demandRate - 1.0, (float i))) (food, NthItem i), probability } |\u003e SMap2 The createIncrementProbability function takes in a sequence of Food * DemandRate tuples for which we will generate probabilities. Food is a type we are using to represent burger, pizza, or taco. The DemandRate type represents the average daily demand for a food. For each food, we calculate the probability of demand meeting or exceeding the values from 1 to maxItems. maxItems is the maximum quantity of a given food we would consider packing into our food cart. We store this data in a SMap2 for ease use in our model formulation. We now create a function for building a model for our problem. Let’s call it create and have it take all the arguments we will need for our model. let create (revenue: SMap\u003cFood, float\u003cUSD/serving\u003e\u003e) (storage: SMap\u003cFood, float\u003ccm^3/serving\u003e\u003e) (fridgeSpace: SMap\u003cFood, float\u003ccm^3/serving\u003e\u003e) (weight: SMap\u003cFood, float\u003cgm/serving\u003e\u003e) (incrementProbability: SMap2\u003cFood, NthItem, float\u003e) (packDecision: SMap2\u003cFood, NthItem, Decision\u003cserving\u003e\u003e) (maxStorage: float\u003ccm^3\u003e) (maxWeight: float\u003cgm\u003e) (maxFridgeSpace: float\u003ccm^3\u003e) = Parameter Definitions: revenue is the amount of money we make when selling a particular Food storage is the amount of pantry space required to pack a single serving fridgeSpace is the amount of fridge space required to for a single serving weight is the weight for a single serving of a food incrementProbability is the probability of selling a particular quantity of a Food packDecision is a 2-dimensional SliceMap indexed by Food and NthItem. Food will correspond to burger, pizza, or taco. NthItem is the index of particular food within the group. The $1^\\text{st}$ pizza, the $2^\\text{nd}$ pizza, the $3^\\text{rd}$ pizza etc. The value stored in the SliceMap is a Boolean decision variable where 1 indicates that you should pack the food and 0 indicates that you should not. maxStorage is the maximum amount of storage space maxweight is the maxumum amount of weight that can be packed maxFridgeSpace is the maximum amount of refrigerated storage From here creating our model is straightforward. We need to create a constraints for the available storage, weight, and fridge space. We take advantage of the sum function and Hadamard Product operator, .*, included in the SliceMap library to make the notation simple. The Hadamard Product is taking care of the joining of the data before multiplying the values. let weightConstraint = Constraint.create \"MaxWeig","date":"2020-12-21","objectID":"/blog/2020/12/2020-12-21/:2:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 2","uri":"/blog/2020/12/2020-12-21/"},{"categories":null,"content":"Next Steps This post showed that even a simple mathematical planning model can yield significant improvements over simple heuristics for decision making. By taking the variability of demand into account we were able to realize a higher average revenue but also a more reliable amount of revenue. In the next post we will introduce a simple Machine Learning model to make predictions about demand in the future. So far, we have only used the historical average of demand to create our plan. What if we took that data and instead created a model for predicting demand and then fed that into our Mathematical Planning model? ","date":"2020-12-21","objectID":"/blog/2020/12/2020-12-21/:3:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 2","uri":"/blog/2020/12/2020-12-21/"},{"categories":null,"content":"One of the questions that I love to answer is, “What is the difference between Mathematical Planning and Machine Learning?” This is an excellent question. The fields are close to one another and solutions often involve both techniques. The way I differentiate is based on what question they are meant to answer. Mathematical Planning is primarily concerned with answering the question, “What should we do?” Machine Learning answers the question, “What is most likely?” We often ask these questions at the same time which is why the techniques can become conflated. I want to provide an example of a real-world problem that involves the marriage of these two techniques. Due to the amount of material to cover, I decided to break it into several posts. This first post will setup the problem, the Food Cart Packing Problem, and go over the tools we will use to evaluate the quality of different strategies. In the next post we will formulate a Mathematical Planning model to find a better strategy than the simple heuristic we start within this post. Finally, we will implement a Machine Learning model to make predictions on demand trends and feed that into the Mathematical Planning model for even more profitable strategies. Note: To see all the code for this post, go here ","date":"2020-12-14","objectID":"/blog/2020/12/2020-12-14/:0:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 1","uri":"/blog/2020/12/2020-12-14/"},{"categories":null,"content":"The Food Cart Problem An example problem that I frequently use is the Food Cart Problem. It is easy for people to conceptualize so we can focus on the techniques. We are running a Food Cart and we want to know what items to pack at the beginning of the day to maximize our revenue. In this case we sell Burgers, Pizza, and Tacos (it’s an eclectic food cart). Each food takes up a certain amount of pantry space, fridge space, and weight. Our food cart has recently been downsized so we must be purposeful about what we pack. We have kept track of how much we sell each day when we didn’t used to run out of inventory. We decide to start with a simple heuristic. On average, we sell 600 Burgers per day, 900 pizzas, and 700 tacos when we have excess inventory. Due to the downsize we are more restricted on space. We only have $3.0 m^3$ of storage, $2.0 m^3$ of fridge space, and can only carry $1,000 kg$ of weight. We have our same parking spot, so we expect to see the same demand. Below is a breakdown of the space and weight requirements per serving of the food we provide. Burger Pizza Taco Available Storage Space $700.0 cm^3$ $950.0 cm^3$ $800.0 cm^3$ $3.0 m^3$ Fridge Space $900.0 cm^3$ $940.0 cm^3$ $850.0 cm^3$ $2.0 m^3$ Weight Capacity $550.0 gm$ $800.0 gm$ $600.0 gm$ $1,000 kg$ ","date":"2020-12-14","objectID":"/blog/2020/12/2020-12-14/:1:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 1","uri":"/blog/2020/12/2020-12-14/"},{"categories":null,"content":"A Simple Heuristic We decide to prioritize pizza because it makes the most money, then tacos, then burgers. We pack for 900 pizza orders because that’s the average demand. With the remaining space, we pack tacos until we meet the average daily demand. If there is any room leftover, we will pack some burgers. Given the dimensions of our food, the result comes to 900 pizzas, 466 tacos, and 0 burgers. This makes sense to us since we have much less space than we did before, and we thought we may have to drop a food item from the menu. If we run the numbers, we expect to make $2,668.50 per day… or do we? The important nuance to this problem is that this plan was made using the average demand. Half of the time the demand is higher, half of the time the demand is lower. On days where the demand for pizza is greater than 900, we would not be able to capture that additional revenue anymore because we do not have enough inventory. At the same time, when the demand for pizza is below 900, we lose out on the possibility we could have sold more of other foods had they been in stock. We would like to get some idea of what the actual revenue value we should expect. Fortunately there is a tool for this, Monte Carlo Simulation. ","date":"2020-12-14","objectID":"/blog/2020/12/2020-12-14/:2:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 1","uri":"/blog/2020/12/2020-12-14/"},{"categories":null,"content":"Monte Carlo Simulation Monte Carlo simulation is conceptually simple, you can get an idea for the shape of a distribution (in our case expected revenue) by sampling from it many times and then computing descriptive statistics. Our hope is to find what the average revenue is expected to be. Note: I encourage people to read up on Monte Carlo. I do not have the space here to provide an exhaustive explanation. To my Statistician readers, I know I am glossing over details. I am trying to provide an engaging example without intimidating readers with mathematical rigor. Now, if you can remember your course on probability distributions, you may remember that the arrival rate of customers can be modeled by a Poisson Distribution. We will assume that this is a valid distribution to model the demand that we see at our food cart. Part of what makes this distribution easy to work with is that it takes a single parameter, $\\lambda$. The $\\lambda$ is easy to calculate, it is the mean of the data. Therefore, we say that pizza demand is a Poisson distribution with a $\\lambda$ of 900.0 or in math notation: $$\\text{PizzaDemand} \\sim Poisson(\\lambda = 900 )$$ $\\lambda$ can be thought of as the average rate of arrivals or in our case the overage number of orders in a day for a given food. Now we need to do some domain modeling and setup our little simulation. We will start with some types to describe our domain. type Food = Food of string type Lambda = Lambda of float The Food type differentiates between which food item we are referring to. The Lambda is the average demand we have observed and is the input for our Poisson distributions. We will use Units of Measure to track the units in our calculations. This will protect us from making silly conversion errors. [\u003cMeasure\u003e] type USD [\u003cMeasure\u003e] type cm [\u003cMeasure\u003e] type gm [\u003cMeasure\u003e] type serving USD stands for United States Dollars. cm and gm are the SI units for volume and mass. The serving measure is for the quantity of servings we are packing. We will be using the Math.NET library for performing calculations with distributions. We need a function for taking a plan and generating a random outcome for our revenue model. We will call this function many, many times to get an idea of the distribution of revenue. let sample (foodDemands: seq\u003cFood * Lambda\u003e) (revenue: SMap\u003cFood, float\u003cUSD/serving\u003e\u003e) (plan: Map\u003cFood, float\u003cserving\u003e\u003e) (rng: System.Random) = let evaluteSoldQuantity planAmount (Lambda lambda) rng = // Generate a random sample from the Poisson distribution and take the lesser // of the planned inventory or of the random Demand value that was generated let actualQuantity = Math.Min (float planAmount, Sample.poisson lambda rng |\u003e float) // Multiply by 1.0\u003cserving\u003e to get the correct units on the result actualQuantity * 1.0\u003cserving\u003e foodDemands |\u003e Seq.map (fun (food, demandRate) -\u003e food, (evaluteSoldQuantity plan[food] demandRate rng)) |\u003e Seq.sumBy (fun (food, soldQuantity) -\u003e soldQuantity * revenue[food]) What sample does is take the plan and perform a single simulation. It generates a random demand value for the given food and compares that to what we planned to have in inventory. It takes the lesser of our planned quantity or the random demand that was generated which becomes the actual amount sold. It takes the quantity of each food that was sold and multiplies it by the revenue of the food. It sums this across all the foods which gives us our revenue for that single simulation. sample performs a single simulation. We need to run many to calculate statistics on the results. We create an evaluate function which will call the sample function many times and gather the results. evaluate then computes the descriptive statistics and the revenues that were generated in our simulations. let evalute (foodDemands: seq\u003cFood * DemandRate\u003e) (revenue: SMap\u003cFood, float\u003cUSD/serving\u003e\u003e) (plan: Map\u003cFood, float\u003cserving\u003e\u003e) (rng: System.Random) (numberSamples: int) = let samples = seq { for _ in 1..numberSampl","date":"2020-12-14","objectID":"/blog/2020/12/2020-12-14/:3:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 1","uri":"/blog/2020/12/2020-12-14/"},{"categories":null,"content":"Evaluating Our Simple Heuristic Now that we have the ability to simulate the effects of different plans on our revenue, let’s see how well our simple heuristic does. First, we define the data for the parameters of our model. let burger = Food \"Burger\" let pizza = Food \"Pizza\" let taco = Food \"Taco\" let foods = [ burger pizza taco ] let revenue = [ burger, 1.3\u003cUSD/serving\u003e pizza, 1.6\u003cUSD/serving\u003e taco, 1.4\u003cUSD/serving\u003e ] |\u003e SMap let storage = [ burger, 700.0\u003ccm^3/serving\u003e pizza, 950.0\u003ccm^3/serving\u003e taco, 800.0\u003ccm^3/serving\u003e ] |\u003e SMap let fridgeSpace = [ burger, 900.0\u003ccm^3/serving\u003e pizza, 940.0\u003ccm^3/serving\u003e taco, 850.0\u003ccm^3/serving\u003e ] |\u003e SMap let weight = [ burger, 550.0\u003cgm/serving\u003e pizza, 800.0\u003cgm/serving\u003e taco, 600.0\u003cgm/serving\u003e ] |\u003e SMap let demandRates = [ burger, DemandRate 600.0 pizza, DemandRate 900.0 taco, DemandRate 700.0 ] let maxItems = 1_000 let maxWeight = 1_000_000.0\u003cgm\u003e let maxStorage = 3_000_000.0\u003ccm^3\u003e let maxFridge = 2_000_000.0\u003ccm^3\u003e We then define some functions which implement our simple heuristic for prioritizing pizza, then tacos, then burgers. // A function to call Math.Floor on floats with Units of Measure let floor (x: float\u003c'Measure\u003e) = Math.Floor (float x) |\u003e FSharp.Core.LanguagePrimitives.FloatWithMeasure\u003c'Measure\u003e // A function to call Math.Min on floats with Units of Measure let min (a: float\u003c'Measure\u003e, b: float\u003c'Measure\u003e) = Math.Min (float a, float b) |\u003e FSharp.Core.LanguagePrimitives.FloatWithMeasure\u003c'Measure\u003e // We packe the average daily demand of pizzas let pizzaQuantity = 900.0\u003cserving\u003e let tacoQuantity = // The number of possible tacos based on space let tacosBasedOnSpace = List.min [ (maxStorage - (pizzaQuantity * storage[pizza])) / storage.[taco] (maxFridge - (pizzaQuantity * fridgeSpace[pizza])) / fridgeSpace.[taco] (maxWeight - (pizzaQuantity * weight[pizza])) / weight.[taco] ] |\u003e floor // The min of taco demand and the space available min (700.0\u003cserving\u003e, tacosBasedOnSpace) let burgerQuantity = // The number of possible burgers based on space let burgersBasedOnSpace = List.min [ (maxStorage - (pizzaQuantity * storage[pizza]) - (tacoQuantity * storage[taco])) / storage[taco] (maxFridge - (pizzaQuantity * fridgeSpace[pizza]) - (tacoQuantity * fridgeSpace[taco])) / fridgeSpace[taco] (maxWeight - (pizzaQuantity * weight[pizza]) - (tacoQuantity * weight[taco])) / weight[taco] ] |\u003e floor // The min of burgers demand and the space available min (600.0\u003cserving\u003e, burgersBasedOnSpace) let plan = [ burger, burgerQuantity pizza, pizzaQuantity taco, tacoQuantity ] |\u003e Map Now let’s analyze the result of performing 100 simulations and analyze the distribution of the revenue. let rng = System.Random () let stats_100Runs = Simulation.evalute demandRates revenue plan rng 100 I like to print things out in tables, and I am loving the Spectre.Console project. Here is a table with the results of our simulations. ┌──────────────┬─────────┬──────────┬────────┐ │ NumberOfRuns │ Mean │ Variance │ StdDev │ ├──────────────┼─────────┼──────────┼────────┤ │ 100 │ 2075.50 │ 605.37 │ 24.60 │ └──────────────┴─────────┴──────────┴────────┘ Now we need to ask a critical question, “How confident are we in the answer?” Our initial run shows that on average, we appear to make $2,075.50. What happens if we run this experiment again? ┌──────────────┬─────────┬──────────┬────────┐ │ NumberOfRuns │ Mean │ Variance │ StdDev │ ├──────────────┼─────────┼──────────┼────────┤ │ 100 │ 2071.54 │ 926.30 │ 30.44 │ └──────────────┴─────────┴──────────┴────────┘ We get a slightly different answer, \\$2,071.54. Well, this is interesting. Our numbers are making sense based on our earlier thought experiment. We are no longer carrying extra inventory, so we do not benefit from days where demand is exceptionally high. We would have to have excessive demand for pizza and tacos to sell everything and achieve a revenue of \\$2,668.50. What we need to focus on answering now is how well do we know the Mean, the expected revenue. Fortunately, there i","date":"2020-12-14","objectID":"/blog/2020/12/2020-12-14/:4:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 1","uri":"/blog/2020/12/2020-12-14/"},{"categories":null,"content":"Next Steps We’ve covered quite a bit of ground in this post. We have introduced a new problem, the Food Cart Problem. We formulated a simple simulation which allows us to perform some statistical experiments to understand how good our simple heuristic is for packing the Food Cart. We also introduced the idea of a Confidence Interval for understanding how good our estimates are. Next time we will show how to use Mathematical Planning to find a better plan for packing the Food Cart that achieves a higher expected revenue which is also more reliable. ","date":"2020-12-14","objectID":"/blog/2020/12/2020-12-14/:5:0","tags":null,"title":"Maximizing Profitability of Inventory with Mathematical Planning and Machine Learning in F# - Part 1","uri":"/blog/2020/12/2020-12-14/"},{"categories":null,"content":" A designer knows he has achieved perfection not when there is nothing left to add, but when there is nothing left to take away Antoine de Saint-Exupery On my journey of growing as a developer, I am consistently inspired by language features which seem incredibly simple but yield remarkable benefit. As I try to master F#, I am frequently surprised by how powerful the language is for expressing ideas while having so few features. Discussions frequently pop up about the need for ever more powerful abstractions, yet I find myself amazed by how far you can take the language with what is already there. I am no programming language expert, but I admire languages that maintain a lean feature set. Every new feature added to a language makes it just a little bit more difficult to fully understand and a little more intimidating for new developers. It is an impressive design feat when a language can remain approachable for beginners but enable the flexibility that library authors need. I am an Industrial Engineering turned Machine Learning Engineer, and I focus on the problem of maximizing the profitability and efficiency of companies. Often the solution involves a Mathematical Planning Model (aka Mathematical Programming). What I hope to do in the next few paragraphs is illustrate to you how some of the most basic features of F#, Discriminated Unions and Units of Measure, eliminate the most pernicious bugs when developing these models. ","date":"2020-12-09","objectID":"/blog/2020/12/2020-12-09/:0:0","tags":null,"title":"Why I Love F# for Mathematical Planning","uri":"/blog/2020/12/2020-12-09/"},{"categories":null,"content":"The Domain of Mathematical Planning The domain of Mathematical Planning is made up of Decisions, Constraints, and Objectives. A Decision is a choice that a business needs to make. It can be how many of Item X do we buy, do we build in Location A or Location B, or how many people do we assign to each job. Constraints are the rules we need to abide by. They are the limitations on what is possible. A Constraint could be that we only have 10 people available, or we can only build in Seattle or Portland, or we only have $1,000,000 to invest. The Objective is how we measure success. It is the function we want to maximize or minimize. We could minimize waste, maximize profit, or minimize cost. Many of my colleagues are building their models with Python. Python is a great language and I have been productive with it in the past. Here is a snippet of what a mathematical planning model may look like in Python: # Define a list of items to optimize for items = [\"A\", \"B\", \"C\"] # Define a list of locations to assign items to locations = [\"Portland\", \"Seattle\", \"Detroit\"] # Define a dictionary of revenue associated with each item and location tuple revenue = {(\"A\",\"Portland\"):1.5;, (\"A\",\"Seattle\"):1.7 ... } # Define a dictionary with the availability of each item availability = {\"A\":10.0, \"B\":20.0, \"C\":14.0} # Create a Decision for each Item, Location combination. This will be how much # of a given item we decide to send to that location allocation = LpVariable.dicts(\"AmountSent\",(items,locations), 0) # Create an instance of a `Problem` object and state that we want to maximize # the objective we give it problem = LpProblem(\"ItemAllocation\", LpMaximize) # We create an expression which evaluates the total revenue revenue_expr = lpSum([revenue[i][l] * allocation[i][l] for i in items for l in locations]) # We set the Objective of the Problem by adding it problem += revenue_expr, \"MaximizeRevenue\" # For each item in items, create a constraint which states that the total number # of items that is allocated cannot exceed the availability of the item for i in items: problem += lpSum([allocation[l][i] for l in location] \u003c= availability[i]) This is the beginning of a straightforward assignment problem. We have a list of items, items. For each item in items, we must decide how many we send to each location in locations. There is a limit on how much of each item is available for us to send. There is a revenue associated with sending a particular item to a given location. In this problem we want to maximize our revenue which is calculated by multiplying the decision for a given item and location by the revenue associated with it. Finally, we create a constraint for each item in items which states that the total number of a given item that is allocated cannot exceed the total that is available. This is only part of the problem. Normally there would be more constraints that would make it more interesting. This is enough of a problem to illustrate my case though. There are two errors in this model already. If you were paying close attention you may have found one. I promise you cannot detect the second. ","date":"2020-12-09","objectID":"/blog/2020/12/2020-12-09/:1:0","tags":null,"title":"Why I Love F# for Mathematical Planning","uri":"/blog/2020/12/2020-12-09/"},{"categories":null,"content":"The Power of Domain Modeling Using Discriminated Unions F# provides two simple but powerful features which help ensure against the errors in the Python code. The first is Discriminated Unions. If we were to reformulate this problem using F#, the first thing we would do was define some simple types to model our domain. type Item = Item of string type Location = Location of string Instead of just using strings to describe our Items and Locations, we create simple, single case Discriminated Unions (DU). These DUs provide context around what the strings are meant to represent. Let’s go ahead and create our item and locations lists again. This time, wrapping them in DUs. let items = [\"A\"; \"B\"; \"C\"] |\u003e List.map Item let locations = [\"Portland\"; \"Seattle\"; \"Detroit\"] |\u003e List.map Location We will also update our availability information to use these new types. let availability = [ Item \"A\", 10.0 Item \"B\", 20.0 Item \"C\", 14.0 ] |\u003e Map We will create the Decisions for each item and location. We store these Decision types in a Map which is indexed by an (Item * Location) tuple. let allocation = DecisionBuilder\u003cServings\u003e \"AmountSent\" { for i in items do for l in locations -\u003e Continuous (0.0, infinity) } |\u003e Map We now attempt to create the same constraints we did in Python with a direct translation. let allocationContraints = ConstraintBuilder \"ItemLimit\" { for i in items -\u003e List.sum [for l in locations -\u003e 1.0 * allocation[l, i]] \u003c== availability.[i] } Except, the compiler is gives us an error on the indexing of allocation. What some of you may have noticed in the Python code is that the allocation collection is indexed by an Item then Location. The original code was trying to access it by location then by item. This would have thrown an error at runtime due to a missing value. In F# this becomes a compiler error. The type system itself it is helping you. This may seem small, but this is one of the most painful types of errors when debugging a Mathematical Planning model. Someone may say that this can be accomplished in other languages and I would agree. I believe where F# is unique is in the simplicity and ease of using single case Discriminated Unions for wrapping primitives. It is virtually no additional effort. ","date":"2020-12-09","objectID":"/blog/2020/12/2020-12-09/:2:0","tags":null,"title":"Why I Love F# for Mathematical Planning","uri":"/blog/2020/12/2020-12-09/"},{"categories":null,"content":"Units of Measure: The Achilles Heel of Numbers There is an underappreciated problem in software development, numbers are rarely just numbers. They represent something: cm, feet, kg, or meters. Normally we do not care about a raw number. Our primary concern is with what the number represents. In most languages there are no easy mechanisms for tracking the Units of Measure associated with a number. F# on the other hand has baked the concept of a Unit of Measure into the type system. The Units of Measure feature will reveal the second problem with the Python code that otherwise may remain undetected. Let’s update our domain with some new types to track the units on our numbers. [\u003cMeasure\u003e] type Servings [\u003cMeasure\u003e] type Kg We now have units to represent Servings and Kg. Let’s update our availability collection to store numbers with these units attached. let availability = [ Item \"A\", 10.0\u003cKg\u003e Item \"B\", 20.0\u003cKg\u003e Item \"C\", 14.0\u003cKg\u003e ] |\u003e Map We have now provided more context around our availability numbers. We now know they are stored in units of Kg. The F# compiler will enforce correct algebra as we work with them. We now update our Decisions to be in units of Servings. let allocation = DecisionBuilder\u003cServings\u003e \"AmountSent\" { for i in items do for l in locations -\u003e Continuous (0.0\u003cServings\u003e, 1_000_000.0\u003cServings\u003e) } |\u003e Map With our Decisions updated, we go back to our constraint definition and we now see a new bug. The important part of this message is at the bottom. The compiler is complaining that the left-hand is in units of Servings and the right-hand side is in units of Kg. It does not make sense to compare values that are in different units, so the compiler is throwing an error. In other languages this error would go undetected. Worse, it may not even be caught in unit testing because the math will still work, it just won’t give correct results. Let’s go ahead and add some conversion data so that we can fix this. let itemMass = [ Item \"A\", 1.1\u003cKg/Servings\u003e Item \"B\", 2.0\u003cKg/Servings\u003e Item \"C\", 0.7\u003cKg/Servings\u003e ] |\u003e Map We now have data which will allow us to convert from Serving to Kg. Let’s incorporate it into our constraint creation expression. let allocationContraints = ConstraintBuilder \"ItemLimit\" { for i in items -\u003e List.sum [for l in locations -\u003e itemMass[i] * itemAllocation[i, l]] \u003c== availability[i] } Now the compiler is happy because the units are in Kg on both sides. This simple feature of ensuring correct Units of Measure eliminates what is possibly the most nefarious bug in Mathematical Planning. It would be hard to calculate the number of hours wasted on badly formulated models due to mismatched Units of Measure. ","date":"2020-12-09","objectID":"/blog/2020/12/2020-12-09/:3:0","tags":null,"title":"Why I Love F# for Mathematical Planning","uri":"/blog/2020/12/2020-12-09/"},{"categories":null,"content":"Simple Building Blocks F# is an incredibly expressive language while staying lean on the number of features. Other languages have taken the approach of throwing every possible feature in. F# is relatively slow to incorporate new features and they are always purposeful. Most of the time the feature is orthogonal to the rest of the language. This is keeping the language approachable for newcomers so the climb to mastery is not nearly as steep. I believe these two simple features, Discriminated Unions and Units of Measure, uniquely position F# as an awesome language for Mathematical Planning. ","date":"2020-12-09","objectID":"/blog/2020/12/2020-12-09/:4:0","tags":null,"title":"Why I Love F# for Mathematical Planning","uri":"/blog/2020/12/2020-12-09/"},{"categories":null,"content":" Note: To see the completed code, please go here. All code for Model Mondays is kept in this repo and is MIT licensed. Feel free to use it! I was having a chat with a friend about what types of problems are good candidates for Mathematical Planning. He posed the question, “Would a Secret Santa exchange be a good candidate?” At first, I thought, “no.” As we kept chatting though, I changed my mind. This is a great problem for Mathematical Planning. For those who are not familiar with what a Secret Santa exchange is, it is when a group of people get together and all put their names in a hat. Everyone draws out a name. You then buy a gift for the person who’s name you drew. Normally everyone would get back together at a party and exchange gifts. The fun part is that you don’t know who is giving you a gift, so it is a double surprise. I initially did not think that this was a good candidate for Mathematical Planning because I didn’t see a quantifiable objective. There was no way to measure the difference in quality of the different pairings. All valid pairings are equally as good. Normally you would use Constraint Programming and/or SAT Solvers for these problems. SAT Solvers and Constraint Programming is interested in answering the question, “Is there an answer which satisfies these constraints?” versus Mathematical Planning which asks, “What is the best answer which satisfies these constraints?” The difference seems small, but the problem is wildly different. Finding the best answer to a problem is more difficult than finding an answer. Our conversation continued and my mind was still turning. A new piece of information dropped. This family does Secret Santa every year. Wait a minute, I thought. Don’t we want an answer which pairs you with new people each year? Wouldn’t it be better if you had someone different than you had the year before or the year before that?. This problem just became a Mathematical Planning problem! ","date":"2020-12-07","objectID":"/blog/2020/12/2020-12-07/:0:0","tags":null,"title":"Reindeer Secret Santa Assignment Problem","uri":"/blog/2020/12/2020-12-07/"},{"categories":null,"content":"A Reindeer Gift Exchange I decide to embrace a bit of whimsy and instead of people exchanging gifts, I make it Reindeer. I mean, wouldn’t Santa’s reindeer want gifts as well? I begin modeling my problem by creating a simple domain to describe my problem. type Reindeer = Reindeer of string type Giver = Giver of Reindeer type Receiver = Receiver of Reindeer type SecretSanta = { Reindeer : Reindeer PreviousReceivers : Receiver list } I assume I am going to get the names of the Reindeer as string so I wrap them in a single case Discriminated Union to provide context to the data. The Reindeer will be both givers and receivers so I make additional types to represent the direction of relationships: Giver and Receiver. The SecretSanta type represents a Reindeer and the other Reindeer they have given gifts in years past. The PreviousReceivers for a SecretSanta is an ordered list where the first element was the last reindeer the SecretSanta gave a gift to, the second element was the receipent two years ago, the third element the receiver three years ago, and so on. Our ideal solution has reindeer giving a gift to a different reindeer they have not given a gift to recently. Now I create a function which takes a list of SecretSanta, builds the model, solves it, and returns the assignments. I will call the function findAssignments. let findAssignments (santas:SecretSanta list) = I now want to get the list of reindeer that I will be working with and create sets of Giver and Receiver. The reason for storing this data in a Set will become apparent in a few moments. let reindeer = santas |\u003e List.map (fun s -\u003e s.Reindeer) |\u003e Set let givers = reindeer |\u003e Set.map Giver let receivers = reindeer |\u003e Set.map Receiver ","date":"2020-12-07","objectID":"/blog/2020/12/2020-12-07/:1:0","tags":null,"title":"Reindeer Secret Santa Assignment Problem","uri":"/blog/2020/12/2020-12-07/"},{"categories":null,"content":"Measuring Solution Quality I now need to create the penalty values for assigning a reindeer to a reindeer whom they have given a gift to recently. I will do this by using the List.mapi function which allows me to iterate through a list while providing the index for the item you are on. I use a simple heuristic for calculating the penalty. $$\\text{penalty} = \\text{NumberOfPreviousRecipients}-\\text{index}$$ What this does is provide a high penalty cost for assigning a reindeer to the reindeer they just given a gift to. From there the cost keeps going down. I will store the result in a SMap2 that will be indexed by the Giver type in the first dimension and the Receiver type in the second dimension. I have found using simple Discriminated Unions as a powerful tool for tracking how data is indexed. let penalty = [ for s in santas do // Get the number of receivers once let numberOfReceivers = s.PreviousReceivers.Length s.PreviousReceivers |\u003e List.mapi (fun idx r -\u003e ((Giver s.Reindeer), r), float (numberOfReceivers - idx)) ] |\u003e List.concat |\u003e SMap2 I now want to create the possible assignments. The key thing here is that it should not be possible to assign a reindeer to give a gift to itself. Therefore, I stored this data in a Set. The Set module has a convenient function Set.Remove which returns a new set with the single value removed from it. I will wrap the Reindeer values in the Giver and Receiver Discriminated Unions to provide context on what the values represent. This becomes incredibly valuable with using the slice notation of SliceMaps. let possibleAssignments = seq { for giver in reindeer do // We only want pairings with different reindeer for receiver in reindeer.Remove giver -\u003e (Giver giver, Receiver receiver) } We now have all the possible assignments for this reindeer Secret Santa exchange. I need to create a Boolean decision variable for each assignment. This decision variable is what the Solver engine will use to adjust to find a solution to my problem. 1.0 will indicate that the assignment should be used. 0.0 will indicate that the assignment should not be used. let assignDecisions = DecisionBuilder \"Assignment\" { for pairing in possibleAssignments -\u003e Boolean } |\u003e SMap2 The decision variables are stored in a 2-dimensional SliceMap, SMap2, where the first index is of type Giver and the second dimension is Receiver. We now need to create some constraints to ensure our solutions make sense. ","date":"2020-12-07","objectID":"/blog/2020/12/2020-12-07/:2:0","tags":null,"title":"Reindeer Secret Santa Assignment Problem","uri":"/blog/2020/12/2020-12-07/"},{"categories":null,"content":"The Secret Santa Constraints We now need to provides some Constraints for our problem. The Constraints describe what is and is not allowed. Without Constraints, our Model would give nonsensical answers The first set of constraints we will create are the giveOnlyOnce constraints. These state that a particular Giver may only give a gift once. We use the slice notation of SliceMaps to easily subset the values in assignDecisions. The compiler ensures that we are slicing the correct dimension because we have created types for Giver and Receiver. let giveOnlyOnce = ConstraintBuilder \"GiveOnlyOnce\" { for giver in givers -\u003e sum assignDecisions.[giver, All] == 1.0 } The second set of constraints stipulate that a Receiver may only receiver one gift. We iterate through the receivers values and create a constraint for each. let receiveOnlyOnce = ConstraintBuilder \"ReceiveOnlyOnce\" { for receiver in receivers -\u003e sum assignDecisions.[All, receiver] == 1.0 } ","date":"2020-12-07","objectID":"/blog/2020/12/2020-12-07/:3:0","tags":null,"title":"Reindeer Secret Santa Assignment Problem","uri":"/blog/2020/12/2020-12-07/"},{"categories":null,"content":"How We Quantify the Assignments We now want to create our penalty expression which is the function the solver engine will try to minimize. Because we stored our data in SliceMaps, we can use the sum function and the Hadamard Product, .*, to express this in a single line. let penaltyExpression = sum (penalty .* assignDecisions) We now create an Objective which states we want to Minimize this expression. let objective = Objective.create \"MinimizePreviousPairings\" Minimize penaltyExpression Now let’s build the model by composing these elements. let model = Model.create objective |\u003e Model.addConstraints giveOnlyOnce |\u003e Model.addConstraints receiveOnlyOnce We now attempt to solve the model and get the result. let result = Solver.solve Settings.basic model We match on the case of the result to decide what to call next. In our case, we just want to print the assignments. If this were a production model, we would do something more sophisticated. If the model is solved successfully, we select the decisions where the value is equal to 1.0 which indicates that the solver thinks we should use that assignment. We return a list of parings if successful, we return an string saying we couldn’t find a solution if the solve was unsuccessful. match result with | Optimal solution -\u003e let selectedPairings = Solution.getValues solution assignDecisions |\u003e Map.filter (fun pair value -\u003e value = 1.0) |\u003e Map.toSeq |\u003e Seq.map fst Result.Ok selectedPairings | _ -\u003e Result.Error \"Unable to find pairings\" We will create some data to test it out and use the amazing Specture.Console library to print it out as a nice table. Here is the function for printing out the results of solving our model. let prettyPrintResults (pairings: seq\u003cGiver * Receiver\u003e) = let table = Table() table.AddColumn(\"Giver\") |\u003e ignore table.AddColumn(\"Receiver\") |\u003e ignore for (Giver (Reindeer g), Receiver (Reindeer r)) in pairings do table.AddRow(g, r) |\u003e ignore AnsiConsole.Render(table) ","date":"2020-12-07","objectID":"/blog/2020/12/2020-12-07/:4:0","tags":null,"title":"Reindeer Secret Santa Assignment Problem","uri":"/blog/2020/12/2020-12-07/"},{"categories":null,"content":"Finding the Santa Plan Now let’s throw some data together and see what we get. let santas = [ { Reindeer = Reindeer \"Rudolph\"; PreviousReceivers = [ Receiver (Reindeer \"Blitzen\")]} { Reindeer = Reindeer \"Dasher\"; PreviousReceivers = [ Receiver (Reindeer \"Vixen\")]} { Reindeer = Reindeer \"Dancer\"; PreviousReceivers = [ Receiver (Reindeer \"Rudolph\")]} { Reindeer = Reindeer \"Prancer\"; PreviousReceivers = [ Receiver (Reindeer \"Cupid\")]} { Reindeer = Reindeer \"Vixen\"; PreviousReceivers = [ Receiver (Reindeer \"Dancer\")]} { Reindeer = Reindeer \"Comet\"; PreviousReceivers = [ Receiver (Reindeer \"Dasher\")]} { Reindeer = Reindeer \"Cupid\"; PreviousReceivers = [ Receiver (Reindeer \"Donner\")]} { Reindeer = Reindeer \"Donner\"; PreviousReceivers = [ Receiver (Reindeer \"Comet\")]} { Reindeer = Reindeer \"Blitzen\"; PreviousReceivers = [ Receiver (Reindeer \"Prancer\")]} ] let findResult = findAssignments santas match findResult with | Ok pairings -\u003e prettyPrintResults pairings | Error _ -\u003e printfn \"No Christmas this year :(\" When we run this, the console reports… ┌─────────┬──────────┐ │ Giver │ Receiver │ ├─────────┼──────────┤ │ Blitzen │ Dasher │ │ Comet │ Rudolph │ │ Cupid │ Blitzen │ │ Dancer │ Prancer │ │ Dasher │ Dancer │ │ Donner │ Cupid │ │ Prancer │ Vixen │ │ Rudolph │ Donner │ │ Vixen │ Comet │ └─────────┴──────────┘ Excellent work! Another Secret Santa successfully planned! ","date":"2020-12-07","objectID":"/blog/2020/12/2020-12-07/:5:0","tags":null,"title":"Reindeer Secret Santa Assignment Problem","uri":"/blog/2020/12/2020-12-07/"},{"categories":null,"content":"Real World Applications Though the domain for this problem was silly, the type of model this represents is common. Instead of reindeer and Secret Santa, this could have been operators and machines that they run. You would want operators to rotate through machines so that they are getting experience with all of them. This model would ensure that operators are regularly getting exposed to different machines. It could also be software developers and projects. Each project could have a set of technologies they require, and the developers have the type of project they were just on. To ensure that the developers are getting exposed to different tools, a model like the one we just built ensures they are being moved around. Assignment problems are incredibly common and normally there is some quantification of “goodness”. Assigning people to projects, assigning tools to work sites, assigning jobs to groups. Assignment problems come up in every industry. Let me know if there are specific problems you would like me to work on. Feel free to use this model and modify it for your own purposes. ","date":"2020-12-07","objectID":"/blog/2020/12/2020-12-07/:6:0","tags":null,"title":"Reindeer Secret Santa Assignment Problem","uri":"/blog/2020/12/2020-12-07/"},{"categories":null,"content":"There is a feature of F# this is incredibly powerful and rarely talked about: The Object Expression. It is such a simple idea that it is almost boring but the implications of it are profound. We often define an interface to abstract the implementation of an object so that we can code against different implementations. By programming against an IEnumerable or IDictionary, we can write algorithms which work against a host of different backing data structures. To use these abstractions, we implement the interface on our classes. What if I told you that there is a different way? What if I told you it was possible to build objects in a Frankenstien style where we could define each member as we chose? This may sound a little odd but the freedom it gives you in testing is incredible. Before we dive deep into how we use Object Expressions, let’s talk about what they are. ","date":"2020-12-04","objectID":"/blog/2020/12/2020-12-04/:0:0","tags":null,"title":"The Under Appreciated Power of Object Expressions","uri":"/blog/2020/12/2020-12-04/"},{"categories":null,"content":"Object Expression: What art thou? Whenever I start working with a new developer, there are two things I stress with them when it comes to reasoning about F#. F# does not have variables, it has values$^1$ Everything is an expression. Everything returns something, even if that something is nothing$^2$ These two ideas can cause some cognitive dissonance for imperative programmers used to C, C++, C# or their brethren. Those are great languages. The family of C languages and their imperative programming cousins just have a different philosophy. In imperitive programming you can just perform an action without it returning anything. In F#, something will always be returned because everything is an expression. Alright, so I’ve established that F# has a different way of thinking. But what makes Object Expressions special? Well, if we were working in C# and we wanted to create an object which fulfilled an interface contract, we would need to declare that type and then implement the interface. Let’s declare an incredibly simple interface IAnimal. interface IAnimal { string Name { get; } double Size { get; } } We have now declared an IAnimal interface. If we want to work with an instance of it, we must define a class which implements this interface. Let us define a Chicken class which implements this interface. Let’s assume that I want instances of this class to be immutable to protect against mutation. I’ll go ahead and make my life easier by using a C# 9.0 feature, Records. public record Chicken : IAnimal { public string Name { get; } public double Size { get; } public Chicken(string name, double size) =\u003e (Name, Size) = (name, size); } Okay, I have an interface and now I have a class which implements that interface. If I want to play around with functions or methods that take that interface as an argument, I will need to create instances of Chicken unless I want to declare another class which implements IAnimal. For testing purposes I may define a MockAnimal which has different behaviors. I may have to define several different MockAnimal classes if I want test different behaviors. So, this is all fine and doesn’t seem too cumbersome but the more complex the interface the more you will find yourself creating mock versions to test with. F# provides a different mechanism for implementing interfaces, the Object Expression! Let’s say we’ve abstracted the interaction with a database behind an IRepository inteface. Let’s declare a silly IRepository for storing and retrieving Pigs. Here’s that interface in F#. type Pig = { // Some fields which define a pig } type IRepository = abstract member GetById : id:int64 -\u003e Pig abstract member Save : pig:Pig -\u003e unit Our interface has two methods. One for saving Pigs and one for retrieving Pigs by an Id. In both cases we could possibly get an exception due to the database not being available. If we were in C# and we wanted to work with this interface, we would need to define a class which implements this interface. What if we could just define an object which implements this interface out of thin air? Let me show you how to do that. let geyById id = // A function in our domain that does the work of querying let save (pig:Pig) = // A function in our domain that does the work of saving let repository = { new IRepository with member _.GetById id = getById id member _.Save pig = save pig } What just happened? There are two functions in the domain which know how to do the work of retrieving and saving our pigs: getById and save. Instead of defining a new class, we said, “Hey F#, I want an object which fulfilles the IRepository interface and I want you to use these functions to do it.” We don’t need to define a class; we can just make an object and bolt together functions to fulfill the IRepository contract. I think of this as interface by Frankenstein assembly. ","date":"2020-12-04","objectID":"/blog/2020/12/2020-12-04/:1:0","tags":null,"title":"The Under Appreciated Power of Object Expressions","uri":"/blog/2020/12/2020-12-04/"},{"categories":null,"content":"Buy Why Do I Care? While that example may be cute, it may not wow you. Let’s talk where this is impactful: testing! One of the most difficult challenges we face when writing robust code is ensuring that our services both succeed when given clean inputs but also fail, the way we want them to. As a policy, we always write tests which confirm that the happy path works as intended and that the various failure modes are dealt with correctly. This type of testing gets painful when you have external dependencies like databases or APIs that are outside your domain. You either must be able to stand up a mock version of the service in your CI/CD pipeline or implement mock versions of your classes just for testing. What if instead of that, we could create an object which behaved exactly as intended? Let’s look at testing the IRepository interface. I have abstracted out what the backing service is in the case. Let’s I want to verify that my business logic responds appropriately to a SqlException. Rather than standing up a SQL Server instance and artificially creating a bad state, I’ll create an object that behaves exactly how I want it. let mockRepository = { new IRepository with member _.GetById id = raise (new SqlException()) member _.Save pig = () } This mockRespository will always raise a SqlException and will appear to be successful any time I call the Save method. I can have my mock repository behave any way that I want by just swapping out different functions to give the desired behavior. I don’t have to define a new class each time I want slightly different behavior. Where this gets even more exciting is when you are trying to test failure modes which involve multiple external services. It is relatively easy to abstract all of them behind interfaces and the construct mock instances that will behave exactly as intended. I believe that the Object Expression feature in F# is really under appreciated. It makes it easy to create arbitrary implementations of interfaces. I am sure that it has utility outside of testing as well. I have found that it makes unit and property-based testing relatively painless. I hope this opens your eyes a little to how you can streamline your testing needs in F#. $^1$ Technically it is possible to declare a variable in F#. You must add the mutable modifier to the declaration. The key point is that F# deals with values by default, not variables. $^2$ When F# returns “nothing”, it returns the type unit. The key thing is that even an action which does not “seem” to return a value is still actually returning something. The implications are deeper than I have time for here. I encourage the curious to look into “Category Theory for Programmers” by Barstosz Milewski. ","date":"2020-12-04","objectID":"/blog/2020/12/2020-12-04/:2:0","tags":null,"title":"The Under Appreciated Power of Object Expressions","uri":"/blog/2020/12/2020-12-04/"},{"categories":null,"content":"There are few things I love more than a fresh mathematical planning challenge so I was delighted when Kevin Avignon reached out to me and asked me to look at a question he had posted on the Software Engineer Stack Exchange site. He wanted to know whether the question was a candidate for Mathematical Planning. The question is a really interesting problem of pairing Mentors and Mentees. Mentors have a set of skills they can teach. Mentees have a set of skills they are interested in learning. Both Mentors and Mentees only have certain times they are available. Mentors are also capable of mentoring multiple mentees. So, for this sounds like a straightforward assignment problem but then there is a twist. The questioner wanted pairings of rare skills to be a higher priority than pairings of more common skills. Ah, now we have a Mathematical Planning problem! We have the three key ingredients: A Quantifiable Objective: We want to maximize the value of pairings based on the rarity of the skill Constraints to follow: Mentors and Mentees must match on skills and availability Decision to make: Which Mentors and Mentees do we pair at what time? Any time I see those three ingredients, I know I have a Mathematical Planning problem on my hands. I respond to Kevin and say, “Yes! You have a great candidate for Mathematical Planning! I’ll put something together for you this evening.” I begin mulling this problem as the day goes by. Once the family is all in bed, I turn on my PC and start modeling! ","date":"2020-12-03","objectID":"/blog/2020/12/2020-12-03/:0:0","tags":null,"title":"The Mentor Matching Problem","uri":"/blog/2020/12/2020-12-03/"},{"categories":null,"content":"Define the Domain The first thing I do is put together a tiny domain of types to represent my problem. I encourage anyone who is writing these models in F# to take advantage of domain modeling because it will protect you again some nefarious bugs. To learn more, check out Scott Wlaschin’s book Domain Modeling Made Functional. namespace MentorMatching module Types = type MentorId = MentorId of string type MenteeId = MenteeId of string type Skill = Skill of string type Period = Period of int type MenteeCapacity = MenteeCapacity of int type Mentor = { MentorId : MentorId Skills : Skill Set Periods : Period Set MaxMentees : MenteeCapacity } type Mentee = { MenteeId : MenteeId Skills : Skill Set Periods : Period Set } This simple set of types represent the problem space that I will build a mathematical planning model around. I now want to create a function which builds the model. I like to type out the functions signatures of what I am trying to implement beforehand as a way to get a high-level overview for that I am about to build. It’s an idea I got from Mark Seemann in his excellent PluralSight course Type-Driven Development with F#. Here is the function signature that I come to. let private buildModel (mentees:Mentee seq) (mentors:Mentor seq) (skillValue:SMap\u003cSkill,float\u003e) (pairingDecision:SMap4\u003cSkill,Period,Mentor,Mentee,Decision\u003e) : Model = The mentees and mentors arguments are self-explanatory. They are the mentors and mentees we want to pair up. The skillValue argument is the value we have assigned to each Skill for this problem. I will discuss the heuristic we use to calculate the value for a skill in a following section. The pairingDecision argument is a 4-dimensional SliceMap. The first dimension is the Skill, the second dimension is the Period, the third dimension is the Mentor, and the final dimension is the Mentee. The value in the SliceMap is a Boolean decision which indicates whether to use the pairing or not. We will be taking advantage of the slicing capabilities of SliceMaps to make the formulation more streamlined. The first thing we need to do is create a set of constraints which state that a given Mentee may only be assigned once. We use a ConstraintBuilder Computation Expression and iterate through the mentees, creating a constraint for each mentee. let menteeSingleAssignmentConstraints = ConstraintBuilder \"MenteeSingleAssignment\" { for mentee in mentees -\u003e sum pairingDecision[All, All, All, mentee] \u003c== 1.0 } Now we need to limit how many Mentees a Mentor can take on. This value is the MenteeCapacity of the Mentor. We will loop through the sequence of Mentors and create the constraints that we need. let mentorMaxAssignmentConstraints = ConstraintBuilder \"MentorMaxAssignments\" { for mentor in mentors -\u003e let (MenteeCapacity menteeCapacity) = mentor.MaxMentees sum pairingDecision[All, All, mentor, All] \u003c== float menteeCapacity } There is an important nuance to this problem that could easily be missed. Mentors can take on multiple Mentees. It is entirely possible that the same Mentor is mentoring two different Mentees at the same time. This does not make sense. We need to create a set of constraints which prevents this from happening. We will create a constraint for each Mentor and each period to ensure this is not the case. let mentorSingleAssignmentForPeriodConstraints = ConstraintBuilder \"MentorSingleAssignmentForPeriod\" { for mentor in mentors do for period in mentor.Periods -\u003e let x = pairingDecision[All, period, mentor, All] sum pairingDecision[All, period, mentor, All] \u003c== 1.0 } That is all the constraints that we need but we still need to create an expression that quantifies success, our objective function. We have assigned a value to each Skill. We want to award pairings of valuable skills. To do that we can multiply the Value of the Skill by the decision that corresponds to that skill. The SliceMap library provides a couple of conveniences for expressing this. The sum function and the .* operator. The s","date":"2020-12-03","objectID":"/blog/2020/12/2020-12-03/:1:0","tags":null,"title":"The Mentor Matching Problem","uri":"/blog/2020/12/2020-12-03/"},{"categories":null,"content":"Quantifying the Value of a Skill Earlier I mentioned that I would talk about the heuristic that was used to evaluate the value of a skill. I propose that we just rank the skills by the frequency of their occurrence and assign value based on its rank. let private getSkillValue (mentees:Mentee seq) (mentors:Mentor seq) = let menteeSkills = mentees |\u003e Seq.collect (fun m -\u003e m.Skills) let mentorSkills = mentors |\u003e Seq.collect (fun m -\u003e m.Skills) let skillCounts = Seq.append menteeSkills mentorSkills |\u003e Seq.countBy id let numberOfSkills = Seq.length skillCounts skillCounts |\u003e Seq.sortBy snd |\u003e Seq.mapi (fun i (skill, _) -\u003e skill, float (numberOfSkills - i)) |\u003e Map.ofSeq This ranking and scoring will ensure that the solver will choose low frequency skills over high frequency skills. There are some bizarre edge cases that could crop up, but I don’t have the space to cover them here. ","date":"2020-12-03","objectID":"/blog/2020/12/2020-12-03/:2:0","tags":null,"title":"The Mentor Matching Problem","uri":"/blog/2020/12/2020-12-03/"},{"categories":null,"content":"Why Mathematical Planning instead some other algorithm? While this was a fun challenge, it is important to step back and ask, “Why would I choose this technique over some other?” The question on Stack Exchange has an interesting discussion of different approaches people proposed. Each of them has their own merit. The reason that I often propose Mathematical Planning that it is easy to modify should the nature of the problem change. If some new constraint was required, it would be relatively easy to refactor the code and add it. Bespoke implementations of heuristics are often difficult to refactor and evolve over time. This was a fun challenge and I hope it ends up being useful. If you have questions about whether your problem is a good candidate for Mathematical Planning, please reach out! ","date":"2020-12-03","objectID":"/blog/2020/12/2020-12-03/:3:0","tags":null,"title":"The Mentor Matching Problem","uri":"/blog/2020/12/2020-12-03/"},{"categories":null,"content":"Recently I was asked if it would be possible to add the log function to the Flips library. Flips is a library for modeling and solving Linear and Mixed-Integer Programming problems. Both classes of problems are constrained to only having linear (i.e. straight) lines. You may ask, “What do you mean by straight?” The following are examples of linear functions. $$ \\displaylines{ \\text{Linear Functions}\\\\ y=1.0x+2.0 \\\\ y=2.0x_{1}+3.0x_{2} \\\\ y=1.2x_{1}+1.7x_{2}+x_{3} } $$ The following are non-linear functions. $$ \\displaylines{ \\text{Non-Linear Functions} \\\\ y=1.0x^2+2.0 \\\\ y=2.0/x_{1}+3.0x_{2} \\\\ y=1.2x_{1}+1.7x_{2}\\times x_{3} } $$ For a function to be linear in the domain of Linear/Mixed-Integer Programming the variables can only be added, subtracted, or multiplied by a coefficient. The reason this is important is because a Solver takes advantage of this structure which searching for solutions. ","date":"2020-12-01","objectID":"/blog/2020/12/2020-12-01/:0:0","tags":null,"title":"Modeling Non-Linear Functions with Flips","uri":"/blog/2020/12/2020-12-01/"},{"categories":null,"content":"What if we need a Non-Linear Function Fortunately, we have ways of working around this limitation. Another way to think of a curve is just a series of straight lines. We could approximate our curve using a series of straight lines that were close enough to the original function to make our answer meaningful. For this example, let’s try modeling the parabola $y=-x^2+10.0$. We will use this to represent the Objective Function of our model. Below you see a plot which has a smooth grey line for the exact values of our parabola and a series of point connect by blue line segments. You will notice that the blue line segments closely match the shape of the parabola. Our goal is to now model our original parabola with a series of segments. We will create a Decision variable which corresponds to each point on the plot. To get a value along the line segments we take a percent of the adjacent points. If I wanted the value of $y$ at the point $x=0.5$, I would use 50% of the value of $x$ at 0.0 and 50% of the value of $x$ at $1.0$. You may recognize this as linear-interpolation. If we want a value for x that is between our Decision variable, we just use a percent of the adjacent decisions. Let’s get to the code! We open the Flips library and generate the set of points we want Decisions for. We create a range of values from -5.0 to 5.0 and provide an index for the value. We extract the index values to be elsewhere in our code. open Flips open Flips.Types open Flips.SliceMap // The Range of values we want to consider and the index for the value let valueRange = [-5.0..5.0] |\u003e List.mapi (fun index value -\u003e index, value) // We will need the indices for the vertices of our lines let indices = valueRange |\u003e List.map fst We now want to create a Decision which corresponds for each of these points. // Create a decision variable for each point let decs = DecisionBuilder \"Amount\" { for i in indices -\u003e Continuous (0.0, 1.0) } |\u003e SMap Next, we need to create a constraints which says the total percentage of the points that we use must be equal to 1.0. This ensures that the solver is selecting a point along one of our segments. // We create a constraint saying that we must let totalOneConstraint = Constraint.create \"TotalValue\" (sum decs == 1.0) One of the other rules that we need to impose is that the Solver can only use adjacent points for interpolation. It would make no sense if the Solver interpolated between the points -5.0 and 5.0. To enforce this behavior, we are going to need to create an additional set of Decisions which correspond to the adjacent points along our line. We use the List.pairwise function to iterate through the adjacent indices and create the corresponding Decision. This decision type will be a Boolean because we either want the solver to use the pair of points or to not use them at all. // We create an indicator variable which corresponds to pairs of points // on the line we are modeling let usePairDecisions = DecisionBuilder \"UsePair\" { for pair in List.pairwise indices -\u003e Boolean } |\u003e SMap Now that we have a Boolean decision which corresponds to the pairs of Decisions, we need to create a set of constraints which will ensure that the Solver is only using one pair of points. We will do this with two types of constraints. The first constraint states that only one of the Pair decisions can be on at any given time. // A constraint stating that only one pair may be used let onlyOnePair = Constraint.create \"OnlyOnePair\" (sum usePairDecisions == 1.0) The second type of constraints is for each pair of points. It states that if the usePairDecision is set to 1.0, then the Solver must assign a total of 1.0 to the two corresponding decisions. // We state that if we want to use the pair of vertices, // the indicator variable associated with that pair must // be on as well let pairConstraints = ConstraintBuilder \"UsePair\" { for KeyValue ((i, j), d) in usePairDecisions -\u003e decs.[i] + decs.[j] \u003e== d } We now have all the structure we need in pla","date":"2020-12-01","objectID":"/blog/2020/12/2020-12-01/:1:0","tags":null,"title":"Modeling Non-Linear Functions with Flips","uri":"/blog/2020/12/2020-12-01/"},{"categories":null,"content":"Constraints on Non-Linear Functions To make things more interesting, let’s add a constraint which says that our parabola can only go up to -1.0. This would correspond to saying $x\\leq -1.0$. Now remember, we do not actually have a single $x$, we have a series of them which correspond to the different points on our plat. So how do we model this? Quite easily! We add a constraint which says the value of our decisions multiplied the corresponding y value, must be less or equal to -1.0. let lessThanNegativeOne = let valueExpression = List.sum [for (idx, v) in valueRange -\u003e v * decs[idx]] Constraint.create \"LessThan-1.0\" (valueExpression \u003c== -1.0) We can use the same code for creating the model and solving. We just add our new constraint to the model. let model = Model.create objective |\u003e Model.addConstraints pairConstraints |\u003e Model.addConstraint onlyOnePair |\u003e Model.addConstraint decisionsTotalToOne |\u003e Model.addConstraint lessThanNegativeOne let settings = Settings.basic let result = Solver.solve settings model match result with | Optimal solution -\u003e printfn \"Objective Value: %f\" (Objective.evaluate solution objective) And the result… Objective Value: 9.000000 val it : unit = () Again, we look at our plot and this makes sense. Hopefully, that provides a little insight into how to model non-linear functions using linear approximations. ","date":"2020-12-01","objectID":"/blog/2020/12/2020-12-01/:2:0","tags":null,"title":"Modeling Non-Linear Functions with Flips","uri":"/blog/2020/12/2020-12-01/"},{"categories":null,"content":"I was recently asked by someone, “What do I need to do to get into Machine Learning and Finance industry?” I told them that I would think about it and get back to them. I have been in many interviews for Machine Learning Engineers and I do have a set of questions that I frequently use to get a feeling for where someone is in their career and where they are hoping to go. We are looking for people at various phases in their growth, so it is less of a concern about whether someone is some elite developer and more about whether they are inquisitive and eager to learn. I will not share the questions I use in interviews, but I will happily share the attributes I am looking for. ","date":"2020-10-07","objectID":"/blog/2020/10/2020-10-08/:0:0","tags":null,"title":"What I Look for in a Machine Learning Engineer","uri":"/blog/2020/10/2020-10-08/"},{"categories":null,"content":"Sound Engineering Before anything else, I am looking for someone’s engineering acumen. I went to college for Chemical Engineering and I believe on our first day the professor defined engineering as problem solving under constraints. The key thing that we learned was how to take an abstract problem statement, break it down into manageable pieces, and then solve the smaller subproblems. I believe that most of engineering discplines could be boiled down to this, they just deal with different problem domains. I believe a key differentiator between more and less experienced developers is in understanding where to break the problem down. Problems have an inherent amount of complexity. Good design can mitigate that complexity keep it from growing out of control. It is the same concept of algorithmic complexity. We analyze an algorithm and see how the runtime grows as the size of the problem increases. In the same way, we can look at an architecture and see how the complexity grows as we increase functionality and services. The domain of Machine Learning is still relatively fresh, and the industry is still churning with new Python libraries popping out every other week. Therefore, the solutions we build for delivering Machine Learning Models must be well designed because they will likely need to evolve. Building a system that can cleanly evolve over time is a difficult engineering challenge. I am looking for people who understand the implications of decisions in the near term but also how it will affect the evolveability of the system going forward. ","date":"2020-10-07","objectID":"/blog/2020/10/2020-10-08/:1:0","tags":null,"title":"What I Look for in a Machine Learning Engineer","uri":"/blog/2020/10/2020-10-08/"},{"categories":null,"content":"Strong Machine Learning Fundamentals Though this may seem obvious, there are some nuances I wanted to highlight. While I am fascinated by the advances we are seeing with Neural Networks and Reenforcement Learning, they should not be the first tool you reach for. I am going to ask someone about a hypothetical problem and get their feedback on how they would approach it. If the first thing they suggest is, “Oh, use a Neural Network!” red flags are going to go off. Neural Networks are powerful, but they should not be the first tool you reach for in your toolbox. It is impressive just how far you can get with Linear Regression and Logistic Regression. These algorithms are robust, and their behavior is easily explained. It is enourmously valuable to be able to explain the behavior of a model to your non-ML colleagues in terms they can understand. You can then take things a step further and do some data exploration and try subsetting your population and having different models for the different populations. Think of it as a brute force Ensemble technique. Now, if you reach the end of Linear and Logistic Regression, I am all for trying more sophisticated techniques. You just should not start with the fanciest tool in your toolbox. I also want someone to understand the failure modes of the model and how they impact the outcome. It is easy to get caught up in trying to tune a model for a particular metric but is that really the best thing for how it will be used in production? Machine Learning models really took off when they were applied to the field of advertising where the upside of being correct was large but the downside of being wrong was low. This was a perfect application of these tools. What happens when the downside of a wrong prediction is large? What if a wrong prediction leads to someone dying? What metric are you going to tune the model for in that scenario? I want someone who is going to ask the question, “Why do you want to predict that? How do you plan to use the prediction?” because only when you know that can you really discern the best model. ","date":"2020-10-07","objectID":"/blog/2020/10/2020-10-08/:2:0","tags":null,"title":"What I Look for in a Machine Learning Engineer","uri":"/blog/2020/10/2020-10-08/"},{"categories":null,"content":"Team Skills The industry often calls these “Soft Skills”. I think that undervalues how critical they are. The problems we are facing now are so large and complex they cannot be completed by a single individual. If someone does not have the ability to operate well within a team, I am not interested. I do not care how much of a wizard you are. If you are one of the fabled 10X developers but you apply a negative multiplier to the rest of the team, it is going to be a net loss for us. The first and most critical Team Skill is the ability to communicate in writing. This includes the clarity of your code, emails, chat, diagrams, all of it. If you do not have the ability to put your thoughts down into a written form, it is going to be difficult for you to pass your ideas around the team. So much of our communication now occurs through text whether it be email, code, pull requests, and chat messages. I will not automatically disqualify someone if there writing is not amazing, but it is a clear differentiator. If you have a blog where I can see your work and how you explain your thoughts, you are automatically in the top 5% of people I interview. Next, I want to know if you can explain complex ideas. Throughout the interview I am going to be listening for what you are passionate about or have a deep understanding of. I am then going to ask you to explain that concept to me. I may already be familiar with the subject, but I want to know if you can explain it to someone who is not a domain expert. Developers who can break complex problems down into explainable pieces are a rare breed. ","date":"2020-10-07","objectID":"/blog/2020/10/2020-10-08/:3:0","tags":null,"title":"What I Look for in a Machine Learning Engineer","uri":"/blog/2020/10/2020-10-08/"},{"categories":null,"content":"Technical Skills There is a reason this section comes last. While it is important, it is the the that least differentiates people. I care little about which stack or set of technologies you have worked with in the past. If you happen to be familiar with what we are using it is a small bonus, but it is nowhere near the most important thing. I will want to know if you have worked with the concepts though. I want to know if you have worked with Object-Oriented and Functional languages. I want to know if you have worked with Relational Databases or streaming systems. The reason for this is more to guage what training someone would need should they come on board. Finding someone who spends time learning on their own is a huge bonus because it shows curiousity and initiative, not because I expect them to work overtime to get up to speed. ","date":"2020-10-07","objectID":"/blog/2020/10/2020-10-08/:4:0","tags":null,"title":"What I Look for in a Machine Learning Engineer","uri":"/blog/2020/10/2020-10-08/"},{"categories":null,"content":"Academics I don’t care. I honestly don’t. I used to be really intimidated by people with degrees from top end schools and honestly, the above areas matter more in the long run. I work with people from wildly different backgrounds. Do not count yourself out if you do not have a Computer Science degree. I cannot remember where anyone on my team went to school or whether they have degrees. ","date":"2020-10-07","objectID":"/blog/2020/10/2020-10-08/:5:0","tags":null,"title":"What I Look for in a Machine Learning Engineer","uri":"/blog/2020/10/2020-10-08/"},{"categories":null,"content":"Final Thoughts So, what do I say to my friend who asks me, “How do I get into Machine Learning and Finance?” I say, practice and show your work. Start a blog. Talk about what you are learning each week. Learn to turn complex problems into clear prose. Find a problem that you are intersted in and create a GitHub repo where you document your progress. Find some interesting data set and perform some analysis on it. Document what you find. Create a model for predicting an outcome and explain which metrics you used to tune the model and why you chose those metrics. Do it using Pull Requests with tagging. Create a simple ML Model that you deploy as a Nuget package and setup an automated CI/CD pipeline to deliver new versions of the model when you merge to the main branch. These are the daily things we do as being developers. Finally, and perhaps most importantly, get involved in the community. Just find a developer tribe you jive with and connect with some people. I know there are some hostile communities out there but there is also an abundance of generous developers. I know Python has tons of resources. I love F# and it is the most generous community I have been a part of. R also has great people and meetups. Connecting with people and learning from them will create more opprotunities than anything else. ","date":"2020-10-07","objectID":"/blog/2020/10/2020-10-08/:6:0","tags":null,"title":"What I Look for in a Machine Learning Engineer","uri":"/blog/2020/10/2020-10-08/"},{"categories":null,"content":"I am on a bit of a quest to bring Mathematical Optimization to the masses, or at least to Software Developers. I often come across problems where people are wanting to find the best plan for their problem but they lack to tools to express the problem. Typically the way this is “solved” is by some domain expert coming up with a laborious heuristic in Excel which involves outrageous amount of copying and pasting. I have seen this is take place in tiny companies all the way up to multi-billion dollar enterprises. What really breaks my heart when I see this is that I know there is a better way, but people are just are not aware of it. This is why I am pushing for more training on Mathematical Optimization at my company and why I am starting a blog series on Mathematical Modeling. ","date":"2019-11-11","objectID":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/:0:0","tags":null,"title":"The Anatomy of an Optimization Model","uri":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/"},{"categories":null,"content":"The Goal My hope with these blog posts is to start from a barebones introduction to the concepts which undergird mathematical modeling and slowly introduce newer and more advanced modeling techniques. I don’t think I will ever finish because I am always across new and interesting problems. What I will emphasize is the beautiful interplay between Machine Learning and Mathematical Optimization. I have been blessed to work on several projects where we were able to marry these tools to great effect. If you have any problems that you would like me to look at and write a blog post on, I would be happy to. My hope is to give examples which help people with their own work. ","date":"2019-11-11","objectID":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/:1:0","tags":null,"title":"The Anatomy of an Optimization Model","uri":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/"},{"categories":null,"content":"Ingredients for an Optimization Problem There are three things you need to have before you are reading to apply Mathematical Optimization. You need 1) A Quantifiable Objective 2) Decisions you control and 3) Rules to follow. When you have these three things, you have a problem ripe for Mathematical Optimization. Let me unpack them a little to make sure you understand what I am talking about as we move forward. ","date":"2019-11-11","objectID":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/:2:0","tags":null,"title":"The Anatomy of an Optimization Model","uri":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/"},{"categories":null,"content":"A Quantifiable Objective For you to apply Mathematical Optimization to a problem you need a way to measure how successful you are. Without this, a Solver will not have a way to check if the solutions that it is finding are actually an improvement or not. You are typically trying to maximize some value or minimize it. Common Objectives are Maximize Revenue, Minimize Waste, Maximize User Engagement, Minimize Energy Use, or Minimize Cost. What you are trying to achieve could really be anything, the key is that you have the ability to quantify it. ","date":"2019-11-11","objectID":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/:2:1","tags":null,"title":"The Anatomy of an Optimization Model","uri":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/"},{"categories":null,"content":"Decisions You Control When you create a Mathematical Model of your problem you are going to define the Decisions which need to be made. This is often something like how many people you assign to a task, how much water to put where, where to place a warehouse, which ad to show where. These are all examples of decisions you will be making in your business. The important thing is that you actually have control over them. When you solve the Optimization Model, you will get answers for what values you should use for your Decisions. It will tell you how many people to assign to a task or where to put the water or which ad to show where. ","date":"2019-11-11","objectID":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/:2:2","tags":null,"title":"The Anatomy of an Optimization Model","uri":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/"},{"categories":null,"content":"Rules to Follow Strictly speaking, it is possible to have an Optimization Problem without a set of rules to follow but it is rare. Most real world problems will have some kind of constraint on what you are allowed to do. Typically there are a limited number of people or only so many locations where you can place a warehouse or only so much power available. It is often the rules to follow which make a Optimization Problem interesting. Most often someone is trying to find the best plan given a set of restrictions that they need to follow. These restrictions are what make finding the best answer difficult. ","date":"2019-11-11","objectID":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/:2:3","tags":null,"title":"The Anatomy of an Optimization Model","uri":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/"},{"categories":null,"content":"Where we go from here Let’s say that we have checked all the boxes and it looks like you have a problem which is a good candidate for Mathematical Optimization. What do we do now? We formulate a model. How we do that is what I am looking forward to opening up to you in this and future blog posts. Once we have a Model of our problem we are able to hand it to a piece of software called a Solver which knows how to take the model and search for the best possible solution. My plan for this series is to follow a simple pattern. First, present a real world problem which will help us ground the concepts. Second, develop the mathematical model and walk through how it works. There will be some math notation but I’ll walk through it slowly so you don’t get lost. Thirdly, translate the model into code. I will be using Python and the PuLP library for my examples. Python is ubiquitous and the PuLP library is open source and easy to install. In the rest of this post I will walk through a toy problem for the purpose of introducing the vocabulary of Mathematical Optimization Modeling. In future posts I will work more complex problems which will have interesting characteristics. ","date":"2019-11-11","objectID":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/:2:4","tags":null,"title":"The Anatomy of an Optimization Model","uri":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/"},{"categories":null,"content":"The Food Truck Problem One of the example problems I like to use is that of a Food Truck. I am from Portland, OR USA originally and we had food trucks everywhere. In this example we are running a food truck and we have to decide what items to pack for the day. We sell Hamburgers and Burritos. Hamburgers sell for \\$5.00 and Burritos sell for \\$7.50 (they are big Burritos). Each Hamburger requires us to carry 1.0 Lb. of ingredients on the truck and each Burrito requires 1.5 Lbs. (I told you they are big). We have a small Food Truck so we can only carry up to 650 Lbs. of ingredients. We also forgot to go to stock up the day before so we only have 200 Hamburger buns on hand and only 300 tortillas for Burritos. Since we run an awesome Food Truck, we always sell out of everything we bring. The question now becomes, how much Hamburgers do we pack for and how many Burritos so that we maximize our profit? Note: This example problem is meant to be simple. I am mostly concerned with introducing the vocabulary of Optimization Modeling. Future problems will be more complex. This problem is a clear example of a Mathematical Optimization Problem. It has a Quantifiable Objective, Maximize Revenue. It has Decisions which we can control: the number of Hamburgers and Burritos we will pack for. Finally it has rules we must follow, the Food Truck can only carry 650 Lbs, we only have 200 Hamburger Buns and we only have 300 tortillas for Burritos. Now, I am going to show you how we formulate this as an Optimization Model and then I will walk through each piece so that it makes sense. For this model I am going to use the variable $x_{1}$ to represent the number of Hamburgers we are going to pack for and $x_{2}$ to represent the number of Burritos. $$ \\begin{align} \u0026\\text{Maximize: }5.00 x_{1} + 7.50 x_{2} \\\\ \u0026\\text{Subject to:} \\\\ \\end{align} $$ $$ \\begin{align} x_{1} \\leq\u0026 200 \\\\ x_{2} \\leq\u0026 300 \\\\ 1.0x_{1} + 1.5x_{2}\\leq\u0026 650 \\\\ x_{1}, x_{2} \\geq\u0026 0 \\end{align} $$ Let’s unpack this. The first line of any Mathematical Optimization Model is going to be the Objective Function. This is the function which is used to quantify success. It will start with whether we are trying to Maximize the value of the Objective Function or Minimize it. In this case we are trying to Maximize. The formula that you see is the calculation for Revenue. Remember, $x_{1}$ is the number of Hamburgers and $x_{2}$ is the number of Burritos. For every Hamburger we will earn \\$5.00 and for each Burrito we will earn \\$7.50. This means to calculate the total revenue we multiply the number of Hamburgers by the revenue per Hamburger and the number of Burritos by the revenue per Burrito: $5.00x_{1} + 7.50x_{2}$. After the Objective Function we get to a section referred to as the Constraints. This section typically begins with either a “Subject to” or just “S.t.” as a shorthand. This section is describing the rules that we need to follow. The first constraint is our limitation on the number of Hamburgers due to the number of buns that we have. We only have 200 buns available which means that $x_{1}$ must be less than or equal to 200. We write that as a constraint in this way: $x_{2} \\leq 200$. The next constraint is describing our limit on the number of Burritos we could pack since we only have 300 tortillas. $x_{2}$ represents the number of Burritos we plan to pack and it must be less than 300 therefore we add this constraint: $x_{2} \\leq 300$. The third constraint represents the weight limit of our Food Truck. We can only carry 650 Lbs. so the combination of the number of Hamburgers and the number of Burritos must be less than this. We multiply the number of Hamburgers by the lbs per Hamburger and the number of Burritos by the lbs per Burrito and add them together to get the total weight. That total must be less than the capacity of the Food Truck. This gives us this constraint: $1.0x_{1} + 1.5x_{2} \\leq 650$. The final line of the model states the number of Hamburgers and Burritos ca","date":"2019-11-11","objectID":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/:3:0","tags":null,"title":"The Anatomy of an Optimization Model","uri":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/"},{"categories":null,"content":"To the Code Now that we have walked through this small problem, let’s see how it translates to code. I am using Python 3.8 and PuLP 1.6.8. The first thing we do is import PuLP and create a new instance of a problem object. # Import the PuLP Library from pulp import * # Create an instance of a Problem object to populate problem = LpProblem(\"Food Truck\", LpMaximize) PuLP provides us the tools we will need to create the Optimization Model and then solve it. Out of the box the PuLP library comes with some open source solvers so you can build and solve models without having to purchase a solver license. The first argument of the LpProblem function is the name of our problem. The second is the type of optimization we want to perform, Maximization or Minimization. In this case we are wanting to maximize revenue so we use the argument value of LpMaximize. If we wanted to minimize we could have used the LpMinimize value. Now let’s create some decision variables to represent how many burgers and tacos we are going to bring on our food truck. We do this by using the LpVariable function. x1 = LpVariable(\"burgers\", 0) x2 = LpVariable(\"tacos\", 0) The arguments for LpVariable are the name of the variable and the lower bound on the possible value. In some problems, the decision variables can take on negative numbers. In this case, having negative tacos or negative hamburgers does not make any sense so we specifiy that the lower bound is 0. We now have the Decision Variables for the problems so we can now add the Objective Function and the Constraints. Let’s start with adding the Objective Function. Whenever we want to add something to a problem we use the += operator. The PuLP library will infer whether we are adding an Objective Function or a Constraint based on the right hand side argument. All we need to do for the Objective Function is to provide the equation. # Add the Objective Function problem += 5.00*x1 + 7.50*x2 The problem object now has an Objective Function. Now let’s go about adding the constraints. The first constraint is the Max Burgers constraint. To do this we use the += operator to add constraints to our problem object. We then give the equation for the constraint and the name of the constraint. # Add Max Burgers constraint problem += x1 \u003c= 200, \"Max Burgers\" We then need to add the Max Burritos constraint. # Add Max Burritos Constraint problem += x2 \u003c= 300, \"Max Burritos\" Finally we need the Max Weight constraint. # Add Max Weight Constraint problem += 1.0*x1 + 1.5*x2 \u003c= 650, \"Max Weight\" We now have a fully populated problem. To solve it, all we need to do is call the solve() method. # Solve the problem problem.solve() \u003e 1 We get a numeric response back but it will not mean much until we translate it to something we can understand. Fortunately, the PuLP library has a dictionary which stores the mapping from the numeric status of the problem to a human readable string. This dictionary is the LpStatus dictionary. Let’s use this to print out the string representation of the problem status. # Print the problem status print(LpStatus[problem.status]) \u003e Optimal We should see the string optimal. This means that the solver was able to find the optimal answer. In the future we will go over the other possible statuses and what they mean. Now, let’s look at what values for the Decision Variables the Solver chose. # Loop through each of the Decision Variables in the problem for v in problem.variables(): # Print the name of the Variable and the Value the Solver chose print(v.name,'=',v.varValue) \u003e burgers = 200.0 \u003e tacos = 300.0 Let’s see what kind of Revenue we should expect if we follow this plan. # Get the expected Revenue revenue = value(problem.objective) # Print the expected result print(f\"${revenue:,.2f}\") \u003e $3,250.00 There you have it. A tiny Mathematical Optimization problem. Granted, this was completely overkill for such a simple problem. My goal was to introduce these concepts and start growing our vocabulary around Optimizat","date":"2019-11-11","objectID":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/:4:0","tags":null,"title":"The Anatomy of an Optimization Model","uri":"/blog/2019/2019-10-27-the-anatomy-of-an-optimization-model/"},{"categories":null,"content":"About Matthew Crews","date":"2019-08-02","objectID":"/about/","tags":null,"title":"About Matthew Crews","uri":"/about/"},{"categories":null,"content":"Performance, Simulation, and Optimization. Three of my favorite words 😊. I’ve been obsessed with building decision optimization tools for business since college. I studied Chemical and Industrial Engineering so that I can build tools to maximize profitability and efficiency. To optimize a business, you need a simulation or model of how it operates. For a simulation to be useful, it must be fast. To be fast, you need high-performance code. By bringing these three passions together I have helped companies make millions in additional profit and massively reduce waste and inefficiency. In this blog I share my experience of using F# to write high-performance simulations to optimize profit and efficiency. Feel free to reach out if you have questions. I’d love to help! Email: hi@fastfsharp.com Twitter: @FastFsharp ","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"About Matthew Crews","uri":"/about/"},{"categories":null,"content":"Previously I described how we can perform Discrete Optimization using the Branch and Bound technique. Today I want to describe another foundational technique for Discrete Optimization, Cutting Planes. Cutting Planes is like the Branch and Bound technique in that it uses a series of LP Relaxations to search for solutions. Where it is different from Branch and Bound is in how it refines the LP Formulations. Branch and Bound would subdivide the solution space by branching on a decision variable and creating two new subproblems. Instead, what Cutting Planes does is iteratively add constraints which eliminate the nonintegral solutions from the feasible space while not eliminating any feasible integral solutions. These constraints that we add are called “Cuts” because they are cutting off nonintegral solutions from the feasible space. ","date":"2018-05-25","objectID":"/blog/2018/2018-05-24-discrete-optimization-with-cutting-planes/:0:0","tags":null,"title":"Discrete Optimization with Cutting Planes","uri":"/blog/2018/2018-05-24-discrete-optimization-with-cutting-planes/"},{"categories":null,"content":"The Cutting Planes Algorithm Conceptually the Cutting Plane Algorithm is rather simple. It is made up of the following three steps. Step 1: Solve the LP Relaxation of the Current Problem Step 2: Check if the integrality requirements of the initial problem have been met. If so, terminate, an optimal solution has been found. If integrality has not been achieved proceed to Step 3. Step 3: Add a constraint to the problem which removes the current optimal solution from the feasible space but does not eliminate any of the integer feasible solutions. Return to Step 1 Obviously you need to check for infeasibility and unboundedness as well. If either one of those conditions arise, terminate. That is really all there is to this algorithm. The art is in Step 3, generating cuts. A good cut will accelerate the convergence toward an integral solution. Part of the challenge is that there are frequently an infinite number of cuts which could work to reduce the size of the solution space while not eliminating integer feasible solutions. The trick then becomes finding good cuts quickly. Searching for the best cut is in of itself an optimization problem. We cannot afford to spend an infinite amount of time searching for the best cut though. There are recipes for being able to calculate good cuts quickly. We will go over some of them in a future post. For now, let’s walk through a graphical example of how Cutting Planes works. ","date":"2018-05-25","objectID":"/blog/2018/2018-05-24-discrete-optimization-with-cutting-planes/:1:0","tags":null,"title":"Discrete Optimization with Cutting Planes","uri":"/blog/2018/2018-05-24-discrete-optimization-with-cutting-planes/"},{"categories":null,"content":"Graphical Walkthrough Let’s say that we have a Discrete Optimization problem with two constraints, Constraint A and Constraint B. These two constraints define a space in which the solution must lie. Our decision variables are $x_1$ and $x_2$. For this scenario they are integer decision variables. Here is a quick sketch of the example problem. This is just a conceptual walkthrough of Cutting Planes so don’t fret about exactly what the numbers are. The dotted grey lines are the boundaries of the constraints. The blue dots indicate the integer feasible solutions and the green arrow is the direction in which the objective function is pointing (the direction of greatest improvement). If we take the LP Relaxation of this problem the optimum would be at the intersection of Constraint A and Constraint B. In the following image this point is indicated by the green dot. We have labeled the solution to this initial LP $Z_{LP}^{0}$ . The superscript indicates which iteration this is the solution to and the $LP$ subscript indicates it is a solution to the LP Relaxation. We check if the integrality requirements of the original problem have been met. They have not since the optimal solution to the LP does not lie on integer values for $x_1$ and $x_2$. Now we need to generate a new constraint that we can add to the problem which removes $Z_{LP}^{0}$ from the feasible space but does not eliminate any of the integer feasible solutions (the blue dots). Later we will go over some methods for calculating these constraints. For now let’s just use visual analysis. One of the easiest constraints that we can add is $x_2 \\leq 4$. It removes $Z_{LP}^{0}$ from the feasible space but does not cut off any of our integer feasible solutions. Let’s add this constraint and redraw our problem. Let’s draw the new constraint as a yellow dotted line. I have taken the liberty of marking the new solution $Z_{LP}^{1}$ on the diagram. The good news is that now $x_2$ has taken on an integer value but $x_1$ has not. $x_1$ lies between $2$ and $3$ so we need to add another constraint. Again we can just look at the problem and see we can add the constraint $x_1 + x_2 \\leq 6$. This does not remove any integer feasible solutions but it will remove $Z_{LP}^{1}$ from the feasible space. Let’s add this constraint and see what we get. We have added the constraint of $x_1 + x_2 \\leq 6$ and found the new solution, $Z_{LP}^{2}$ . Our new solution $Z_{LP}^{2}$ lies on integral values of $x_1$ and $x_2$. We can now end our search since we have found a solution which meets the integrality requirements of the original problem. We have successfully used Cutting Planes to solve a Discrete Optimization problem! ","date":"2018-05-25","objectID":"/blog/2018/2018-05-24-discrete-optimization-with-cutting-planes/:2:0","tags":null,"title":"Discrete Optimization with Cutting Planes","uri":"/blog/2018/2018-05-24-discrete-optimization-with-cutting-planes/"},{"categories":null,"content":"Next Steps This walkthrough had some nice pictures but you should have this nagging question, “How do we generate these cuts?” In this problem it was easy to see which cuts were and were not feasible. We just chose some obvious ones based on what we could see. We need to be able to do this in much higher dimensionality though. Next time we will introduce some of the most common cuts and how we generate them. Today we just wanted to lay a conceptual foundation for how Cutting Planes worked. I hope you enjoyed the post and I always welcome feedback! ","date":"2018-05-25","objectID":"/blog/2018/2018-05-24-discrete-optimization-with-cutting-planes/:3:0","tags":null,"title":"Discrete Optimization with Cutting Planes","uri":"/blog/2018/2018-05-24-discrete-optimization-with-cutting-planes/"},{"categories":null,"content":"If you have spent any time with me you will know that I am passionate about Optimization. Now, you may pass this off as a bit of geekiness on my part but the reason I care about Optimization is that it has profound implications for how we care for people. When I get a moment to describe Optimization to someone the way I start off is by saying, “Optimization is the mathematics of caring for people.” If you care about making the world a better place for humanity, then you should care about Optimization. The difficulty is that Optimization is often shrouded in mystery due to the math. My hope in this series is to clarify the mathematics of Optimization and make it more approachable. This will be the resource I wish I had when going through school. My desire is that by the end of the series you will have a firmer footing as you begin to scale the beautiful heights of Optimization and you come to enjoy it as art and as a tool for serving others. Note: I am not putting these posts out in a particular order. Right now I am just writing on what I am researching currently. When the series is complete I will recompile them into a more sensible order. The field is vast so I may jump around as I focus on different areas in my professional life. ","date":"2018-05-22","objectID":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/:0:0","tags":null,"title":"Discrete Optimization with Branch and Bound","uri":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/"},{"categories":null,"content":"Integer and Mixed Integer Programming Integer Programming (IP) and Mixed Integer Programming (MIP) both fall under the umbrella of Discrete Optimization since some or all of their decision variables must take on discrete values. They are amazing tools for mathematically modeling problems. By adding decision variables which must take on integer values we can describe complex logic in ways that Linear Programming (LP) cannot. The downside to this is that the problems are far more difficult to solve. Thankfully, there are a host of algorithms which can solve even incredibly large problems in a reasonable amount of time. The performance of these algorithms is highly dependent on the quality of the implementation though. One of the immediate challenges we face when trying to solve IP and MIP problems is that we cannot directly deal with the fact that some of the decision variables need to take on integral values. What most algorithms do is solve a Linear Programming relaxation of the original problem. The integral requirements of the decision variables is relaxed. This relaxed LP is solved and then constraints are added which force the integer decision variables to converge toward integral solutions. This means that we can really think of solving IP and MIP problems as recursively solving LPs where we add constraints at each recursive step. These first few posts will describe some of the algorithms in this family of solution techniques. ","date":"2018-05-22","objectID":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/:1:0","tags":null,"title":"Discrete Optimization with Branch and Bound","uri":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/"},{"categories":null,"content":"Branch and Bound Branch and Bound is the most straightforward method of searching for IP/MIP solutions. We solve an initial LP Relaxation of the original problem. If we find a solution where some of the integer decision variables have taken on nonintegral values we select one to branch one, much like binary search. We end up creating two new subproblems, referred to as nodes in the search tree, which are copies of the original problem but each one has a new constraint which forces the variable we are branching on to take on an integral value. For example, if we solved the LP Relaxation and the decision variable $x_1 = 1.5$ but it is supposed to be integral we can branch on $x_1$. We do this by creating two new instances of the original problem but in one of the subproblems, or node in the search tree, we add the constraint $x_1 \\leq1$ and in the other subproblem , or node, we add the constraint that $x_1 \\geq 2$. These new problems will no longer allow $x_1$ to take on the value of $1.5$. We now solve these new problems (nodes) and add new constraints to force other nonintegral integer variables toward integral values. As we successively solve these subproblems (nodes) we may come across a solution where the integrality requirements are met. This is called the incumbent solution. This solution represents the best feasible solution to our original problem we have found thus far. As we continue to search we may find a better solution which also meets the integrality requirements. This improved solution becomes the new incumbent solution. Eventually we will prune all of the branches and the remaining incumbent solution is the optimal solution for the original problem. Let’s walk through a more formal description of the algorithm. ","date":"2018-05-22","objectID":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/:2:0","tags":null,"title":"Discrete Optimization with Branch and Bound","uri":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/"},{"categories":null,"content":"Algorithm Description Let’s put together a rough outline of how the Brand and Bound algorithm works. For that we will need some parameters. $P =$ Our Initial Problem $P_{LP} =$ The LP Relaxation of our initial problem $z^\\ast =$ The objective function value for $P_{LP}$ $P^n =$ The $n^{th}$ subproblem of problem $P$ $P^n_{LP} =$ The LP relaxation of the $P^n$ problem $z^n =$ The objective function value for the solution to $P^n$ $z_{UB}^n =$ The best upper bound on the objective function for node $n$ $z_{LB}^n =$ The best lower bound on the objective function for node $n$ $Z_{UB} =$ The best upper bound for the objective function observed so far $Z_{LB} =$ The best lower bound for the objective function so far ","date":"2018-05-22","objectID":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/:3:0","tags":null,"title":"Discrete Optimization with Branch and Bound","uri":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/"},{"categories":null,"content":"Step 0: Initialize the Problem Create a LP Relaxation of the original problem, $P$, and solve it. We refer to this relaxed problem as $P_{LP}$ and it will be the initial node in our tree of nodes that we search. We attempt to solve $P_{LP}$ and check for one of the following conditions. Infeasible If the problem is infeasible at this stage we are done. If there is no solution to the $P_{LP}$ there is no solution to the original problem $P$. Unbounded If $P_{LP}$ has no bounds then we are done. The problem is unconstrained and therefore no optimal solution exists. Integer Solution All of the integeral requirements of $P$ have been met. We are done since the solution to $P_{LP}$ and $P$ is the same. Fractional Solution Some number of the integer variables have taken on nonintegral. We set $Z_{UB} = z^\\ast$ where $z^\\ast$ is the objective value for the initial problem $P_{LP}$ and $Z_{LB} = -\\infty$. $Z_{LB} = -\\infty$ is used to track what the best lower bound is for the original problem. We will use this value to prune nodes as we continue to search. Select one of the nonintegral decision variables and branch. To branch we create two new nodes from the parent problem $P$. We make a copy of $P$ but we add a constraint to the child nodes which will force the nonintegral variable toward and integral value. Let’s say that in $P$ $x_1$ is an integer decision variable. When we solve $P_{LP}$ we find that in the solution $x_1 = 1.5$. $x_1$ is supposed to take on an integral value so we decide to branch on this variable. We create a new child node $P^1$ which is the same as $P$ but with a new constraint $x_1 \\leq 1$. The other child node we create is $P^2$ and is the same as $P$ but with the opposing constraint that $x_{1} \\geq 2$. We add both of these child nodes to the Candidate List. We do not have an incumbent solution initially. ","date":"2018-05-22","objectID":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/:3:1","tags":null,"title":"Discrete Optimization with Branch and Bound","uri":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/"},{"categories":null,"content":"Step 1: Select a Node from the Candidate List Now assuming that a feasible solution to $P$ was not found when we solved $P_{LP}$ then we need to choose a node to solve from the Candidate List. After Step 0 there will only be 2 nodes to the Candidate List but as we continue to iterate we will add more nodes in the Candidate List. Which node we choose to solve is an algorithm design decision. We could choose the node with the best lower bound or continue down the children of the node we just solved. What some people do is do a depth first search to find a better $Z_{LB}$ to aid in pruning other nodes. The best choice is often problem dependent. Whatever strategy you employ, you will continue to evaluate nodes until the Candidate List has been emptied. If at any point we arrive at Step 1 and find there are no nodes in the Candidate List yet have an incumbent solution we terminate the algorithm and declare the incumbent solution to be the optimal. ","date":"2018-05-22","objectID":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/:3:2","tags":null,"title":"Discrete Optimization with Branch and Bound","uri":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/"},{"categories":null,"content":"Step 2: Solve the LP Relaxation of the $n^{th}$ node Based on whatever node selection rule we used in Step 1 we have chosen to solve the $P^n$ node. When you solve the LP Relaxation of the given node, $P_{LP}^n$, you will find $z^n$ which is the objective function value for $P_{LP}^n$. We then update $z_{UB}^n = z^n$. This represents the best possible objective function that could be achieved by the children of this node. ","date":"2018-05-22","objectID":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/:3:3","tags":null,"title":"Discrete Optimization with Branch and Bound","uri":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/"},{"categories":null,"content":"Step 3: Check for Infeasibility If while solving $P_{LP}^n$ you find the solution is infeasible you can “prune” this branch. None of the children of this node will be feasible either so there is no point in continuing to search down this branch. ","date":"2018-05-22","objectID":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/:3:4","tags":null,"title":"Discrete Optimization with Branch and Bound","uri":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/"},{"categories":null,"content":"Step 4: Check against $Z_{LB}$ If $z^n \\leq Z_{LB}$ we can prune this branch. If we are on the first iteration of the problem though $Z_{LB} = -\\infty$ so no node will be eliminated by this check. In Step 5 we update this value so eventually it will be an effective means of guiding our search down the tree. Now, why can we prune based on $z^n \\leq Z_{LB}$ you may ask. This is because we know that there is another branch which guarantees better solutions than the current branch. There is no point in us spending time searching down this branch because we already know we can do just as well if not better searching a somewhere else. If we have not pruned based on this test proceed to Step 5 or Step 6 depending on the condition of the solution. ","date":"2018-05-22","objectID":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/:3:5","tags":null,"title":"Discrete Optimization with Branch and Bound","uri":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/"},{"categories":null,"content":"Step 5: Check for Integrality If the solution to $P_{LP}^n$ meets the integrality requirements of $P$ we have found a feasible solution. We store this new incumbent solution and update the value of $Z_{LB} = z^n$ if $Z_{LB} \u003c z^n$. Again, $z^n$ is the value of the objective function for $P_{LP}^n$ which is the node we just solved. We prune this branch since it will not be possible to find a better solution. We then return to Step 1. ","date":"2018-05-22","objectID":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/:3:6","tags":null,"title":"Discrete Optimization with Branch and Bound","uri":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/"},{"categories":null,"content":"Step 6: Branch the Solution If we have reached this step there are still nonintegral values for the integer decision variables so we must branch the current node $P^n$. From here we select a nonintegral decision variable to branch on and create two child nodes and add them to the Candidate List. For example, let’s say that $x_2$ is an integer decision variable in problem $P$ but in the current solution, $P_{LP}^n$, we find $x_2=4.5$. We decide to branch on this decision variable since we need it to take on an integral value. We will create two new problems which are the same as our current problem $P^n$ but each has a new constraint forcing $x_2$ toward an integral value. One of the child nodes will have the constraint $x_2 \\leq 4$ and the other node will have the constraint $x_2 \\geq 5$. Both of these new nodes are added to the Candidate List. Loop back to Step 1 and continue. ","date":"2018-05-22","objectID":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/:3:7","tags":null,"title":"Discrete Optimization with Branch and Bound","uri":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/"},{"categories":null,"content":"Wrapping Up In this post I have given a quick overview of the Branch and Bound algorithm for solving IP and MIP problems. While there may be a lot of terminology the whole thing boils down to what is essentially a binary search with some rules for eliminating branches. Branch and Bound is a foundational technique for solving this class of problems. More advanced methods typically take the framework of Branch and Bound and add additional steps for speeding up convergence and strengthening bounds. In my next post I hope to provide some worked examples to illustrate how this technique works. ","date":"2018-05-22","objectID":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/:4:0","tags":null,"title":"Discrete Optimization with Branch and Bound","uri":"/blog/2018/2018-05-22-discrete-optimization-with-branch-and-bound/"},{"categories":null,"content":"One of the reasons that I love F# is that is makes it incredibly easy to model domains. By creating a Domain Model which represents the business domain it becomes relatively easy to create workflows and algorithms which streamline business processes. In this post I show how to create types for a domain which are summable, a feature I use frequently in my work. ","date":"2018-05-13","objectID":"/blog/2018/2018-05-13-creating-summable-domain-types/:0:0","tags":null,"title":"Creating Summable Domain Types","uri":"/blog/2018/2018-05-13-creating-summable-domain-types/"},{"categories":null,"content":"The Value of Restricting Values When I have to create a new Domain Model one of the first things that I do is define a single case Discriminated Union of decimal for the basic building blocks that I am going to work with (Costs, Items, Sales Rates, Days of Inventory, etc.). For example, when I am creating an algorithm to evaluate the financial viability of a product on marketplaces I have to calculate costs, I therefore create a Cost type. In my domain, a Cost is never negative therefore I can create a constructor which will enforce this behavior. type Cost = Cost of decimal // Define a single case DU 'Cost' for decimal module Cost = let create c = // Function for creating 'Cost' values if c \u003c= 0M then // Check that the value is greater than 0.0M None // Return None if outside bounds else Some(Cost c) // Return input wrapped in a 'Cost' value The beautiful thing about this is that when I am working with a Cost type I never have to worry about it being negative. This is a powerful thing when it comes to composing algorithms because I have eliminated a whole host of possible values that I would need to handle. It is amazing how easy it is for a negative numbers to sneak in and cause havoc. I force myself to deal with this bad data at the boundary of the domain instead of inside the algorithm performing the analysis. ","date":"2018-05-13","objectID":"/blog/2018/2018-05-13-creating-summable-domain-types/:1:0","tags":null,"title":"Creating Summable Domain Types","uri":"/blog/2018/2018-05-13-creating-summable-domain-types/"},{"categories":null,"content":"The Downside: Where Did Addition Go? There is a downside to doing this though, basic math operations will not work. At this point if I try to add two different Cost values I will get a compiler error. let totalCost = cost1 + cost2 // Error: The type 'Cost' does not support the '+' operator Fortunately this is easy to overcome. All we need to do is implement the + operator for the type. We do this by adding a static member to our type alias. We add the keyword with to the end of our previous type alias definition and provide the + static member. // Updated definition of 'Cost' type Cost = Cost of decimal with static member (+) (Cost c1, Cost c2) = Cost (c1 + c2) The arguments for the + function may look a little odd so let me explain. By declaring the arguments of the function as (Cost c1, Cost c2) I am telling the compiler that I expect a Cost type as the input and I want you to unpack the value inside of Cost and put it in the c1 and c2 values respectively. This allows me to work with the decimal values inside of the Cost type. The function itself adds the two values together and then wraps the result in a Cost. Now when we go to add two Cost values we no longer get an error. The beauty of this is that I have maintained control over the values that Cost can take on. I declared a create function which insures positive values. I only allow addition of Cost types which means that a Cost will only ever be positive. Some people may brush this off as trivial but as someone who has seen the damage that can happen from values going outside of the expected range, this extra work for reliability and peace of mind is worth it. For me, it is more efficient to ensure values cannot go outside their allowed bounds through controlling construction and operator definitions than to have value checks all over the place. let totalCost = cost1 + cost2 // Result: val totalCost : Cost = Cost 15.0M ","date":"2018-05-13","objectID":"/blog/2018/2018-05-13-creating-summable-domain-types/:2:0","tags":null,"title":"Creating Summable Domain Types","uri":"/blog/2018/2018-05-13-creating-summable-domain-types/"},{"categories":null,"content":"Enabling Summation Well, that is great and all but what happens when we have a List of Cost values and we want to sum them. What happens then? let sumCosts = [cost1; cost2] |\u003e List.sum // Error: The type 'Cost' does not support the operator 'get_Zero' Now when I first came across this I was confused. I had no idea what this get_Zero operator meant. After digging around for a while I was able to find some examples of what it was referring to. The sum function wants a starting point for the summation and it gets that by calling the Zero function on the type. I don’t know why the compiler is saying does not support the operator 'get_Zero' instead of saying the type does not have a function named 'Zero'. Again, F# makes this easy to implement. // Summable 'Cost' type type Cost = Cost of decimal with static member (+) (Cost c1, Cost c2) = Cost (c1 + c2) static member Zero = Cost 0.0M Now when we try to sum a list of Cost values we get the expected result. let sumCosts = [cost1; cost2] |\u003e List.sum // Result: val sumCosts : Cost = Cost 15.0M ","date":"2018-05-13","objectID":"/blog/2018/2018-05-13-creating-summable-domain-types/:3:0","tags":null,"title":"Creating Summable Domain Types","uri":"/blog/2018/2018-05-13-creating-summable-domain-types/"},{"categories":null,"content":"Freedom Through Constraints The more I dive into Domain Driven Design with F#, the more I love it. By ensuring values comply with expectations at the boundary of the domain, I am freed to reason about my algorithms without worrying about data going awry inside the domain. While it takes a few more keystrokes to define operations on these domain types, I hope that I showed you that it takes little effort in F# and can lead to more reliable and robust code. Keep calm and curry on! ","date":"2018-05-13","objectID":"/blog/2018/2018-05-13-creating-summable-domain-types/:4:0","tags":null,"title":"Creating Summable Domain Types","uri":"/blog/2018/2018-05-13-creating-summable-domain-types/"},{"categories":null,"content":"I recently attended a training event hosted by Gurobi. For those who don’t know, Gurobi produces one of the best mathematical solvers in the industry. It was a great event and we were able to spend ample time with engineers and experts in the field. Using a mathematical solver requires the ability to formulate models and at this time one of the easiest languages for doing that is Python. Python is a great language for many use cases. One is providing a quick and easy means of formulating models that can then be fed to a solver. I was able to spend some time with one of the engineers who implemented Gurobi’s Python library, gurobipy. He pointed to the formulation of the netflow problem as an example of how terse and concise Python could be for modeling. Since I love F#, I naturally wanted to see if I could accomplish the same thing using F#. What started as a silly proof of concept is slowly turning into a more full fledged library for wrapping the Gurobi .NET library in a functional F# wrapper. Below I give an example of how the power of functions in F# allows us to nearly duplicate the functionality of Python. The library I am working on can be found here. Note I am not saying one language is better than another. I merely like to challenge myself with formulating ideas in different languages. It forces me to translate across paradigms which I find a useful exercise for the mind. ","date":"2018-05-04","objectID":"/blog/2018/2018-05-04-fsharp-for-optimization-modeling/:0:0","tags":null,"title":"F# for Optimization Modeling","uri":"/blog/2018/2018-05-04-fsharp-for-optimization-modeling/"},{"categories":null,"content":"Netflow Example The following shows an example of a network flow problem provided by Gurobi and modeled in Python. The full formulation can be found here. In this example I am just comparing and contrasting the Python and F# constraint formulation methods. Disclaimer: All Python code is copyrighted by Gurobi Optimization, LLC ","date":"2018-05-04","objectID":"/blog/2018/2018-05-04-fsharp-for-optimization-modeling/:1:0","tags":null,"title":"F# for Optimization Modeling","uri":"/blog/2018/2018-05-04-fsharp-for-optimization-modeling/"},{"categories":null,"content":"Creating a Model Python In Python the creation of the model and decision variables is quite straightforward. # Copyright 2018, Gurobi Optimization, LLC # Create optimization model m = Model('netflow') # Create variables flow = m.addVars(commodities, arcs, obj=cost, name=\"flow\") With Gurobi.Fsharp In F# we have a similar syntax but instead of flow being a Dictionary of decision variables indexed by tuples, we produce a Map\u003cstring list, GRBDecVar\u003e which is essentially the same for our purposes. I am using string list as the index instead of tuples because we need an indexer which has dynamic length. I could do it with tuples but it would be less straightforward. // Create a new instance of the Gurobi Environment object // to host models // create: GRBEnv let env = Environment.create // Create a new model with the environment variable // create: env:GRBEnv -\u003e name:string -\u003e GRBModel let m = Model.create env \"netflow\" // Create a Map of decision variables for the model // addVarsForMap: model:GRBModel -\u003e lowerBound:float -\u003e upperBound:float -\u003e varType:string -\u003e indexMap:Map\u003c'a,float\u003e let flow = Model.addVarsForMap m 0.0 INF CONTINUOUS costs Instead of using the methods on the object, functions have been provided which operate on the values that are passed in. This is more idiomatic for F#. The Model module in the library hosts all of the functions for working with objects of type Model. The Model.adddVarsForMap function takes a Map\u003cstring list, float\u003e and produces a Map\u003cstring list, GRBDecVar\u003e for the modeler to work with. This is similar to how the Python tuples are working in the gurobipy library. Instead of indexing into a Python dictionary with tuples, F# uses a string list as the index. ","date":"2018-05-04","objectID":"/blog/2018/2018-05-04-fsharp-for-optimization-modeling/:1:1","tags":null,"title":"F# for Optimization Modeling","uri":"/blog/2018/2018-05-04-fsharp-for-optimization-modeling/"},{"categories":null,"content":"Adding Constraints Python The gurobipy library offers a succinct way of expressing a whole set of constraints by using generators. There is additional magic going on under the hood though that may not be obvious at first. The following method generates a set of constraints for each element in arcs but also creates a meaningful constraint name. The prefix for the constraint name is the last argument of the method (\"capacity\" in this instance). # Arc capacity constraints capacityConstraints = m.addConstrs( (flow.sum('*',i,j) \u003c= capacity[i,j] for i,j in arcs), \"capacity\") There is also special sauce occuring in the flow.sum('*',i,j) syntax. flow is a dictionary which is indexed by a 3 element tuple. What this sum() method is doing is summing across all elements in the dictionary which fit the pattern. The * symbol is a wildcard and will match against any element. This is a powerful way to sum across dimensions of the optimization model. With Gurobi.Fsharp In F# we can do something similar but instead of having a generator we pass in a lambda to create the constraints. The sinature of this function for creating the constraint set is: model-\u003estring-\u003estring list-\u003e(Map\u003cstring list, Gurobi.GRBConstr) // addConstrs: model:GRBModel -\u003e setName:string -\u003e setIndexes: string list list -\u003e constraintFunc:(string list -\u003e ConstraintTuple) -\u003e Map\u003cstring list, GRBConstr\u003e let capacityConstraints = Model.addConstrs m \"capacity\" arcs (fun [i; j] -\u003e (sum flow [\"*\"; i; j] \u003c== capacity.[[i; j]])) The function Model.addConstrs takes a model object as its first argument (m in this case), the prefix for what the constraints are going to be named (\"capacity\" in this case), and the set of indices the constraints will be created over, arcs in this case. The key point is that the types of the indices must match the input type of the lambda. The addConstrs function will iterate through each of the indices in the set, create a constraint from the lambda that was passed, and name the constraint appropriatly. If the first element of the arcs set was [\"Detroit\"; \"Boston\"] then the name of the first constraint would be capacity_Detroit_Boston. This helps the modeler by maintaining a consistent naming scheme for the constraints in the model. ","date":"2018-05-04","objectID":"/blog/2018/2018-05-04-fsharp-for-optimization-modeling/:1:2","tags":null,"title":"F# for Optimization Modeling","uri":"/blog/2018/2018-05-04-fsharp-for-optimization-modeling/"},{"categories":null,"content":"I am a huge fan of Domain Driven Design and I have been trying to apply it more and more. I ran into a problem last week that kept beating me over the head though. I kept using a bottom up approach and kept coming up with terrible solutions. Finally, I took a more outside to in approach which cleaned up the solution. I credit Mark Seemann for the idea to work from the outside in. I am wanting to show some of the difficulties you can run into using a bottom up approach so that others don’t make the same mistakes that I did. Hopefully this little exercise helps provide others some guidance on how to get unstuck when attempting Domain Driven Design. ","date":"2018-01-27","objectID":"/blog/2018/2018-01-27-domains-run-amok/:0:0","tags":null,"title":"Domains Run Amok","uri":"/blog/2018/2018-01-27-domains-run-amok/"},{"categories":null,"content":"Our Refactoring Problem I have a project where we are rebuilding how we calculate the replenishment logic for our Supply Chain. Replenishment is the process of ordering product from Vendors for your Warehouses so that we can fill customer orders. I work for an e-commerce company so Replenishment is at the heart of what we do. The current solution is a monolith application which is all fed from an Azure SQL instance. It is comprised of a large set of batch process that run in order and populate tables in the database. This mess was inherited from an old system and has been warped beyond comprehension at this point. It is so fragile we don’t dare touch it. The plan is to decompose the monolith into separate services which communicate via messages. To do this though, we need to create those separate services. At the heart of one of those services is the analysis of Time Series data. This is my attempt to create a tiny little domain for modeling this analysis and the mistakes I made along the way. ","date":"2018-01-27","objectID":"/blog/2018/2018-01-27-domains-run-amok/:1:0","tags":null,"title":"Domains Run Amok","uri":"/blog/2018/2018-01-27-domains-run-amok/"},{"categories":null,"content":"Modeling TimeSeries Take 1: From the Bottom Up All of our Replenishment logic is built on analyzing Time Series data. This data can be thought of as a array of tuples where one value is the timestamp and the other is the observed value, DateTimeOffset * 'a. What I set out to do is create a domain model that allows us to analyze these Time Series in a robust and performant way. My initial thought was, “I know that my data will always be Decimal or String so I can think of an ObservedValue in my Time Series as a Discriminated Union and an Observation is a record with a DateTimeOffset and an ObservedValue. A TimeSeries is just an array of the type Observation. When I am done with an analysis the result will be either decimal or string so I’ll define an AnalysisResult type to contain the result.” type ObservedValue = | Decimal of decimal | String of string type Observation = { DateTime : DateTimeOffset Value : ObservedValue } type TimeSeries = array\u003cObservation\u003e type AnalysisResult = | Decimal of decimal | String of string This doesn’t seem bad so far. Now I need to add some basic functions for analyzing my TimeSeries. Some simple and obvious ones are mean, first, and last. There are actually many functions I will need but these will suffice to make my point. I now try to write these simple functions for my TimeSeries type. module TimeSeries = let private create observedType t : TimeSeries = t |\u003e Seq.map (fun (t, v) -\u003e {DateTime = t; Value = observedType v}) |\u003e Seq.toArray let fromDecimal s : TimeSeries = create ObservedValue.Decimal s let fromString s : TimeSeries = create ObservedValue.String s let first (ts : TimeSeries) = ts.[0].Value let last (ts : TimeSeries) = ts.[-1].Value let mean (ts : TimeSeries) = ts |\u003e Array.averageBy (fun x -\u003e x.Value) // Error: The type ObservedValue does not support the operator '+' I have encountered my first problem with this approach. I want to be able to take the mean of my TimeSeries but the ObservedValue type does not support the + operator. I think, “No problem, I’ll just add the + operator.” I then look at the type again and realize I may be doing something wrong. Adding a decimal to a decimal makes sense and I also understand adding string to string but this is going to require me to have a + defined for decimal to string and string to decimal. That does not make any sense. ","date":"2018-01-27","objectID":"/blog/2018/2018-01-27-domains-run-amok/:2:0","tags":null,"title":"Domains Run Amok","uri":"/blog/2018/2018-01-27-domains-run-amok/"},{"categories":null,"content":"Modeling TimeSeries Take 2: Homogenous Values My problem is that I am allowing a single TimeSeries to be heterogenous, containing both decimal and string values. Really a single TimeSeries needs to be homogeneous, containing only decimal or only string. Okay, no problem! I’ll reformulate the domain to have the TimeSeries be a Discriminated Union instead of the ObservedValue. open System type Observation\u003c'a\u003e = { DateTime : DateTimeOffset Value : 'a } type TimeSeries = | Decimal of array\u003cObservation\u003cdecimal\u003e\u003e | String of array\u003cObservation\u003cstring\u003e\u003e type AnalysisResult = | Decimal of decimal | String of string Now let’s try to implement our analysis functions again. Don’t judge me for what you see next. Once I wrote it, I felt a little ill. I’ll go into why after the code. module TimeSeries = let private create observedType t : TimeSeries = t |\u003e Seq.map (fun (t, v) -\u003e {DateTime = t; Value = v}) |\u003e Seq.toArray |\u003e observedType let fromDecimal t = create TimeSeries.Decimal t let fromString t = create TimeSeries.String t let private map df sf ts = match ts with | TimeSeries.Decimal t -\u003e df t | TimeSeries.String t -\u003e sf t let first (ts : TimeSeries) = let f = fun (t : array\u003cObservation\u003c'a\u003e\u003e) -\u003e t.[0].Value map (f \u003e\u003e AnalysisResult.Decimal) (f \u003e\u003e AnalysisResult.String) ts let last (ts : TimeSeries) = let f = fun (t : array\u003cObservation\u003c'a\u003e\u003e) -\u003e t.[-1].Value map (f \u003e\u003e AnalysisResult.Decimal) (f \u003e\u003e AnalysisResult.String) ts let mean (ts : TimeSeries) = let df = fun (t : array\u003cObservation\u003cdecimal\u003e\u003e) -\u003e t |\u003e Array.averageBy (fun x -\u003e x.Value) |\u003e AnalysisResult.Decimal let sf = fun (t : array\u003cObservation\u003cstring\u003e\u003e) -\u003e \"\" |\u003e AnalysisResult.String map df sf ts I will readily admit this is clunky. Let me explain the thought process. I know that the TimeSeries type is a Discriminated Union and therefore I should have a map like function for easily applying the correct function, depending on which value TimeSeries takes on. In many cases I would use the exact same logic (Ex: first and last) so I just defined a generic function and used that for both arguments of the map function. When I get to the mean function I run into another problem. It does not make sense to take the mean of a set of string observations but the code allows it. In this code I am returning an empty string but that is not in line with the heart of what I am going for. If something does not make sense, I don’t want to allow it. I want invalid states to be unrepresentable in the code. I don’t want myself or someone else to even be able to call mean with a TimeSeries containing string values. It’s at this point I start to feel really dumb. How can this be so hard? Here is what I am wanting to accomplish: Model a TimeSeries made up of either decimal or string Reuse function logic wherever I can (DRY principle) Prevent unrepresentable states ","date":"2018-01-27","objectID":"/blog/2018/2018-01-27-domains-run-amok/:3:0","tags":null,"title":"Domains Run Amok","uri":"/blog/2018/2018-01-27-domains-run-amok/"},{"categories":null,"content":"Modeling TimeSeries Take 3: Generic TimeSeries I would rather not admit how long I was stumped at this point. It felt like I was missing something glaringly obvious. I mulled on this problem for awhile until the next thought came to me, “What is really going on is that I have two special cases of TimeSeries\u003c'a\u003e here. I have a TimeSeries\u003cdecimal\u003e and a TimeSeries\u003cstring\u003e. Why not have a full set of functions for TimeSeries\u003c'a\u003e and then have two different types for the TimeSeries\u003cdecimal\u003e case and the TimeSeries\u003cstring\u003e case which only have a subset of the functions available?” Here is what I came up with. open System type Observation\u003c'a\u003e = { DateTime : DateTimeOffset Value : 'a } type TimeSeries\u003c'a\u003e = array\u003cObservation\u003c'a\u003e\u003e type DecimalSeries = TimeSeries\u003cdecimal\u003e type StringSeries = TimeSeries\u003cstring\u003e type AnalysisResult = | Decimal of decimal | String of string module TimeSeries = let private create t : TimeSeries\u003c'a\u003e= t |\u003e Seq.map (fun (t, v) -\u003e {DateTime = t; Value = v} ) |\u003e Seq.toArray let private first (ts : TimeSeries\u003c'a\u003e) = ts.[0].Value let private last (ts : TimeSeries\u003c'a\u003e) = ts.[-1].Value let inline private mean (ts : TimeSeries\u003c'a\u003e) = ts |\u003e Array.averageBy (fun x -\u003e x.Value) module DecimalSeries = let create t : DecimalSeries = create t let first (ds : DecimalSeries) = first ds |\u003e AnalysisResult.Decimal let last (ds : DecimalSeries) = last ds |\u003e AnalysisResult.Decimal let mean (ds : DecimalSeries) = mean ds |\u003e AnalysisResult.Decimal module StringSeries = let create t : StringSeries = create t let first (ds : StringSeries) = first ds |\u003e AnalysisResult.String let last (ds : StringSeries) = last ds |\u003e AnalysisResult.String One thing to note, I had to add the keyword inline to the mean function in the TimeSeries module. This makes the compiler figure out the types at the point the function is used. Now I am still not really proud of this code yet but it is accomplishing most of my goals. I am getting code reuse while being able to control which functions can be used by which type of TimeSeries. Since I only define a create function for the DecimalSeries and StringSeries types, I don’t have to fear someone creating a random TimeSeries\u003c'a\u003e if they follow the convention of using the create function. The functions for TimeSeries are also private and can only be called from the sub-modules DecimalSeries and StringSeries. ","date":"2018-01-27","objectID":"/blog/2018/2018-01-27-domains-run-amok/:4:0","tags":null,"title":"Domains Run Amok","uri":"/blog/2018/2018-01-27-domains-run-amok/"},{"categories":null,"content":"Conclusion I hope my failures prove useful and an encouragement to others wandering through the process of learning Domain Driven Design. This was just one small problem that made me feel rather silly as I wrestled with it. Maybe I will come up with a more elegant solution but as of now, I like the code reuse and guarantees this is providing me. If you have a better solution, please message me on Twitter (@McCrews). When you get stuck coding, remember most of progress feels like wandering down dark halls until you come to the light. Keep calm and curry on! ","date":"2018-01-27","objectID":"/blog/2018/2018-01-27-domains-run-amok/:5:0","tags":null,"title":"Domains Run Amok","uri":"/blog/2018/2018-01-27-domains-run-amok/"},{"categories":null,"content":"One of the things that most attracted me to F# is the ability to accurately model your domain. What first turned me on to this was a talk by Scott Wlaschin on Functional programming design patterns. Scott has a more focused talk on Domain Modeling Made Functional that he did a few years later and a book with the same title. This whole concept was blowing my mind. The idea of modeling your domain such that illegal states are unrepresentable sounds immensely satisfying to me. This new way of looking at the world has been slowly transforming all of my code. Everywhere I look now I am asking, “Is it possible for this state to be illegal? What can I do to ensure I am covering all scenarios?” With this new focus I quickly came across an operator in F# that lies, the division operator. ","date":"2018-01-14","objectID":"/blog/2018/2018-01-14-the-divide-operator-is-a-lie/:0:0","tags":null,"title":"The Divide Operator is a Lie","uri":"/blog/2018/2018-01-14-the-divide-operator-is-a-lie/"},{"categories":null,"content":"The Divide Lie If you hover over the / operator in Visual Studio you will get the following function signature val(/): x:'T1 -\u003e y:'T2 -\u003e 'T3 (requires member (/)) There is nothing surprising here. The / operator is expecting two values and will produce a third. Now let’s look at what the compiler says is supposed to happen when we divide two decimals. If I input the following lines into a fsx script in Visual Studio I will get the following types from the compiler. let a = 10M // val a : decimal let b = 5M // val b : decimal let c = a / b // val c : decimal This is where my problem is. The compiler says that taking two decimal values and dividing them will produce a third decimal value. This is not always the case though. If b = 0M then this will throw an exception. This runs counter to the idea of making illegal states unrepresentable. We would rather that the operator returned 'T option which would force us to deal with both scenarios. ","date":"2018-01-14","objectID":"/blog/2018/2018-01-14-the-divide-operator-is-a-lie/:1:0","tags":null,"title":"The Divide Operator is a Lie","uri":"/blog/2018/2018-01-14-the-divide-operator-is-a-lie/"},{"categories":null,"content":"Defining a new Operator Fortunately for us, it is easy to add operators to F# but there are a couple of gotchas I will cover here. The F# Language Reference has a great page describing the rules around Operator Overloading. The key thing to know is that there are a limited set of characters that are permitted: !, %, \u0026, *, +, -, ., /, \u003c, =, \u003e, ?, @, ^, |, and ~. ~ is a special character to be used when making a unary operator. In this case, I need a binary operator so I will avoid using it. I want to create a new divide operator that will check if the divisor is 0. If the divisor is equivalent to 0, I want the operator to return None. Since I want this to be intuitive when looking at the operator I will combine the divide symbol, /, with the bang symbol, !, to make my new operator /!. The reason I am using the ! symbol is because it often indicates a warning which is what I am wanting to communicate to the developer. This means my function signature needs to look like this: val(/!): x:'T1 -\u003e y:'T2 -\u003e 'T3 option (requires member (/)) My first attempt looked like the following: let (/!) a b = match b \u003c\u003e 0 with | true -\u003e a / b |\u003e Some | false -\u003e None When I look at the function signature of my operator though I see the following: val(/!): x:int -\u003e y:int -\u003e int option This is no good. This will only work with inputs of int and I am wanting something that is generic. The problem is in two places. The first, and more obvious one, is that I am comparing the value of b with the value of 0 which is an int. The F# compiler is therefore restricting the input types to be int. I know this because I can change the value b is compared to and change the function signature. For example if I change 0 to 0M, the type of a and b is restricted to decimal. If I change 0 to 0., making it a float, the type of a and b is restricted to float. ","date":"2018-01-14","objectID":"/blog/2018/2018-01-14-the-divide-operator-is-a-lie/:2:0","tags":null,"title":"The Divide Operator is a Lie","uri":"/blog/2018/2018-01-14-the-divide-operator-is-a-lie/"},{"categories":null,"content":"Making the Operator Generic Fortunately, F# has a fix for this, it is called GenericZero. GenericZero is a type function which returns the 0 equivalent for any numeric type or type with a static member called Zero. It is contained in the F# Language Primitives, Microsoft.FSharp.Core.LanguagePrimitives. More information can be found in the language reference entry on GenericZero. The other problem with this function is that it needs to be an inline function. The inline keyword in F# tells the compiler to figure out the types for the function at the place of usage instead of restricting the types. Here is a simple example of an add function without the inline keyword. // non-inlined function let add a b = a + b // val add : a:int -\u003e b:int -\u003e int You would think that the add function would be generic but the F# compiler will restrict this to int because that is the best match it can deduce from the context. Now, if we use the add function with float values it will change the function signature but it will still be restricted to only a single type. Here I show using the add function with float values before trying to use it with int values. F# updates the function signature to using float but now throws an error when we try to use int values. // non-inlined function let add a b = a + b // val add : a:float -\u003e b:float -\u003e float let r = add 1. 2. // r : float let r2 = add 1 2 // compiler error The inline keyword can be added to the beginning of the function to have the compiler deduce the types at the point the function is used. let inline add a b = a + b // val add : a:'a -\u003e b:'b -\u003e 'c (requires member(+)) let r = add 1. 2. // r : float let r2 = add 1 2 // r2 : int We now have all of the ingredients we need to update our new operator /! so that it will work with generic types. open Microsoft.FSharp.Core.LanguagePrimitives let inline (/!) a b = match b \u003c\u003e GenericZero with | true -\u003e a / b |\u003e Some | false -\u003e None // val (/!) : a:'a -\u003e b:'b -\u003e 'c option (requires member (/) and member get_Zero and equality) This is exactly what we were looking for in the beginning. Now when we use our new operator we are forced to deal with a situation where the divisor is possibly 0. This solution for dealing with a possible 0 divisor may not be for everyone. Perhaps having to deal with the None scenario is too cumbersome for you. I find that I like having this additional safety in place because it forces me to write more robust code. ","date":"2018-01-14","objectID":"/blog/2018/2018-01-14-the-divide-operator-is-a-lie/:3:0","tags":null,"title":"The Divide Operator is a Lie","uri":"/blog/2018/2018-01-14-the-divide-operator-is-a-lie/"},{"categories":null,"content":"The title for this may be a little over the top but it is not far from the truth. I am wanting to show how Units of Measure in F# can protect against some of the most insidious types of errors, mismatched units. One of the most difficult parts of putting together algorithms has been making sure that the Units of Measure for numbers match. For example, you should not be able to add lbs and cm, it doesn’t make sense. In most programming languages though, a number is just a number. You may be working with a strict language which requires you to convert from int to float before multiplying, but many will do this implicitly. When I am writing in R, Python, or C# I don’t have any kind of Units of Measure checking. This has led to a lot of frustrating debugging in the past where I missed some simple multiplication or division in my code. These types of bugs can be really nefarious because you can often get numbers which seem sensible at first but then blow up when outlier data is introduced. ","date":"2018-01-06","objectID":"/blog/2018/2018-01-06-my-most-expensive-error/:0:0","tags":null,"title":"My Most Expensive error","uri":"/blog/2018/2018-01-06-my-most-expensive-error/"},{"categories":null,"content":"The Initial Error I was tasked with writing a simple fee calculation for our products on Amazon. We need to know the impact of the new fees on our costing before they go into effect. This is such a simple thing. On my first pass I decided to just throw something together in Python. When I did this, I made a very expensive mistake. Can you see it? def calculate_item_fba_fee(cost_config, item): weight_tiers = cost_config[item['item_size']]['WeightTiers'] weight_tier = [tier for tier in weight_tiers if (tier['MinWeight'] \u003c item['item_weight']) \u0026 (tier['MaxWeight'] \u003e= item['item_weight'])][0] fee = weight_tier['BaseFee'] + max(0.0, item['item_weight'] - weight_tier['weight_fee_lb_cutoff']) return fee This function is taking a Dictionary, cost_config, which holds some configuration values and a row of a Pandas DataFrame, called item. The first line of the function looks up the weight tiers which may apply to the item. It then searches through the tiers to find the weight_tier which matches the weight of the item. It then calculates the fee, which is where the error is. The fee value is composed of a base_fee, in US Dollars (USD), and a USD/lb fee if the weight is above the weight_fee_lb_cutoff value. In this case the weight_fee_lb_cutoff value is 2.0 lbs. So, for every lb over 2.0, the item is charged an additional fee per lb. You may see the error now, I never multiply the overage weight by the [USD/lb], (US Dollars / pound), fee rate. If you look at the units of the fee calculation I am adding the base_fee, which is in [USD], to [lbs]. That does not make any sense. You can’t add different types of units, but most languages will let you do this all day. This was insidious because for most of our items, the fee was right. Only in cases where the item was over 2.0 [lbs] did we get an incorrect fee. I’ll be honest, I didn’t actually catch this bug. I put this code in production but I never felt really good about it. I couldn’t explain it but there was disquiet in my soul. I was already starting to rewrite parts of our system in F# so I decided that I would rewrite this little piece while it was fresh in my mind. ","date":"2018-01-06","objectID":"/blog/2018/2018-01-06-my-most-expensive-error/:1:0","tags":null,"title":"My Most Expensive error","uri":"/blog/2018/2018-01-06-my-most-expensive-error/"},{"categories":null,"content":"F# Units of Measure Save the Day For the last several years I have been moving toward more and more strict programming languages. When I heard that F# allows you to put Units of Measure on your numbers, I fell in love. I have longed for such a feature. So many errors can be eliminated when dealing with numbers if you can track and enforce units alignment in numbers. Because my soul never settled with my initial Python solution, I decided to rewrite the fee calculation. When I started I immediately declared the Units of Measure that I would need: // Units of Measure Types [\u003cMeasure\u003e] type USD (* US Dollar *) [\u003cMeasure\u003e] type lb (* Imperial pound *) I then wrote my fee calculation with the Units of Measure on the numbers to ensure everything matched. I then immediately saw the mistake. You will notice in this new function that I do multiply by the feeRate. // New fee function let calculateWeightFee (baseFee : decimal\u003cUSD\u003e) (weightFeeCutoff : decimal\u003clb\u003e) (feeRate : decimal\u003cUSD/lb\u003e) (weight : decimal\u003clb\u003e) = baseFee + (max 0M\u003clb\u003e (weight - weightFeeCutoff)) * feeRate I felt pretty stupid after such an obvious mistake. Fortunately, the previous version of the code was only in production for a couple of days. Had this gone on for longer, we could have missed huge volumes of opportunity because products would have look too expensive due to the new fee. Now granted, better unit testing would have caught this. Also, this post is not meant to disparage Python, or any other language, in any way. Rather, I am highlighting that F# is eliminating an entire class of errors for me and making me more productive. I much prefer the compiler barking at me about my units not matching than me spending hours or days hunting for where I missed a multiplication or a division. It feels great knowing that my units line up and that if I miss a small detail like this, the compiler will gently guide me back to sanity. Check out this wonderful post by Scott Wlaschin for a more detailed discussion on what can be done with F# and Units of Measure. ","date":"2018-01-06","objectID":"/blog/2018/2018-01-06-my-most-expensive-error/:2:0","tags":null,"title":"My Most Expensive error","uri":"/blog/2018/2018-01-06-my-most-expensive-error/"},{"categories":null,"content":"At work I have been tasked with extracting the product description information for several of our products to be used as an import for an external system. Normally I would just write a query for the database to get all of this information but in this case I do not have access to the database directly. Even if I did have it, I am not familiar with the schema so I would rather not have to spend the effort digging into if I do not have to. I have been putting off this project since it was not high priority but recently I came across an excellent talk by Evelina Gabasova at NDC Oslo where she showed the use of TypeProviders to connect to IMDB to extract data on cast members of Star Wars. As I watched this I had a eureka moment, “Why not just use F# to pull the data directly from the website instead of dealing with the SQL Schema?” Initial Attempt with HtmlProvider Now, this may seem a little silly but for my case it has some advantages. I do not have to bother with getting permissions for the database running the company e-commerce website and it allows me to use some F#. I quickly fire up a new F# project in VS Code and stub out the following: #I \"./packages\" #r \"FSharp.Data/lib/net40/FSharp.Data.dll\" open FSharp.Data type Product = HtmlProvider\u003c\"https://www.b-glowing.com/skincare/cleansers/paulas-choice-calm-redness-relief-cleanser-for-oily-skin/\"\u003e let test = Product.GetSample() This is when I run into a problem. In the talk that Evelina Gabasova gave the data on IMDB was in a nice table. This meant that the TypeProvider could detect it automatically and provide it as a nice property of the test object in the above example. My problem is that the data I need is in the Description area of the page, specifically the \u003cspan\u003e with the attribute itemprop=\"description\". I am trying to turn this information: \u003cspan itemprop=\"description\"\u003e \u003cp\u003e \u003cstrong\u003eWHAT IT IS\u0026nbsp;\u003c/strong\u003e \u003cbr\u003e A lightweight silky gel cleanser for Normal to Oily skin types that gently removes makeup and soothes red, sensitive skin. \u003c/p\u003e \u003cp\u003e \u003cstrong\u003eBENEFITS FOR YOU\u003c/strong\u003e \u003cbr\u003e• Safe for even the most sensitive skin. \u003cbr\u003e• Removes excess oil and makeup. \u003cbr\u003e• Soothes and refreshes senstive, irritated\u0026nbsp;skin. \u003c/p\u003e \u003cp\u003e \u003cstrong\u003eYOU’LL EXPERIENCE\u003c/strong\u003e \u003cbr\u003eThis lightweight gel texture lathers beautifully to remove excess oils, impurities and makeup. Skin is left calm, clean and soft. \u003c/p\u003e \u003cp\u003e \u003cstrong\u003eWHY IT’S GLOWING\u003c/strong\u003e \u003cbr\u003e The calming cleanser works wonders for those of us who\u0026nbsp;experience sensitivity and redness without drying or stripping skin. The formula increases our skins natural barrier so overtime skin is less sensitive and red on its own.\u0026nbsp; \u003c/p\u003e \u003c/span\u003e into something like this: Tag Text WHAT IT IS A lightweight silky gel cleanser for Normal to Oily skin types that gently removes makeup and soothes red, sensitive skin. BENEFITS FOR YOU Safe for even the most sensitive skin. Removes excess oil and makeup. Soothes and refreshes sensitive, irritated skin. This means that I need a different approach. Thankfully, F# delivered. Using HTML Parser If the HtmlProvider does not give you what you need for HTML parsing then FSharp.Data also has a handy HTML Parser which includes some excellent documentation and examples. I put together a new script to extract the data from the website. #I \"./packages\" #r \"FSharp.Data/lib/net40/FSharp.Data.dll\" open FSharp.Data let productHtml = HtmlDocument.Load(\"https://www.b-glowing.com/skincare/cleansers/paulas-choice-calm-redness-relief-cleanser-for-oily-skin/\") let getDescription (html:HtmlDocument) = html.Descendants [\"span\"] |\u003e Seq.filter (fun x -\u003e match x.TryGetAttribute(\"itemprop\") with | None -\u003e false | Some att -\u003e match att.Value() with | \"description\" -\u003e true | _ -\u003e false ) |\u003e Seq.exactlyOne |\u003e (fun x -\u003e x.Descendants [\"p\"] |\u003e Seq.map (fun t -\u003e let tag = t.Descendants [\"strong\"] |\u003e Seq.exactlyOne |\u003e (fun b -\u003e b.InnerText()) let text = t.InnerText().[(tag.Length)..(t.InnerText().Length - 1)] tag, text ) ) let","date":"2018-01-05","objectID":"/blog/2018/2018-01-05-using-fsharp-to-parse-html/:0:0","tags":null,"title":"Using F# to Parse HTML","uri":"/blog/2018/2018-01-05-using-fsharp-to-parse-html/"}]